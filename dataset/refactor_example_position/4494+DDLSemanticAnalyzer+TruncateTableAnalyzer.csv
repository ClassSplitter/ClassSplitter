index,name,type,inner invocations,external invocations,calls,visits,length,lines,start location,definition line,modifier,commit,full text,moved,removed
1,TokenToTypeName,Field,"""","""","""","""",91,1,4425,93,26,,"private static final Map<Integer, String> TokenToTypeName = new HashMap<Integer, String>();",False,False
2,ddlDescWithWriteId,Field,"setAcidDdlDesc(DDLDescWithWriteId) setAcidDdlDesc(DDLDescWithWriteId) setAcidDdlDesc(DDLDescWithWriteId)""","""","""","""",46,1,4591,96,2,,private DDLDescWithWriteId ddlDescWithWriteId;,False,False
3,getTypeName(ASTNode),Method,"""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTypeStringFromAST(ASTNode)""","org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.parse.ParseUtils+getCharTypeInfo(ASTNode) org.apache.hadoop.hive.ql.parse.ParseUtils+getCharTypeInfo(ASTNode) org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo+getQualifiedName() org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo+getQualifiedName() org.apache.hadoop.hive.ql.parse.ParseUtils+getVarcharTypeInfo(ASTNode) org.apache.hadoop.hive.ql.parse.ParseUtils+getVarcharTypeInfo(ASTNode) org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo+getQualifiedName() org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo+getQualifiedName() org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory+getTimestampTZTypeInfo(ZoneId) org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory+getTimestampTZTypeInfo(ZoneId) org.apache.hadoop.hive.serde2.typeinfo.TimestampLocalTZTypeInfo+getQualifiedName() org.apache.hadoop.hive.serde2.typeinfo.TimestampLocalTZTypeInfo+getQualifiedName() org.apache.hadoop.hive.ql.parse.ParseUtils+getDecimalTypeTypeInfo(ASTNode) org.apache.hadoop.hive.ql.parse.ParseUtils+getDecimalTypeTypeInfo(ASTNode) org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo+getQualifiedName() org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo+getQualifiedName() java.util.Map+get(Object) java.util.Map+get(Object)""","org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+TokenToTypeName""",1146,32,6189,119,9,,"public static String getTypeName(ASTNode node) throws SemanticException {
    int token = node.getType();
    String typeName;

    // datetime type isn't currently supported
    if (token == HiveParser.TOK_DATETIME) {
      throw new SemanticException(ErrorMsg.UNSUPPORTED_TYPE.getMsg());
    }

    switch (token) {
    case HiveParser.TOK_CHAR:
      CharTypeInfo charTypeInfo = ParseUtils.getCharTypeInfo(node);
      typeName = charTypeInfo.getQualifiedName();
      break;
    case HiveParser.TOK_VARCHAR:
      VarcharTypeInfo varcharTypeInfo = ParseUtils.getVarcharTypeInfo(node);
      typeName = varcharTypeInfo.getQualifiedName();
      break;
    case HiveParser.TOK_TIMESTAMPLOCALTZ:
      TimestampLocalTZTypeInfo timestampLocalTZTypeInfo =
          TypeInfoFactory.getTimestampTZTypeInfo(null);
      typeName = timestampLocalTZTypeInfo.getQualifiedName();
      break;
    case HiveParser.TOK_DECIMAL:
      DecimalTypeInfo decTypeInfo = ParseUtils.getDecimalTypeTypeInfo(node);
      typeName = decTypeInfo.getQualifiedName();
      break;
    default:
      typeName = TokenToTypeName.get(token);
    }
    return typeName;
  }",False,True
4,DDLSemanticAnalyzer(QueryState),Method,"""","""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+createHiveDB(HiveConf) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+createHiveDB(HiveConf) org.apache.hadoop.hive.ql.QueryState+getConf() org.apache.hadoop.hive.ql.QueryState+getConf()""","""",138,3,7339,152,1,,"public DDLSemanticAnalyzer(QueryState queryState) throws SemanticException {
    this(queryState, createHiveDB(queryState.getConf()));
  }",False,False
5,"DDLSemanticAnalyzer(QueryState,Hive)",Method,"""","""","""","""",116,3,7481,156,1,,"public DDLSemanticAnalyzer(QueryState queryState, Hive db) throws SemanticException {
    super(queryState, db);
  }",False,False
6,analyzeInternal(ASTNode),Method,"""","""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getQualifiedTableName(ASTNode,String) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getQualifiedTableName(ASTNode,String) org.apache.hadoop.hive.metastore.utils.MetaStoreUtils+getDefaultCatalog(Configuration) org.apache.hadoop.hive.metastore.utils.MetaStoreUtils+getDefaultCatalog(Configuration) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getPartSpec(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getPartSpec(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getValidatedPartSpec(Table,ASTNode,HiveConf,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getValidatedPartSpec(Table,ASTNode,HiveConf,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableRename(TableName,ASTNode,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableRename(TableName,ASTNode,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableTouch(TableName,CommonTree) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableTouch(TableName,CommonTree) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableUpdateStats(ASTNode,TableName,Map<String,String>) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableUpdateStats(ASTNode,TableName,Map<String,String>) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableOwner(ASTNode,TableName) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableOwner(ASTNode,TableName) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeTruncateTable(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeTruncateTable(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getQualifiedTableName(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getQualifiedTableName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableRename(TableName,ASTNode,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+analyzeAlterTableRename(TableName,ASTNode,boolean) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) java.util.List+isEmpty() java.util.List+isEmpty() org.apache.hadoop.hive.ql.exec.Task+setFetchSource(boolean) java.util.List+get(int) java.util.List+get(int) java.util.List+size() java.util.List+size() org.apache.hadoop.hive.ql.exec.Task+setFetchSource(boolean)""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+fetchTask org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks""",3050,65,7601,161,1,,"@Override
  public void analyzeInternal(ASTNode input) throws SemanticException {

    ASTNode ast = input;
    switch (ast.getType()) {
    case HiveParser.TOK_ALTERTABLE: {
      ast = (ASTNode) input.getChild(1);
      final TableName tName =
          getQualifiedTableName((ASTNode) input.getChild(0), MetaStoreUtils.getDefaultCatalog(conf));
      // TODO CAT - for now always use the default catalog.  Eventually will want to see if
      // the user specified a catalog
      Map<String, String> partSpec = null;
      ASTNode partSpecNode = (ASTNode)input.getChild(2);
      if (partSpecNode != null) {
        //  We can use alter table partition rename to convert/normalize the legacy partition
        //  column values. In so, we should not enable the validation to the old partition spec
        //  passed in this command.
        if (ast.getType() == HiveParser.TOK_ALTERTABLE_RENAMEPART) {
          partSpec = getPartSpec(partSpecNode);
        } else {
          partSpec = getValidatedPartSpec(getTable(tName), partSpecNode, conf, false);
        }
      }

      if (ast.getType() == HiveParser.TOK_ALTERTABLE_RENAME) {
        analyzeAlterTableRename(tName, ast, false);
      } else if (ast.getType() == HiveParser.TOK_ALTERTABLE_TOUCH) {
        analyzeAlterTableTouch(tName, ast);
      } else if (ast.getType() == HiveParser.TOK_ALTERTABLE_PROPERTIES) {
        analyzeAlterTableProps(tName, null, ast, false, false);
      } else if (ast.getType() == HiveParser.TOK_ALTERTABLE_DROPPROPERTIES) {
        analyzeAlterTableProps(tName, null, ast, false, true);
      } else if (ast.getType() == HiveParser.TOK_ALTERTABLE_UPDATESTATS ||
          ast.getType() == HiveParser.TOK_ALTERPARTITION_UPDATESTATS) {
        analyzeAlterTableProps(tName, partSpec, ast, false, false);
      } else if(ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_UPDATECOLSTATS ||
          ast.getToken().getType() == HiveParser.TOK_ALTERPARTITION_UPDATECOLSTATS){
        analyzeAlterTableUpdateStats(ast, tName, partSpec);
      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_OWNER) {
        analyzeAlterTableOwner(ast, tName);
      }
      break;
    }
    case HiveParser.TOK_TRUNCATETABLE:
      analyzeTruncateTable(ast);
      break;
    case HiveParser.TOK_ALTERVIEW: {
      final TableName tName = getQualifiedTableName((ASTNode) ast.getChild(0));
      ast = (ASTNode) ast.getChild(1);
      if (ast.getType() == HiveParser.TOK_ALTERVIEW_PROPERTIES) {
        analyzeAlterTableProps(tName, null, ast, true, false);
      } else if (ast.getType() == HiveParser.TOK_ALTERVIEW_DROPPROPERTIES) {
        analyzeAlterTableProps(tName, null, ast, true, true);
      } else if (ast.getType() == HiveParser.TOK_ALTERVIEW_RENAME) {
        analyzeAlterTableRename(tName, ast, true);
      }
      break;
    }
   default:
      throw new SemanticException(""Unsupported command: "" + ast);
    }
    if (fetchTask != null && !rootTasks.isEmpty()) {
      rootTasks.get(rootTasks.size() - 1).setFetchSource(true);
    }
  }",True,True
7,"analyzeAlterTableUpdateStats(ASTNode,TableName,Map<String,String>)",Method,"analyzeInternal(ASTNode)""","""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getUnescapedName(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getUnescapedName(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getProps(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getProps(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName) org.apache.hadoop.hive.metastore.Warehouse+makePartName(Map<String,String>,boolean) org.apache.hadoop.hive.metastore.Warehouse+makePartName(Map<String,String>,boolean) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) java.lang.Object+toString() java.lang.Object+toString() org.apache.hadoop.hive.ql.metadata.Table+getCols() org.apache.hadoop.hive.ql.metadata.Table+getCols() java.lang.String+equalsIgnoreCase(String) java.lang.String+equalsIgnoreCase(String) org.apache.hadoop.hive.metastore.api.FieldSchema+getName() org.apache.hadoop.hive.metastore.api.FieldSchema+getName() org.apache.hadoop.hive.metastore.api.FieldSchema+getType() org.apache.hadoop.hive.metastore.api.FieldSchema+getType() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork+ColumnStatsUpdateWork(String,Map<String,String>,String,String,String,String) org.apache.hadoop.hive.ql.metadata.Table+getDbName() org.apache.hadoop.hive.ql.metadata.Table+getDbName() org.apache.hadoop.hive.ql.metadata.Table+getTableName() org.apache.hadoop.hive.ql.metadata.Table+getTableName() org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.io.AcidUtils+isTransactionalTable(Table) org.apache.hadoop.hive.ql.io.AcidUtils+isTransactionalTable(Table) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) java.util.List+add(E) java.util.List+add(E)""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks""",1525,40,10655,226,2,,"private void analyzeAlterTableUpdateStats(ASTNode ast, TableName tblName, Map<String, String> partSpec)
      throws SemanticException {
    String colName = getUnescapedName((ASTNode) ast.getChild(0));
    Map<String, String> mapProp = getProps((ASTNode) (ast.getChild(1)).getChild(0));

    Table tbl = getTable(tblName);
    String partName = null;
    if (partSpec != null) {
      try {
        partName = Warehouse.makePartName(partSpec, false);
      } catch (MetaException e) {
        throw new SemanticException(""partition "" + partSpec.toString()
            + "" not found"");
      }
    }

    String colType = null;
    List<FieldSchema> cols = tbl.getCols();
    for (FieldSchema col : cols) {
      if (colName.equalsIgnoreCase(col.getName())) {
        colType = col.getType();
        break;
      }
    }

    if (colType == null) {
      throw new SemanticException(""column type not found"");
    }

    ColumnStatsUpdateWork columnStatsUpdateWork =
        new ColumnStatsUpdateWork(partName, mapProp, tbl.getDbName(), tbl.getTableName(), colName, colType);
    ColumnStatsUpdateTask cStatsUpdateTask = (ColumnStatsUpdateTask) TaskFactory
        .get(columnStatsUpdateWork);
    // TODO: doesn't look like this path is actually ever exercised. Maybe this needs to be removed.
    addInputsOutputsAlterTable(tblName, partSpec, null, AlterTableType.UPDATESTATS, false);
    if (AcidUtils.isTransactionalTable(tbl)) {
      setAcidDdlDesc(columnStatsUpdateWork);
    }
    rootTasks.add(cStatsUpdateTask);
  }",False,False
8,analyzeTruncateTable(ASTNode),Method,"analyzeInternal(ASTNode)""","""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getUnescapedName(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getUnescapedName(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(String,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(String,boolean) org.apache.hadoop.hive.ql.parse.HiveTableName+of(Table) org.apache.hadoop.hive.ql.parse.HiveTableName+of(Table) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+checkTruncateEligibility(ASTNode,ASTNode,String,Table) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+checkTruncateEligibility(ASTNode,ASTNode,String,Table) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getPartSpec(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getPartSpec(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addTruncateTableOutputs(ASTNode,Table,Map<String,String>) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addTruncateTableOutputs(ASTNode,Table,Map<String,String>) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+getTruncateTaskWithoutColumnNames(TableName,Map<String,String>,Table) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+getTruncateTaskWithoutColumnNames(TableName,Map<String,String>,Table) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+getTruncateTaskWithColumnNames(ASTNode,TableName,Table,Map<String,String>,ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+getTruncateTaskWithColumnNames(ASTNode,TableName,Table,Map<String,String>,ASTNode) java.util.List+add(E) java.util.List+add(E)""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks""",937,23,12184,267,2,,"private void analyzeTruncateTable(ASTNode ast) throws SemanticException {
    ASTNode root = (ASTNode) ast.getChild(0); // TOK_TABLE_PARTITION
    final String tableName = getUnescapedName((ASTNode) root.getChild(0));

    Table table = getTable(tableName, true);
    final TableName tName = HiveTableName.of(table);
    checkTruncateEligibility(ast, root, tableName, table);

    Map<String, String> partSpec = getPartSpec((ASTNode) root.getChild(1));
    addTruncateTableOutputs(root, table, partSpec);

    Task<?> truncateTask = null;

    // Is this a truncate column command
    ASTNode colNamesNode = (ASTNode) ast.getFirstChildWithType(HiveParser.TOK_TABCOLNAME);
    if (colNamesNode == null) {
      truncateTask = getTruncateTaskWithoutColumnNames(tName, partSpec, table);
    } else {
      truncateTask = getTruncateTaskWithColumnNames(root, tName, table, partSpec, colNamesNode);
    }

    rootTasks.add(truncateTask);
  }",True,True
9,"checkTruncateEligibility(ASTNode,ASTNode,String,Table)",Method,"analyzeTruncateTable(ASTNode)""","""","org.apache.hadoop.hive.ql.metadata.Table+getTableType() org.apache.hadoop.hive.ql.metadata.Table+getTableType() java.lang.String+equalsIgnoreCase(String) java.util.Map+getOrDefault(Object,V) org.apache.hadoop.hive.ql.metadata.Table+getParameters() org.apache.hadoop.hive.ql.metadata.Table+getParameters() java.util.Map+getOrDefault(Object,V) java.lang.String+equalsIgnoreCase(String) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.metadata.Table+isNonNative() org.apache.hadoop.hive.ql.metadata.Table+isNonNative() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.metadata.Table+isPartitioned() org.apache.hadoop.hive.ql.metadata.Table+isPartitioned() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String)""","org.apache.hadoop.hive.metastore.utils.MetaStoreUtils+EXTERNAL_TABLE_PURGE""",842,17,13125,291,2,,"private void checkTruncateEligibility(ASTNode ast, ASTNode root, String tableName, Table table)
      throws SemanticException {
    boolean isForce = ast.getFirstChildWithType(HiveParser.TOK_FORCE) != null;
    if (!isForce) {
      if (table.getTableType() != TableType.MANAGED_TABLE &&
          (table.getParameters().getOrDefault(MetaStoreUtils.EXTERNAL_TABLE_PURGE, ""FALSE""))
              .equalsIgnoreCase(""FALSE"")) {
        throw new SemanticException(ErrorMsg.TRUNCATE_FOR_NON_MANAGED_TABLE.format(tableName));
      }
    }
    if (table.isNonNative()) {
      throw new SemanticException(ErrorMsg.TRUNCATE_FOR_NON_NATIVE_TABLE.format(tableName)); //TODO
    }
    if (!table.isPartitioned() && root.getChildCount() > 1) {
      throw new SemanticException(ErrorMsg.PARTSPEC_FOR_NON_PARTITIONED_TABLE.format(tableName));
    }
  }",True,True
10,"addTruncateTableOutputs(ASTNode,Table,Map<String,String>)",Method,"analyzeTruncateTable(ASTNode)""","""","org.apache.hadoop.hive.ql.metadata.Table+isPartitioned() org.apache.hadoop.hive.ql.metadata.Table+isPartitioned() java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Table,WriteType) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartitions(Hive,Table,Map<String,String>,boolean) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartitions(Hive,Table,Map<String,String>,boolean) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Partition,WriteType) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+isFullSpec(Table,Map<String,String>) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+isFullSpec(Table,Map<String,String>) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+validatePartSpec(Table,Map<String,String>,ASTNode,HiveConf,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+validatePartSpec(Table,Map<String,String>,ASTNode,HiveConf,boolean) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartition(Hive,Table,Map<String,String>,boolean) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartition(Hive,Table,Map<String,String>,boolean) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Partition,WriteType) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+validatePartSpec(Table,Map<String,String>,ASTNode,HiveConf,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+validatePartSpec(Table,Map<String,String>,ASTNode,HiveConf,boolean) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartitions(Hive,Table,Map<String,String>,boolean) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartitions(Hive,Table,Map<String,String>,boolean) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Partition,WriteType)""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteType org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteType org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteType org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteType""",1103,23,13971,309,2,,"private void addTruncateTableOutputs(ASTNode root, Table table, Map<String, String> partSpec)
      throws SemanticException {
    if (partSpec == null) {
      if (!table.isPartitioned()) {
        outputs.add(new WriteEntity(table, WriteEntity.WriteType.DDL_EXCLUSIVE));
      } else {
        for (Partition partition : PartitionUtils.getPartitions(db, table, null, false)) {
          outputs.add(new WriteEntity(partition, WriteEntity.WriteType.DDL_EXCLUSIVE));
        }
      }
    } else {
      if (isFullSpec(table, partSpec)) {
        validatePartSpec(table, partSpec, (ASTNode) root.getChild(1), conf, true);
        Partition partition = PartitionUtils.getPartition(db, table, partSpec, true);
        outputs.add(new WriteEntity(partition, WriteEntity.WriteType.DDL_EXCLUSIVE));
      } else {
        validatePartSpec(table, partSpec, (ASTNode) root.getChild(1), conf, false);
        for (Partition partition : PartitionUtils.getPartitions(db, table, partSpec, false)) {
          outputs.add(new WriteEntity(partition, WriteEntity.WriteType.DDL_EXCLUSIVE));
        }
      }
    }
  }",True,True
11,"getTruncateTaskWithoutColumnNames(TableName,Map<String,String>,Table)",Method,"analyzeTruncateTable(ASTNode)""","""","org.apache.hadoop.hive.ql.ddl.table.misc.TruncateTableDesc+TruncateTableDesc(TableName,Map<String,String>,ReplicationSpec,Table) org.apache.hadoop.hive.ql.ddl.table.misc.TruncateTableDesc+mayNeedWriteId() org.apache.hadoop.hive.ql.ddl.table.misc.TruncateTableDesc+mayNeedWriteId() org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) org.apache.hadoop.hive.ql.ddl.DDLWork+DDLWork(Set<ReadEntity>,Set<WriteEntity>,DDLDesc) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T)""","""",422,9,15078,333,2,,"private Task<?> getTruncateTaskWithoutColumnNames(TableName tableName, Map<String, String> partSpec, Table table) {
    TruncateTableDesc truncateTblDesc = new TruncateTableDesc(tableName, partSpec, null, table);
    if (truncateTblDesc.mayNeedWriteId()) {
      setAcidDdlDesc(truncateTblDesc);
    }

    DDLWork ddlWork = new DDLWork(getInputs(), getOutputs(), truncateTblDesc);
    return TaskFactory.get(ddlWork);
  }",True,True
12,"getTruncateTaskWithColumnNames(ASTNode,TableName,Table,Map<String,String>,ASTNode)",Method,"analyzeTruncateTable(ASTNode)""","""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getColumnNames(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getColumnNames(ASTNode) org.apache.hadoop.hive.ql.io.AcidUtils+isInsertOnlyTable(Map<String,String>) org.apache.hadoop.hive.ql.io.AcidUtils+isInsertOnlyTable(Map<String,String>) org.apache.hadoop.hive.ql.metadata.Table+getParameters() org.apache.hadoop.hive.ql.metadata.Table+getParameters() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.metadata.Table+isPartitioned() org.apache.hadoop.hive.ql.metadata.Table+isPartitioned() org.apache.hadoop.hive.ql.metadata.Hive+getPartition(Table,Map<String,String>,boolean) org.apache.hadoop.hive.ql.metadata.Hive+getPartition(Table,Map<String,String>,boolean) org.apache.hadoop.hive.ql.metadata.Table+getPath() org.apache.hadoop.hive.ql.metadata.Table+getPath() org.apache.hadoop.hive.ql.metadata.Partition+getDataLocation() org.apache.hadoop.hive.ql.metadata.Partition+getDataLocation() java.lang.Object+Object() org.apache.hadoop.hive.ql.metadata.Partition+getCols() org.apache.hadoop.hive.ql.metadata.Partition+getCols() org.apache.hadoop.hive.ql.metadata.Partition+getBucketCols() org.apache.hadoop.hive.ql.metadata.Partition+getBucketCols() org.apache.hadoop.hive.ql.metadata.Partition+getInputFormatClass() org.apache.hadoop.hive.ql.metadata.Partition+getInputFormatClass() org.apache.hadoop.hive.ql.exec.ArchiveUtils+isArchived(Partition) org.apache.hadoop.hive.ql.exec.ArchiveUtils+isArchived(Partition) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+constructListBucketingCtx(List<String>,List<List<String>>,Map<List<String>,String>,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+constructListBucketingCtx(List<String>,List<List<String>>,Map<List<String>,String>,boolean) org.apache.hadoop.hive.ql.metadata.Partition+getSkewedColNames() org.apache.hadoop.hive.ql.metadata.Partition+getSkewedColNames() org.apache.hadoop.hive.ql.metadata.Partition+getSkewedColValues() org.apache.hadoop.hive.ql.metadata.Partition+getSkewedColValues() org.apache.hadoop.hive.ql.metadata.Partition+getSkewedColValueLocationMaps() org.apache.hadoop.hive.ql.metadata.Partition+getSkewedColValueLocationMaps() org.apache.hadoop.hive.ql.metadata.Partition+isStoredAsSubDirectories() org.apache.hadoop.hive.ql.metadata.Partition+isStoredAsSubDirectories() org.apache.hadoop.hive.ql.metadata.Partition+isStoredAsSubDirectories() org.apache.hadoop.hive.ql.metadata.Partition+isStoredAsSubDirectories() org.apache.hadoop.hive.ql.metadata.Partition+getSkewedColNames() org.apache.hadoop.hive.ql.metadata.Partition+getSkewedColNames() org.apache.hadoop.hive.ql.metadata.Table+getPath() org.apache.hadoop.hive.ql.metadata.Table+getPath() org.apache.hadoop.hive.ql.metadata.Table+getPath() org.apache.hadoop.hive.ql.metadata.Table+getPath() org.apache.hadoop.hive.ql.metadata.Table+getCols() org.apache.hadoop.hive.ql.metadata.Table+getCols() org.apache.hadoop.hive.ql.metadata.Table+getBucketCols() org.apache.hadoop.hive.ql.metadata.Table+getBucketCols() org.apache.hadoop.hive.ql.metadata.Table+getInputFormatClass() org.apache.hadoop.hive.ql.metadata.Table+getInputFormatClass() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+constructListBucketingCtx(List<String>,List<List<String>>,Map<List<String>,String>,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+constructListBucketingCtx(List<String>,List<List<String>>,Map<List<String>,String>,boolean) org.apache.hadoop.hive.ql.metadata.Table+getSkewedColNames() org.apache.hadoop.hive.ql.metadata.Table+getSkewedColNames() org.apache.hadoop.hive.ql.metadata.Table+getSkewedColValues() org.apache.hadoop.hive.ql.metadata.Table+getSkewedColValues() org.apache.hadoop.hive.ql.metadata.Table+getSkewedColValueLocationMaps() org.apache.hadoop.hive.ql.metadata.Table+getSkewedColValueLocationMaps() org.apache.hadoop.hive.ql.metadata.Table+isStoredAsSubDirectories() org.apache.hadoop.hive.ql.metadata.Table+isStoredAsSubDirectories() org.apache.hadoop.hive.ql.metadata.Table+isStoredAsSubDirectories() org.apache.hadoop.hive.ql.metadata.Table+isStoredAsSubDirectories() org.apache.hadoop.hive.ql.metadata.Table+getSkewedColNames() org.apache.hadoop.hive.ql.metadata.Table+getSkewedColNames() java.lang.Object+equals(Object) java.lang.Object+equals(Object) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) java.util.HashSet+HashSet() java.util.List+size() java.util.List+size() java.lang.String+equalsIgnoreCase(String) java.lang.String+equalsIgnoreCase(String) org.apache.hadoop.hive.metastore.api.FieldSchema+getName() java.util.List+get(int) java.util.List+get(int) org.apache.hadoop.hive.metastore.api.FieldSchema+getName() java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) java.lang.String+equalsIgnoreCase(String) java.lang.String+equalsIgnoreCase(String) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) java.lang.String+equalsIgnoreCase(String) java.lang.String+equalsIgnoreCase(String) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.Context+getExternalTmpPath(Path) org.apache.hadoop.hive.ql.Context+getExternalTmpPath(Path) org.apache.hadoop.hive.ql.ddl.table.misc.TruncateTableDesc+TruncateTableDesc(TableName,Map<String,String>,ReplicationSpec,Table,List<Integer>,Path,Path,ListBucketingCtx) java.util.ArrayList+ArrayList(Collection) org.apache.hadoop.hive.ql.ddl.table.misc.TruncateTableDesc+mayNeedWriteId() org.apache.hadoop.hive.ql.ddl.table.misc.TruncateTableDesc+mayNeedWriteId() org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) org.apache.hadoop.hive.ql.ddl.DDLWork+DDLWork(Set<ReadEntity>,Set<WriteEntity>,DDLDesc) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.ddl.DDLWork+setNeedLock(boolean) org.apache.hadoop.hive.ql.ddl.DDLWork+setNeedLock(boolean) org.apache.hadoop.hive.ql.exec.Utilities+getTableDesc(Table) org.apache.hadoop.hive.ql.exec.Utilities+getTableDesc(Table) org.apache.hadoop.hive.ql.plan.LoadTableDesc+LoadTableDesc(Path,TableDesc,Map<String,String>) org.apache.hadoop.hive.ql.plan.LoadTableDesc+setLbCtx(ListBucketingCtx) org.apache.hadoop.hive.ql.plan.LoadTableDesc+setLbCtx(ListBucketingCtx) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.plan.MoveWork+MoveWork(Set<ReadEntity>,Set<WriteEntity>,LoadTableDesc,LoadFileDesc,boolean) org.apache.hadoop.hive.ql.exec.Task+addDependentTask(Task<?>) org.apache.hadoop.hive.ql.exec.Task+addDependentTask(Task<?>) org.apache.hadoop.hive.conf.HiveConf+getBoolVar(ConfVars) org.apache.hadoop.hive.conf.HiveConf+getBoolVar(ConfVars) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.TableSpec+TableSpec(Hive,HiveConf,ASTNode) org.apache.hadoop.hive.ql.plan.BasicStatsWork+BasicStatsWork(TableSpec) org.apache.hadoop.hive.ql.plan.BasicStatsWork+BasicStatsWork(LoadTableDesc) org.apache.hadoop.hive.ql.plan.BasicStatsWork+setNoStatsAggregator(boolean) org.apache.hadoop.hive.ql.plan.BasicStatsWork+setNoStatsAggregator(boolean) org.apache.hadoop.hive.ql.plan.BasicStatsWork+setClearAggregatorStats(boolean) org.apache.hadoop.hive.ql.plan.BasicStatsWork+setClearAggregatorStats(boolean) org.apache.hadoop.hive.ql.plan.StatsWork+StatsWork(Table,BasicStatsWork,HiveConf) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.Task+addDependentTask(Task<?>) org.apache.hadoop.hive.ql.exec.Task+addDependentTask(Task<?>) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(Throwable)""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+ctx org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf org.apache.hadoop.hive.conf.HiveConf+ConfVars org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf""",5996,137,15504,343,2,,"private Task<?> getTruncateTaskWithColumnNames(ASTNode root, TableName tName, Table table,
      Map<String, String> partSpec, ASTNode colNamesNode) throws SemanticException {
    try {
      List<String> columnNames = getColumnNames(colNamesNode);

      // It would be possible to support this, but this is such a pointless command.
      if (AcidUtils.isInsertOnlyTable(table.getParameters())) {
        throw new SemanticException(""Truncating MM table columns not presently supported"");
      }

      List<String> bucketCols = null;
      Class<? extends InputFormat> inputFormatClass = null;
      boolean isArchived = false;
      Path newTblPartLoc = null;
      Path oldTblPartLoc = null;
      List<FieldSchema> cols = null;
      ListBucketingCtx lbCtx = null;
      boolean isListBucketed = false;
      List<String> listBucketColNames = null;

      if (table.isPartitioned()) {
        Partition part = db.getPartition(table, partSpec, false);

        Path tabPath = table.getPath();
        Path partPath = part.getDataLocation();

        // if the table is in a different dfs than the partition,
        // replace the partition's dfs with the table's dfs.
        newTblPartLoc = new Path(tabPath.toUri().getScheme(), tabPath.toUri()
            .getAuthority(), partPath.toUri().getPath());

        oldTblPartLoc = partPath;

        cols = part.getCols();
        bucketCols = part.getBucketCols();
        inputFormatClass = part.getInputFormatClass();
        isArchived = ArchiveUtils.isArchived(part);
        lbCtx = constructListBucketingCtx(part.getSkewedColNames(), part.getSkewedColValues(),
            part.getSkewedColValueLocationMaps(), part.isStoredAsSubDirectories());
        isListBucketed = part.isStoredAsSubDirectories();
        listBucketColNames = part.getSkewedColNames();
      } else {
        // input and output are the same
        oldTblPartLoc = table.getPath();
        newTblPartLoc = table.getPath();
        cols  = table.getCols();
        bucketCols = table.getBucketCols();
        inputFormatClass = table.getInputFormatClass();
        lbCtx = constructListBucketingCtx(table.getSkewedColNames(), table.getSkewedColValues(),
            table.getSkewedColValueLocationMaps(), table.isStoredAsSubDirectories());
        isListBucketed = table.isStoredAsSubDirectories();
        listBucketColNames = table.getSkewedColNames();
      }

      // throw a HiveException for non-rcfile.
      if (!inputFormatClass.equals(RCFileInputFormat.class)) {
        throw new SemanticException(ErrorMsg.TRUNCATE_COLUMN_NOT_RC.getMsg());
      }

      // throw a HiveException if the table/partition is archived
      if (isArchived) {
        throw new SemanticException(ErrorMsg.TRUNCATE_COLUMN_ARCHIVED.getMsg());
      }

      Set<Integer> columnIndexes = new HashSet<Integer>();
      for (String columnName : columnNames) {
        boolean found = false;
        for (int columnIndex = 0; columnIndex < cols.size(); columnIndex++) {
          if (columnName.equalsIgnoreCase(cols.get(columnIndex).getName())) {
            columnIndexes.add(columnIndex);
            found = true;
            break;
          }
        }
        // Throw an exception if the user is trying to truncate a column which doesn't exist
        if (!found) {
          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(columnName));
        }
        // Throw an exception if the table/partition is bucketed on one of the columns
        for (String bucketCol : bucketCols) {
          if (bucketCol.equalsIgnoreCase(columnName)) {
            throw new SemanticException(ErrorMsg.TRUNCATE_BUCKETED_COLUMN.getMsg(columnName));
          }
        }
        if (isListBucketed) {
          for (String listBucketCol : listBucketColNames) {
            if (listBucketCol.equalsIgnoreCase(columnName)) {
              throw new SemanticException(
                  ErrorMsg.TRUNCATE_LIST_BUCKETED_COLUMN.getMsg(columnName));
            }
          }
        }
      }

      Path queryTmpdir = ctx.getExternalTmpPath(newTblPartLoc);
      TruncateTableDesc truncateTblDesc = new TruncateTableDesc(tName, partSpec, null, table,
          new ArrayList<Integer>(columnIndexes), oldTblPartLoc, queryTmpdir, lbCtx);
      if (truncateTblDesc.mayNeedWriteId()) {
        setAcidDdlDesc(truncateTblDesc);
      }

      DDLWork ddlWork = new DDLWork(getInputs(), getOutputs(), truncateTblDesc);
      Task<?> truncateTask = TaskFactory.get(ddlWork);

      addInputsOutputsAlterTable(tName, partSpec, null, AlterTableType.TRUNCATE, false);
      ddlWork.setNeedLock(true);
      TableDesc tblDesc = Utilities.getTableDesc(table);
      // Write the output to temporary directory and move it to the final location at the end
      // so the operation is atomic.
      LoadTableDesc ltd = new LoadTableDesc(queryTmpdir, tblDesc, partSpec == null ? new HashMap<>() : partSpec);
      ltd.setLbCtx(lbCtx);
      Task<MoveWork> moveTsk = TaskFactory.get(new MoveWork(null, null, ltd, null, false));
      truncateTask.addDependentTask(moveTsk);

      // Recalculate the HDFS stats if auto gather stats is set
      if (conf.getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {
        BasicStatsWork basicStatsWork;
        if (oldTblPartLoc.equals(newTblPartLoc)) {
          // If we're merging to the same location, we can avoid some metastore calls
          TableSpec tablepart = new TableSpec(this.db, conf, root);
          basicStatsWork = new BasicStatsWork(tablepart);
        } else {
          basicStatsWork = new BasicStatsWork(ltd);
        }
        basicStatsWork.setNoStatsAggregator(true);
        basicStatsWork.setClearAggregatorStats(true);
        StatsWork columnStatsWork = new StatsWork(table, basicStatsWork, conf);

        Task<?> statTask = TaskFactory.get(columnStatsWork);
        moveTsk.addDependentTask(statTask);
      }

      return truncateTask;
    } catch (HiveException e) {
      throw new SemanticException(e);
    }
  }",True,True
13,"isFullSpec(Table,Map<String,String>)",Method,"addTruncateTableOutputs(ASTNode,Table,Map<String,String>) addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean)""","""","org.apache.hadoop.hive.ql.metadata.Table+getPartCols() org.apache.hadoop.hive.ql.metadata.Table+getPartCols() java.util.Map+get(Object) java.util.Map+get(Object) org.apache.hadoop.hive.metastore.api.FieldSchema+getName() org.apache.hadoop.hive.metastore.api.FieldSchema+getName()""","""",241,8,21504,481,9,,"public static boolean isFullSpec(Table table, Map<String, String> partSpec) {
    for (FieldSchema partCol : table.getPartCols()) {
      if (partSpec.get(partCol.getName()) == null) {
        return false;
      }
    }
    return true;
  }",False,False
14,"validateAlterTableType(Table,AlterTableType)",Method,"analyzeAlterTableTouch(TableName,CommonTree)""","""","org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+validateAlterTableType(Table,AlterTableType,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+validateAlterTableType(Table,AlterTableType,boolean)""","""",140,3,21749,490,2,,"private void validateAlterTableType(Table tbl, AlterTableType op) throws SemanticException {
    validateAlterTableType(tbl, op, false);
  }",False,False
15,"validateAlterTableType(Table,AlterTableType,boolean)",Method,"validateAlterTableType(Table,AlterTableType) addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean)""","""","org.apache.hadoop.hive.ql.metadata.Table+isView() org.apache.hadoop.hive.ql.metadata.Table+isView() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.metadata.Table+isNonNative() org.apache.hadoop.hive.ql.metadata.Table+isNonNative() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.metadata.Table+getTableName() org.apache.hadoop.hive.ql.metadata.Table+getTableName()""","""",962,29,21893,494,2,,"private void validateAlterTableType(Table tbl, AlterTableType op, boolean expectView)
      throws SemanticException {
    if (tbl.isView()) {
      if (!expectView) {
        throw new SemanticException(ErrorMsg.ALTER_COMMAND_FOR_VIEWS.getMsg());
      }

      switch (op) {
      case ADDPARTITION:
      case DROPPARTITION:
      case RENAMEPARTITION:
      case ADDPROPS:
      case DROPPROPS:
      case RENAME:
        // allow this form
        break;
      default:
        throw new SemanticException(ErrorMsg.ALTER_VIEW_DISALLOWED_OP.getMsg(op.toString()));
      }
    } else {
      if (expectView) {
        throw new SemanticException(ErrorMsg.ALTER_COMMAND_FOR_TABLES.getMsg());
      }
    }
    if (tbl.isNonNative() && !AlterTableType.NON_NATIVE_TABLE_ALLOWED.contains(op)) {
      throw new SemanticException(ErrorMsg.ALTER_TABLE_NON_NATIVE.format(
          AlterTableType.NON_NATIVE_TABLE_ALLOWED.toString(), tbl.getTableName()));
    }
  }",False,False
16,hasConstraintsEnabled(String),Method,"analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean)""","""","org.apache.hadoop.hive.ql.metadata.Hive+getEnabledNotNullConstraints(String,String) org.apache.hadoop.hive.ql.metadata.Hive+get() org.apache.hadoop.hive.ql.metadata.Hive+get() org.apache.hadoop.hive.ql.metadata.Hive+getEnabledNotNullConstraints(String,String) org.apache.hadoop.hive.metastore.api.Database+getName() org.apache.hadoop.hive.ql.metadata.Hive+getDatabaseCurrent() org.apache.hadoop.hive.ql.metadata.Hive+getDatabaseCurrent() org.apache.hadoop.hive.metastore.api.Database+getName() org.apache.hadoop.hive.ql.metadata.Hive+getEnabledDefaultConstraints(String,String) org.apache.hadoop.hive.ql.metadata.Hive+get() org.apache.hadoop.hive.ql.metadata.Hive+get() org.apache.hadoop.hive.ql.metadata.Hive+getEnabledDefaultConstraints(String,String) org.apache.hadoop.hive.metastore.api.Database+getName() org.apache.hadoop.hive.ql.metadata.Hive+getDatabaseCurrent() org.apache.hadoop.hive.ql.metadata.Hive+getDatabaseCurrent() org.apache.hadoop.hive.metastore.api.Database+getName() java.lang.RuntimeException+RuntimeException(Throwable) java.util.Map+isEmpty() org.apache.hadoop.hive.ql.metadata.NotNullConstraint+getNotNullConstraints() org.apache.hadoop.hive.ql.metadata.NotNullConstraint+getNotNullConstraints() java.util.Map+isEmpty() java.util.Map+isEmpty() org.apache.hadoop.hive.ql.metadata.DefaultConstraint+getDefaultConstraints() org.apache.hadoop.hive.ql.metadata.DefaultConstraint+getDefaultConstraints() java.util.Map+isEmpty()""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db""",797,23,22859,524,2,,"private boolean hasConstraintsEnabled(final String tblName) throws SemanticException{

    NotNullConstraint nnc = null;
    DefaultConstraint dc = null;
    try {
      // retrieve enabled NOT NULL constraint from metastore
      nnc = Hive.get().getEnabledNotNullConstraints(
          db.getDatabaseCurrent().getName(), tblName);
      dc = Hive.get().getEnabledDefaultConstraints(
          db.getDatabaseCurrent().getName(), tblName);
    } catch (Exception e) {
      if (e instanceof SemanticException) {
        throw (SemanticException) e;
      } else {
        throw (new RuntimeException(e));
      }
    }
    if((nnc != null  && !nnc.getNotNullConstraints().isEmpty())
        || (dc != null && !dc.getDefaultConstraints().isEmpty())) {
      return true;
    }
    return false;
  }",False,True
17,"analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean)",Method,"analyzeInternal(ASTNode) analyzeInternal(ASTNode) analyzeInternal(ASTNode) analyzeInternal(ASTNode) analyzeInternal(ASTNode)""","""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getProps(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getProps(ASTNode) java.util.Map+entrySet() java.util.Map+entrySet() java.lang.String+equals(Object) java.util.Map.Entry+getKey() java.util.Map.Entry+getKey() java.lang.String+equals(Object) java.lang.String+equals(Object) java.util.Map.Entry+getKey() java.util.Map.Entry+getKey() java.lang.String+equals(Object) java.lang.Long+parseLong(String) java.lang.Long+parseLong(String) java.util.Map.Entry+getValue() java.util.Map.Entry+getValue() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) java.util.Map.Entry+getKey() java.util.Map.Entry+getKey() java.util.Map.Entry+getValue() java.util.Map.Entry+getValue() java.lang.String+equals(Object) java.util.Map.Entry+getKey() java.util.Map.Entry+getKey() java.lang.String+equals(Object) java.lang.String+equals(Object) java.util.Map.Entry+getValue() java.util.Map.Entry+getValue() java.lang.String+equals(Object) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+hasConstraintsEnabled(String) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+hasConstraintsEnabled(String) org.apache.hadoop.hive.common.TableName+getTable() org.apache.hadoop.hive.common.TableName+getTable() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.common.TableName+getDbTable() org.apache.hadoop.hive.common.TableName+getDbTable() java.lang.String+equals(Object) org.apache.hadoop.hive.ql.QueryState+getCommandType() org.apache.hadoop.hive.ql.QueryState+getCommandType() java.lang.String+equals(Object) java.lang.String+equals(Object) org.apache.hadoop.hive.ql.QueryState+getCommandType() org.apache.hadoop.hive.ql.QueryState+getCommandType() java.lang.String+equals(Object) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) java.util.Map.Entry+getKey() java.util.Map.Entry+getKey() org.apache.hadoop.hive.metastore.api.EnvironmentContext+EnvironmentContext() org.apache.hadoop.hive.metastore.api.EnvironmentContext+putToProperties(String,String) org.apache.hadoop.hive.metastore.api.EnvironmentContext+putToProperties(String,String) org.apache.hadoop.hive.ql.io.AcidUtils+isTablePropertyTransactional(Map<String,String>) org.apache.hadoop.hive.ql.io.AcidUtils+isTablePropertyTransactional(Map<String,String>) java.util.Map+containsKey(Object) java.util.Map+containsKey(Object) org.apache.hadoop.hive.ql.io.AcidUtils+isTransactionalTable(Table) org.apache.hadoop.hive.ql.io.AcidUtils+isTransactionalTable(Table) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName,boolean) org.apache.hadoop.hive.metastore.api.Table+getParameters() org.apache.hadoop.hive.ql.metadata.Table+getTTable() org.apache.hadoop.hive.ql.metadata.Table+getTTable() org.apache.hadoop.hive.metastore.api.Table+getParameters() java.util.Map+keySet() java.util.Map+keySet() java.util.Map+containsKey(Object) java.util.Map+containsKey(Object) org.apache.hadoop.hive.ql.metadata.Table+getTableName() org.apache.hadoop.hive.ql.metadata.Table+getTableName() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableUnsetPropertiesDesc+AlterTableUnsetPropertiesDesc(TableName,Map<String,String>,ReplicationSpec,boolean,Map<String,String>,boolean,EnvironmentContext) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getType() org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getType() org.apache.hadoop.hive.ql.ddl.DDLWork+DDLWork(Set<ReadEntity>,Set<WriteEntity>,DDLDesc) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+addPropertyReadEntry(Map<String,String>,Set<ReadEntity>) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+addPropertyReadEntry(Map<String,String>,Set<ReadEntity>) org.apache.hadoop.hive.ql.io.AcidUtils+isFullAcidTable(Map<String,String>) org.apache.hadoop.hive.ql.io.AcidUtils+isFullAcidTable(Map<String,String>) org.apache.hadoop.hive.ql.io.AcidUtils+isFullAcidTable(Table) org.apache.hadoop.hive.ql.io.AcidUtils+isFullAcidTable(Table) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName,boolean) org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableSetPropertiesDesc+AlterTableSetPropertiesDesc(TableName,Map<String,String>,ReplicationSpec,boolean,Map<String,String>,boolean,boolean,EnvironmentContext) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getType() org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getType() org.apache.hadoop.hive.ql.ddl.DDLWork+DDLWork(Set<ReadEntity>,Set<WriteEntity>,DDLDesc) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.ddl.DDLWork+setNeedLock(boolean) org.apache.hadoop.hive.ql.ddl.DDLWork+setNeedLock(boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T)""","org.apache.hadoop.hive.common.StatsSetupConst+ROW_COUNT org.apache.hadoop.hive.common.StatsSetupConst+RAW_DATA_SIZE org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+queryState org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+queryState org.apache.hadoop.hive.common.StatsSetupConst+ROW_COUNT org.apache.hadoop.hive.common.StatsSetupConst+RAW_DATA_SIZE org.apache.hadoop.hive.common.StatsSetupConst+STATS_GENERATED org.apache.hadoop.hive.common.StatsSetupConst+USER org.apache.hadoop.hive.metastore.api.hive_metastoreConstants+TABLE_TRANSACTIONAL_PROPERTIES org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+inputs org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks""",4614,91,23660,548,2,,"private void analyzeAlterTableProps(TableName tableName, Map<String, String> partSpec, ASTNode ast,
      boolean expectView, boolean isUnset) throws SemanticException {

    Map<String, String> mapProp = getProps((ASTNode) (ast.getChild(0)).getChild(0));
    EnvironmentContext environmentContext = null;
    // we need to check if the properties are valid, especially for stats.
    // they might be changed via alter table .. update statistics or
    // alter table .. set tblproperties. If the property is not row_count
    // or raw_data_size, it could not be changed through update statistics
    boolean changeStatsSucceeded = false;
    for (Entry<String, String> entry : mapProp.entrySet()) {
      // we make sure that we do not change anything if there is anything
      // wrong.
      if (entry.getKey().equals(StatsSetupConst.ROW_COUNT)
          || entry.getKey().equals(StatsSetupConst.RAW_DATA_SIZE)) {
        try {
          Long.parseLong(entry.getValue());
          changeStatsSucceeded = true;
        } catch (Exception e) {
          throw new SemanticException(""AlterTable "" + entry.getKey() + "" failed with value ""
              + entry.getValue());
        }
      }
      // if table is being modified to be external we need to make sure existing table
      // doesn't have enabled constraint since constraints are disallowed with such tables
      else if (entry.getKey().equals(""external"") && entry.getValue().equals(""true"")) {
        if (hasConstraintsEnabled(tableName.getTable())) {
          throw new SemanticException(
              ErrorMsg.INVALID_CSTR_SYNTAX.getMsg(""Table: "" + tableName.getDbTable() + "" has constraints enabled.""
                  + ""Please remove those constraints to change this property.""));
        }
      }
      else {
        if (queryState.getCommandType()
            .equals(HiveOperation.ALTERTABLE_UPDATETABLESTATS.getOperationName())
            || queryState.getCommandType()
                .equals(HiveOperation.ALTERTABLE_UPDATEPARTSTATS.getOperationName())) {
          throw new SemanticException(""AlterTable UpdateStats "" + entry.getKey()
              + "" failed because the only valid keys are "" + StatsSetupConst.ROW_COUNT + "" and ""
              + StatsSetupConst.RAW_DATA_SIZE);
        }
      }

      if (changeStatsSucceeded) {
        environmentContext = new EnvironmentContext();
        environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED, StatsSetupConst.USER);
      }
    }
    boolean isToTxn = AcidUtils.isTablePropertyTransactional(mapProp)
        || mapProp.containsKey(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);
    boolean isExplicitStatsUpdate = changeStatsSucceeded && AcidUtils.isTransactionalTable(getTable(tableName, true));
    AbstractAlterTableDesc alterTblDesc = null;
    DDLWork ddlWork = null;

    if (isUnset) {
      boolean dropIfExists = ast.getChild(1) != null;
      // validate Unset Non Existed Table Properties
      if (!dropIfExists) {
        Table tab = getTable(tableName, true);
        Map<String, String> tableParams = tab.getTTable().getParameters();
        for (String currKey : mapProp.keySet()) {
          if (!tableParams.containsKey(currKey)) {
            String errorMsg = ""The following property "" + currKey + "" does not exist in "" + tab.getTableName();
            throw new SemanticException(
              ErrorMsg.ALTER_TBL_UNSET_NON_EXIST_PROPERTY.getMsg(errorMsg));
          }
        }
      }

      alterTblDesc = new AlterTableUnsetPropertiesDesc(tableName, partSpec, null, expectView, mapProp,
          isExplicitStatsUpdate, environmentContext);
      addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc, alterTblDesc.getType(), isToTxn);
      ddlWork = new DDLWork(getInputs(), getOutputs(), alterTblDesc);
    } else {
      addPropertyReadEntry(mapProp, inputs);
      boolean isAcidConversion = isToTxn && AcidUtils.isFullAcidTable(mapProp)
          && !AcidUtils.isFullAcidTable(getTable(tableName, true));
      alterTblDesc = new AlterTableSetPropertiesDesc(tableName, partSpec, null, expectView, mapProp,
          isExplicitStatsUpdate, isAcidConversion, environmentContext);
      addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc, alterTblDesc.getType(), isToTxn);
      ddlWork = new DDLWork(getInputs(), getOutputs(), alterTblDesc);
    }
    if (isToTxn) {
      ddlWork.setNeedLock(true); // Hmm... why don't many other operations here need locks?
    }
    if (isToTxn || isExplicitStatsUpdate) {
      setAcidDdlDesc(alterTblDesc);
    }

    rootTasks.add(TaskFactory.get(ddlWork));
  }",False,True
18,setAcidDdlDesc(DDLDescWithWriteId),Method,"analyzeAlterTableUpdateStats(ASTNode,TableName,Map<String,String>) getTruncateTaskWithoutColumnNames(TableName,Map<String,String>,Table) getTruncateTaskWithColumnNames(ASTNode,TableName,Table,Map<String,String>,ASTNode) analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) analyzeAlterTableRename(TableName,ASTNode,boolean)""","""","java.lang.IllegalStateException+IllegalStateException(String)""","org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+ddlDescWithWriteId org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+ddlDescWithWriteId org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+ddlDescWithWriteId org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+ddlDescWithWriteId org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+ddlDescWithWriteId org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+ddlDescWithWriteId""",268,6,28278,640,2,,"private void setAcidDdlDesc(DDLDescWithWriteId descWithWriteId) {
    if(this.ddlDescWithWriteId != null) {
      throw new IllegalStateException(""ddlDescWithWriteId is already set: "" + this.ddlDescWithWriteId);
    }
    this.ddlDescWithWriteId = descWithWriteId;
  }",False,False
19,getAcidDdlDesc(),Method,"""","""","""","org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+ddlDescWithWriteId""",91,4,28550,648,1,,"@Override
  public DDLDescWithWriteId getAcidDdlDesc() {
    return ddlDescWithWriteId;
  }",False,False
20,"determineAlterTableWriteType(Table,AbstractAlterTableDesc,AlterTableType)",Method,"addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean)""","""","org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getProps() org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getProps() java.lang.Boolean+parseBoolean(String) java.lang.Boolean+parseBoolean(String) java.util.Map+get(Object) org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getProps() org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getProps() java.util.Map+get(Object) org.apache.hadoop.hive.ql.io.AcidUtils+isTransactionalTable(Table) org.apache.hadoop.hive.ql.io.AcidUtils+isTransactionalTable(Table) org.apache.hadoop.hive.ql.hooks.WriteEntity+determineAlterTableWriteType(AlterTableType) org.apache.hadoop.hive.ql.hooks.WriteEntity+determineAlterTableWriteType(AlterTableType)""","org.apache.hadoop.hive.metastore.api.hive_metastoreConstants+TABLE_IS_TRANSACTIONAL""",654,13,28764,653,2,,"private WriteType determineAlterTableWriteType(Table tab, AbstractAlterTableDesc desc, AlterTableType op) {
    boolean convertingToAcid = false;
    if (desc != null && desc.getProps() != null &&
        Boolean.parseBoolean(desc.getProps().get(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL))) {
      convertingToAcid = true;
    }
    if(!AcidUtils.isTransactionalTable(tab) && convertingToAcid) {
      //non-acid to transactional conversion (property itself) must be mutexed to prevent concurrent writes.
      // See HIVE-16688 for use cases.
      return WriteType.DDL_EXCLUSIVE;
    }
    return WriteEntity.determineAlterTableWriteType(op);
  }",False,False
21,"addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean)",Method,"analyzeAlterTableUpdateStats(ASTNode,TableName,Map<String,String>) getTruncateTaskWithColumnNames(ASTNode,TableName,Table,Map<String,String>,ASTNode) analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) analyzeAlterTableProps(TableName,Map<String,String>,ASTNode,boolean,boolean) analyzeAlterTableRename(TableName,ASTNode,boolean)""","""","org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+isCascade() org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+isCascade() java.util.Map+isEmpty() java.util.Map+isEmpty() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String,Throwable) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName,boolean) org.apache.hadoop.hive.ql.metadata.Table+isPartitioned() org.apache.hadoop.hive.ql.metadata.Table+isPartitioned() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+determineAlterTableWriteType(Table,AbstractAlterTableDesc,AlterTableType) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+determineAlterTableWriteType(Table,AbstractAlterTableDesc,AlterTableType) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.ReadEntity+ReadEntity(Table) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Table,WriteType) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartitions(Hive,Table,Map<String,String>,boolean) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartitions(Hive,Table,Map<String,String>,boolean) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Partition,WriteType) org.apache.hadoop.hive.ql.hooks.ReadEntity+ReadEntity(Table) org.apache.hadoop.hive.ql.hooks.ReadEntity+noLockNeeded() org.apache.hadoop.hive.ql.hooks.ReadEntity+noLockNeeded() java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+isFullSpec(Table,Map<String,String>) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+isFullSpec(Table,Map<String,String>) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartition(Hive,Table,Map<String,String>,boolean) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartition(Hive,Table,Map<String,String>,boolean) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Partition,WriteType) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String,Throwable) org.apache.hadoop.hive.conf.HiveConf+getBoolVar(ConfVars) org.apache.hadoop.hive.conf.HiveConf+getBoolVar(ConfVars) org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartitions(Hive,Table,Map<String,String>,boolean) org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils+getPartitions(Hive,Table,Map<String,String>,boolean) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Partition,WriteType) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+validateAlterTableType(Table,AlterTableType,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+validateAlterTableType(Table,AlterTableType,boolean) org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+expectView() org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+expectView()""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+inputs org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteType org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+inputs org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf org.apache.hadoop.hive.conf.HiveConf+ConfVars org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs""",2717,62,29422,667,2,,"private void addInputsOutputsAlterTable(TableName tableName, Map<String, String> partSpec,
      AbstractAlterTableDesc desc, AlterTableType op, boolean doForceExclusive) throws SemanticException {
    boolean isCascade = desc != null && desc.isCascade();
    boolean alterPartitions = partSpec != null && !partSpec.isEmpty();
    //cascade only occurs at table level then cascade to partition level
    if (isCascade && alterPartitions) {
      throw new SemanticException(
          ErrorMsg.ALTER_TABLE_PARTITION_CASCADE_NOT_SUPPORTED, op.getName());
    }

    Table tab = getTable(tableName, true);
    // cascade only occurs with partitioned table
    if (isCascade && !tab.isPartitioned()) {
      throw new SemanticException(
          ErrorMsg.ALTER_TABLE_NON_PARTITIONED_TABLE_CASCADE_NOT_SUPPORTED);
    }

    // Determine the lock type to acquire
    WriteEntity.WriteType writeType = doForceExclusive
        ? WriteType.DDL_EXCLUSIVE : determineAlterTableWriteType(tab, desc, op);

    if (!alterPartitions) {
      inputs.add(new ReadEntity(tab));
      WriteEntity alterTableOutput = new WriteEntity(tab, writeType);
      outputs.add(alterTableOutput);
      //do not need the lock for partitions since they are covered by the table lock
      if (isCascade) {
        for (Partition part : PartitionUtils.getPartitions(db, tab, partSpec, false)) {
          outputs.add(new WriteEntity(part, WriteEntity.WriteType.DDL_NO_LOCK));
        }
      }
    } else {
      ReadEntity re = new ReadEntity(tab);
      // In the case of altering a table for its partitions we don't need to lock the table
      // itself, just the partitions.  But the table will have a ReadEntity.  So mark that
      // ReadEntity as no lock.
      re.noLockNeeded();
      inputs.add(re);

      if (isFullSpec(tab, partSpec)) {
        // Fully specified partition spec
        Partition part = PartitionUtils.getPartition(db, tab, partSpec, true);
        outputs.add(new WriteEntity(part, writeType));
      } else {
        // Partial partition spec supplied. Make sure this is allowed.
        if (!AlterTableType.SUPPORT_PARTIAL_PARTITION_SPEC.contains(op)) {
          throw new SemanticException(
              ErrorMsg.ALTER_TABLE_TYPE_PARTIAL_PARTITION_SPEC_NO_SUPPORTED, op.getName());
        } else if (!conf.getBoolVar(HiveConf.ConfVars.DYNAMICPARTITIONING)) {
          throw new SemanticException(ErrorMsg.DYNAMIC_PARTITION_DISABLED);
        }

        for (Partition part : PartitionUtils.getPartitions(db, tab, partSpec, true)) {
          outputs.add(new WriteEntity(part, writeType));
        }
      }
    }

    if (desc != null) {
      validateAlterTableType(tab, op, desc.expectView());
    }
  }",False,False
22,"analyzeAlterTableOwner(ASTNode,TableName)",Method,"analyzeInternal(ASTNode)""","""","org.apache.hadoop.hive.ql.parse.authorization.AuthorizationParseUtils+getPrincipalDesc(ASTNode) org.apache.hadoop.hive.ql.parse.authorization.AuthorizationParseUtils+getPrincipalDesc(ASTNode) org.apache.hadoop.hive.ql.ddl.privilege.PrincipalDesc+getType() org.apache.hadoop.hive.ql.ddl.privilege.PrincipalDesc+getType() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.ddl.privilege.PrincipalDesc+getName() org.apache.hadoop.hive.ql.ddl.privilege.PrincipalDesc+getName() org.apache.hadoop.hive.ql.parse.SemanticException+SemanticException(String) org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableSetOwnerDesc+AlterTableSetOwnerDesc(TableName,PrincipalDesc) java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T,HiveConf) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T,HiveConf) org.apache.hadoop.hive.ql.ddl.DDLWork+DDLWork(Set<ReadEntity>,Set<WriteEntity>,DDLDesc) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs()""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+conf""",693,14,32143,730,2,,"private void analyzeAlterTableOwner(ASTNode ast, TableName tableName) throws SemanticException {
    PrincipalDesc ownerPrincipal = AuthorizationParseUtils.getPrincipalDesc((ASTNode) ast.getChild(0));

    if (ownerPrincipal.getType() == null) {
      throw new SemanticException(""Owner type can't be null in alter table set owner command"");
    }

    if (ownerPrincipal.getName() == null) {
      throw new SemanticException(""Owner name can't be null in alter table set owner command"");
    }

    AlterTableSetOwnerDesc alterTblDesc  = new AlterTableSetOwnerDesc(tableName, ownerPrincipal);
    rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), alterTblDesc), conf));
  }",False,True
23,QualifiedNameUtil,MemberClass,"""","""","org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil+getFullyQualifiedName(ASTNode)""","""",964,28,-1,-1,8,"/** 
 * Utility class to resolve QualifiedName
 */
","/**
   * Utility class to resolve QualifiedName
   */
  static class QualifiedNameUtil {

    /**
     * Get the fully qualified name in the ast. e.g. the ast of the form ^(DOT
     * ^(DOT a b) c) will generate a name of the form a.b.c
     *
     * @param ast
     *          The AST from which the qualified name has to be extracted
     * @return String
     */
    static public String getFullyQualifiedName(ASTNode ast) {
      if (ast.getChildCount() == 0) {
        return ast.getText();
      } else if (ast.getChildCount() == 2) {
        return getFullyQualifiedName((ASTNode) ast.getChild(0)) + "".""
        + getFullyQualifiedName((ASTNode) ast.getChild(1));
      } else if (ast.getChildCount() == 3) {
        return getFullyQualifiedName((ASTNode) ast.getChild(0)) + "".""
        + getFullyQualifiedName((ASTNode) ast.getChild(1)) + "".""
        + getFullyQualifiedName((ASTNode) ast.getChild(2));
      } else {
        return null;
      }
    }
  }",False,False
24,"analyzeAlterTableRename(TableName,ASTNode,boolean)",Method,"analyzeInternal(ASTNode) analyzeInternal(ASTNode)""","""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getQualifiedTableName(ASTNode) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getQualifiedTableName(ASTNode) org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableRenameDesc+AlterTableRenameDesc(TableName,ReplicationSpec,boolean,String) org.apache.hadoop.hive.common.TableName+getDbTable() org.apache.hadoop.hive.common.TableName+getDbTable() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(String,boolean) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(String,boolean) org.apache.hadoop.hive.common.TableName+getDbTable() org.apache.hadoop.hive.common.TableName+getDbTable() org.apache.hadoop.hive.ql.io.AcidUtils+isTransactionalTable(Table) org.apache.hadoop.hive.ql.io.AcidUtils+isTransactionalTable(Table) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+setAcidDdlDesc(DDLDescWithWriteId) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+addInputsOutputsAlterTable(TableName,Map<String,String>,AbstractAlterTableDesc,AlterTableType,boolean) org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getType() org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableDesc+getType() java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.ddl.DDLWork+DDLWork(Set<ReadEntity>,Set<WriteEntity>,DDLDesc) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs()""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks""",644,12,33808,774,2,,"private void analyzeAlterTableRename(TableName source, ASTNode ast, boolean expectView)
      throws SemanticException {
    final TableName target = getQualifiedTableName((ASTNode) ast.getChild(0));

    AlterTableRenameDesc alterTblDesc = new AlterTableRenameDesc(source, null, expectView, target.getDbTable());
    Table table = getTable(source.getDbTable(), true);
    if (AcidUtils.isTransactionalTable(table)) {
      setAcidDdlDesc(alterTblDesc);
    }
    addInputsOutputsAlterTable(source, null, alterTblDesc, alterTblDesc.getType(), false);
    rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), alterTblDesc)));
  }",False,False
25,"analyzeAlterTableTouch(TableName,CommonTree)",Method,"analyzeInternal(ASTNode)""","""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getTable(TableName) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+validateAlterTableType(Table,AlterTableType) org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer+validateAlterTableType(Table,AlterTableType) java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.ReadEntity+ReadEntity(Table) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getPartitionSpecs(Table,CommonTree) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getPartitionSpecs(Table,CommonTree) java.util.List+isEmpty() java.util.List+isEmpty() org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableTouchDesc+AlterTableTouchDesc(String,Map<String,String>) org.apache.hadoop.hive.common.TableName+getDbTable() org.apache.hadoop.hive.common.TableName+getDbTable() java.util.Set+add(E) java.util.Set+add(E) org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteEntity(Table,WriteType) java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.ddl.DDLWork+DDLWork(Set<ReadEntity>,Set<WriteEntity>,DDLDesc) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableTouchDesc+AlterTableTouchDesc(String,Map<String,String>) org.apache.hadoop.hive.common.TableName+getDbTable() org.apache.hadoop.hive.common.TableName+getDbTable() java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.exec.TaskFactory+get(T) org.apache.hadoop.hive.ql.ddl.DDLWork+DDLWork(Set<ReadEntity>,Set<WriteEntity>,DDLDesc) org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getInputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs() org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+getOutputs()""","org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+inputs org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteType org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+db org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+outputs org.apache.hadoop.hive.ql.hooks.WriteEntity+WriteType org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer+rootTasks""",1332,31,34456,797,2,"/** 
 * Rewrite the metadata for one or more partitions in a table. Useful when an external process modifies files on HDFS and you want the pre/post hooks to be fired for the specified partition.
 * @param ast The parsed command tree.
 * @throws SemanticException Parsing failed
 */
","/**
   * Rewrite the metadata for one or more partitions in a table. Useful when
   * an external process modifies files on HDFS and you want the pre/post
   * hooks to be fired for the specified partition.
   *
   * @param ast
   *          The parsed command tree.
   * @throws SemanticException
   *           Parsing failed
   */
  private void analyzeAlterTableTouch(TableName tName, CommonTree ast) throws SemanticException {

    Table tab = getTable(tName);
    validateAlterTableType(tab, AlterTableType.TOUCH);
    inputs.add(new ReadEntity(tab));

    // partition name to value
    List<Map<String, String>> partSpecs = getPartitionSpecs(tab, ast);

    if (partSpecs.isEmpty()) {
      AlterTableTouchDesc touchDesc = new AlterTableTouchDesc(tName.getDbTable(), null);
      outputs.add(new WriteEntity(tab, WriteEntity.WriteType.DDL_NO_LOCK));
      rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), touchDesc)));
    } else {
      PartitionUtils.addTablePartsOutputs(db, outputs, tab, partSpecs, false, WriteEntity.WriteType.DDL_NO_LOCK);
      for (Map<String, String> partSpec : partSpecs) {
        AlterTableTouchDesc touchDesc = new AlterTableTouchDesc(tName.getDbTable(), partSpec);
        rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), touchDesc)));
      }
    }
  }",False,False
