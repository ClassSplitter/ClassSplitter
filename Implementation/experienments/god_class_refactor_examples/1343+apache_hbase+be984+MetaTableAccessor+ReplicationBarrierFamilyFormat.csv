index,name,document,type,inner invocations,external invocations,calls,visits,length,lines,modifier,commit,full text,moved,removed,gpt text,gpt response,code summary
1,LOG,log ,Field,,,,,83,1,26,,private static final Logger LOG = LoggerFactory.getLogger(MetaTableAccessor.class);,False,False,,False,
2,METALOG,metalog ,Field,,,,,94,1,26,,"private static final Logger METALOG = LoggerFactory.getLogger(""org.apache.hadoop.hbase.META"");",False,False,,False,
3,MetaTableAccessor(),meta table accessor ,Method,,,,,33,2,2,,"private MetaTableAccessor() {
  }",False,False,This method serves as a private constructor for the MetaTableAccessor class.,False,The code snippet shows a private constructor for a class named MetaTableAccessor. This constructor is empty and does not take any parameters. It is likely used to prevent the instantiation of the class from outside its own scope.
4,REPLICATION_PARENT_QUALIFIER,replication parent qualifier ,Field,,,,,103,2,25,,"@VisibleForTesting
  public static final byte[] REPLICATION_PARENT_QUALIFIER = Bytes.toBytes(""parent"");",True,True,,False,
5,ESCAPE_BYTE,escape byte ,Field,,,,,52,1,26,,private static final byte ESCAPE_BYTE = (byte) 0xFF;,True,True,,False,
6,SEPARATED_BYTE,separated byte ,Field,,,,,48,1,26,,private static final byte SEPARATED_BYTE = 0x00;,True,True,,False,
7,"fullScanRegions(Connection,ClientMetaTableAccessor.Visitor)",full scan regions connection visitor performs a full scan of hbase meta for regions param connection connection we re using param visitor visitor invoked against each row in regions family ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor)",,394,9,9,"/** 
 * Performs a full scan of <code>hbase:meta</code> for regions.
 * @param connection connection we're using
 * @param visitor Visitor invoked against each row in regions family.
 */
","/**
   * Performs a full scan of <code>hbase:meta</code> for regions.
   * @param connection connection we're using
   * @param visitor Visitor invoked against each row in regions family.
   */
  public static void fullScanRegions(Connection connection,
    final ClientMetaTableAccessor.Visitor visitor) throws IOException {
    scanMeta(connection, null, null, QueryType.REGION, visitor);
  }",False,False,"This method performs a full scan of the ""hbase:meta"" table for regions. It takes a connection and a visitor as parameters, and invokes the visitor against each row in the regions family.",False,"The given code is a method named ""fullScanRegions"" that performs a full scan of the ""hbase:meta"" table for regions. It takes a connection and a visitor as parameters, where the visitor is invoked against each row in the regions family."
8,fullScanRegions(Connection),full scan regions connection performs a full scan of hbase meta for regions param connection connection we re using ,Method,,org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas+testCreateTableWithMultipleReplicas() org.apache.hadoop.hbase.util.hbck.OfflineMetaRebuildTestCore+scanMeta(),"org.apache.hadoop.hbase.MetaTableAccessor+fullScan(Connection,QueryType) org.apache.hadoop.hbase.MetaTableAccessor+fullScan(Connection,QueryType)",,265,7,9,"/** 
 * Performs a full scan of <code>hbase:meta</code> for regions.
 * @param connection connection we're using
 */
","/**
   * Performs a full scan of <code>hbase:meta</code> for regions.
   * @param connection connection we're using
   */
  public static List<Result> fullScanRegions(Connection connection) throws IOException {
    return fullScan(connection, QueryType.REGION);
  }",False,False,"The function performs a full scan of the ""hbase:meta"" table in HBase to retrieve information about regions. It takes a connection as input and returns a list of results.",False,"The given code is a method named ""fullScanRegions"" that performs a full scan of the ""hbase:meta"" table in HBase to retrieve information about regions. It takes a connection object as a parameter and returns a list of Result objects."
9,"fullScanTables(Connection,ClientMetaTableAccessor.Visitor)",full scan tables connection visitor performs a full scan of hbase meta for tables param connection connection we re using param visitor visitor invoked against each row in tables family ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor)",,390,9,9,"/** 
 * Performs a full scan of <code>hbase:meta</code> for tables.
 * @param connection connection we're using
 * @param visitor Visitor invoked against each row in tables family.
 */
","/**
   * Performs a full scan of <code>hbase:meta</code> for tables.
   * @param connection connection we're using
   * @param visitor Visitor invoked against each row in tables family.
   */
  public static void fullScanTables(Connection connection,
    final ClientMetaTableAccessor.Visitor visitor) throws IOException {
    scanMeta(connection, null, null, QueryType.TABLE, visitor);
  }",False,False,"The function of this method is to perform a full scan of the ""hbase:meta"" table for tables. It takes a connection and a visitor as parameters, and invokes the visitor against each row in the tables family.",False,"The function `fullScanTables` performs a full scan of the `hbase:meta` table in HBase for tables. It takes a connection and a visitor as parameters, where the visitor is invoked against each row in the tables family."
10,"fullScan(Connection,QueryType)",full scan connection type performs a full scan of hbase meta param connection connection we re using param type scanned part of meta return list of link result ,Method,fullScanRegions(Connection),,"org.apache.hadoop.hbase.ClientMetaTableAccessor.CollectAllVisitor+CollectAllVisitor() org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.ClientMetaTableAccessor.CollectingVisitor+getResults() org.apache.hadoop.hbase.ClientMetaTableAccessor.CollectingVisitor+getResults()",,459,11,10,"/** 
 * Performs a full scan of <code>hbase:meta</code>.
 * @param connection connection we're using
 * @param type scanned part of meta
 * @return List of {@link Result}
 */
","/**
   * Performs a full scan of <code>hbase:meta</code>.
   * @param connection connection we're using
   * @param type scanned part of meta
   * @return List of {@link Result}
   */
  private static List<Result> fullScan(Connection connection, QueryType type) throws IOException {
    ClientMetaTableAccessor.CollectAllVisitor v = new ClientMetaTableAccessor.CollectAllVisitor();
    scanMeta(connection, null, null, type, v);
    return v.getResults();
  }",False,False,"This method performs a full scan of the ""hbase:meta"" table using the given connection and query type. It returns a list of Result objects obtained from the scan.",False,"The given code is a private method named ""fullScan"" that performs a full scan of the ""hbase:meta"" table. It takes a connection and a query type as parameters and returns a list of Result objects. The method uses the ClientMetaTableAccessor.CollectAllVisitor class to collect all the results from the scan."
11,getMetaHTable(Connection),get meta h table connection callers should call close on the returned link table instance param connection connection we re using to access meta return an link table for hbase meta throws null pointer exception if code connection is code null ,Method,"getRegionLocation(Connection,byte[]) getCatalogFamilyRow(Connection,RegionInfo) getRegionResult(Connection,byte[]) scanByRegionEncodedName(Connection,String) scanMeta(Connection,byte[],byte[],QueryType,Filter,int,ClientMetaTableAccessor.Visitor) getClosestRegionInfo(Connection,TableName,byte[]) getTableState(Connection,TableName) putToMetaTable(Connection,Put) putsToMetaTable(Connection,List<Put>) deleteFromMetaTable(Connection,List<Delete>) addSplitsToParent(Connection,RegionInfo,RegionInfo,RegionInfo) getReplicationBarrierResult(Connection,TableName,byte[],byte[]) getReplicationBarrier(Connection,byte[])",org.apache.hadoop.hbase.master.TestMetaFixer+testMergeWithMergedChildRegion() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationsForRegionReplicas() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsRemovedAtTableDeletion() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsAddedAtTableCreation() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsAddedAtRegionSplit() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsAddedAtRegionMerge() org.apache.hadoop.hbase.TestMetaTableAccessor+testMastersSystemTimeIsUsedInUpdateLocations() org.apache.hadoop.hbase.TestMetaTableAccessor+testMastersSystemTimeIsUsedInMergeRegions() org.apache.hadoop.hbase.TestMetaTableAccessor+testEmptyMetaDaughterLocationDuringSplit() org.apache.hadoop.hbase.util.TestHBaseFsckCleanReplicationBarriers+testCleanReplicationBarrierWithDeletedTable(),"java.util.Objects+requireNonNull(T,String) java.util.Objects+requireNonNull(T,String) org.apache.hadoop.hbase.client.Connection+isClosed() org.apache.hadoop.hbase.client.Connection+isClosed() java.io.IOException+IOException(String) org.apache.hadoop.hbase.client.Connection+getTable(TableName) org.apache.hadoop.hbase.client.Connection+getTable(TableName)",org.apache.hadoop.hbase.TableName+META_TABLE_NAME,665,14,9,"/** 
 * Callers should call close on the returned  {@link Table} instance.
 * @param connection connection we're using to access Meta
 * @return An {@link Table} for <code>hbase:meta</code>
 * @throws NullPointerException if {@code connection} is {@code null}
 */
","/**
   * Callers should call close on the returned {@link Table} instance.
   * @param connection connection we're using to access Meta
   * @return An {@link Table} for <code>hbase:meta</code>
   * @throws NullPointerException if {@code connection} is {@code null}
   */
  public static Table getMetaHTable(final Connection connection) throws IOException {
    // We used to pass whole CatalogTracker in here, now we just pass in Connection
    Objects.requireNonNull(connection, ""Connection cannot be null"");
    if (connection.isClosed()) {
      throw new IOException(""connection is closed"");
    }
    return connection.getTable(TableName.META_TABLE_NAME);
  }",False,False,"This method returns a Table instance for accessing the ""hbase:meta"" table. Callers should remember to close the returned Table instance. It throws a NullPointerException if the connection parameter is null, and an IOException if the connection is closed.",False,This code defines a method called `getMetaHTable` that takes a `Connection` object as a parameter and returns a `Table` object for accessing the `hbase:meta` table. The method checks if the connection is null or closed and throws appropriate exceptions if necessary.
12,"getRegion(Connection,byte[])",get region connection region name gets the region info and assignment for the specified region param connection connection we re using param region name region to lookup return location and region info for region name deprecated use link get region location connection byte instead ,Method,,"org.apache.hadoop.hbase.master.MasterRpcServices+unassignRegion(RpcController,UnassignRegionRequest) org.apache.hadoop.hbase.master.TestMaster+testMasterOpsWhileSplitting() org.apache.hadoop.hbase.TestMetaTableAccessor+testGetRegion() org.apache.hadoop.hbase.TestMetaTableAccessor+testGettingTableRegions(Connection,TableName,int) org.apache.hadoop.hbase.TestMetaTableAccessor+testGetRegion(Connection,RegionInfo)","org.apache.hadoop.hbase.MetaTableAccessor+getRegionLocation(Connection,byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getRegionLocation(Connection,byte[]) org.apache.hadoop.hbase.HRegionLocation+getRegion() org.apache.hadoop.hbase.HRegionLocation+getRegion() org.apache.hadoop.hbase.HRegionLocation+getServerName() org.apache.hadoop.hbase.HRegionLocation+getServerName()",,615,13,9,"/** 
 * Gets the region info and assignment for the specified region.
 * @param connection connection we're using
 * @param regionName Region to lookup.
 * @return Location and RegionInfo for <code>regionName</code>
 * @deprecated use {@link #getRegionLocation(Connection,byte[])} instead
 */
","/**
   * Gets the region info and assignment for the specified region.
   * @param connection connection we're using
   * @param regionName Region to lookup.
   * @return Location and RegionInfo for <code>regionName</code>
   * @deprecated use {@link #getRegionLocation(Connection, byte[])} instead
   */
  @Deprecated
  public static Pair<RegionInfo, ServerName> getRegion(Connection connection, byte[] regionName)
    throws IOException {
    HRegionLocation location = getRegionLocation(connection, regionName);
    return location == null ? null : new Pair<>(location.getRegion(), location.getServerName());
  }",False,False,"This method retrieves the region information and assignment for a specified region. It takes a connection and a region name as parameters, and returns the location and region info for the specified region. It is deprecated and suggests using a different method instead.",False,"This code defines a deprecated method called ""getRegion"" that takes a connection and a region name as input and returns the location and region information for that region. It suggests using the ""getRegionLocation"" method instead."
13,"getRegionLocation(Connection,byte[])",get region location connection region name returns the h region location from meta for the given region param connection connection we re using param region name region we re looking for return h region location for the given region ,Method,"getRegion(Connection,byte[])",org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction+evaluate(),org.apache.hadoop.hbase.CatalogFamilyFormat+parseRegionInfoFromRegionName(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+parseRegionInfoFromRegionName(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getMetaKeyForRegion(RegionInfo) org.apache.hadoop.hbase.CatalogFamilyFormat+getMetaKeyForRegion(RegionInfo) org.apache.hadoop.hbase.client.Get+Get(byte[]) org.apache.hadoop.hbase.client.Get+addFamily(byte[]) org.apache.hadoop.hbase.client.Get+addFamily(byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Table+get(Get) org.apache.hadoop.hbase.client.Table+get(Get) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionLocations(Result) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionLocations(Result) org.apache.hadoop.hbase.RegionLocations+getRegionLocation(int) org.apache.hadoop.hbase.RegionLocations+getRegionLocation(int) org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.client.RegionInfo+getReplicaId(),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,1029,26,9,"/** 
 * Returns the HRegionLocation from meta for the given region
 * @param connection connection we're using
 * @param regionName region we're looking for
 * @return HRegionLocation for the given region
 */
","/**
   * Returns the HRegionLocation from meta for the given region
   * @param connection connection we're using
   * @param regionName region we're looking for
   * @return HRegionLocation for the given region
   */
  public static HRegionLocation getRegionLocation(Connection connection, byte[] regionName)
    throws IOException {
    byte[] row = regionName;
    RegionInfo parsedInfo = null;
    try {
      parsedInfo = CatalogFamilyFormat.parseRegionInfoFromRegionName(regionName);
      row = CatalogFamilyFormat.getMetaKeyForRegion(parsedInfo);
    } catch (Exception parseEx) {
      // Ignore. This is used with tableName passed as regionName.
    }
    Get get = new Get(row);
    get.addFamily(HConstants.CATALOG_FAMILY);
    Result r;
    try (Table t = getMetaHTable(connection)) {
      r = t.get(get);
    }
    RegionLocations locations = CatalogFamilyFormat.getRegionLocations(r);
    return locations == null ? null :
      locations.getRegionLocation(parsedInfo == null ? 0 : parsedInfo.getReplicaId());
  }",False,False,"This method retrieves the location of a specific region from the metadata in HBase. It takes a connection and a region name as input, and returns the HRegionLocation for that region.",False,"This code is a method that retrieves the location of a specific region in HBase. It takes a connection and a region name as input, and returns the HRegionLocation for that region. It first parses the region name, then performs a Get operation on the meta table to retrieve the region's location."
14,"getRegionLocation(Connection,RegionInfo)",get region location connection region info returns the h region location from meta for the given region param connection connection we re using param region info region information return h region location for the given region ,Method,,"org.apache.hadoop.hbase.client.TestMetaWithReplicasShutdownHandling+shutdownMetaAndDoValidations(HBaseTestingUtility) org.apache.hadoop.hbase.client.TestMetaWithReplicasShutdownHandling+shutdownMetaAndDoValidations(HBaseTestingUtility) org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction+blockUntilRegionIsInMeta(Connection,long,RegionInfo)","org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionLocation(Result,RegionInfo,int) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionLocation(Result,RegionInfo,int) org.apache.hadoop.hbase.MetaTableAccessor+getCatalogFamilyRow(Connection,RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+getCatalogFamilyRow(Connection,RegionInfo) org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.client.RegionInfo+getReplicaId()",,476,11,9,"/** 
 * Returns the HRegionLocation from meta for the given region
 * @param connection connection we're using
 * @param regionInfo region information
 * @return HRegionLocation for the given region
 */
","/**
   * Returns the HRegionLocation from meta for the given region
   * @param connection connection we're using
   * @param regionInfo region information
   * @return HRegionLocation for the given region
   */
  public static HRegionLocation getRegionLocation(Connection connection, RegionInfo regionInfo)
    throws IOException {
    return CatalogFamilyFormat.getRegionLocation(getCatalogFamilyRow(connection, regionInfo),
      regionInfo, regionInfo.getReplicaId());
  }",False,False,"This method returns the HRegionLocation for a given region by calling the getRegionLocation method from the CatalogFamilyFormat class, passing in the catalog family row, region information, and replica ID.",False,"This code is a method that retrieves the HRegionLocation for a given region from the meta table. It takes a connection and region information as parameters and returns the HRegionLocation. It uses the CatalogFamilyFormat class to get the region location based on the catalog family row, region information, and replica ID."
15,"getCatalogFamilyRow(Connection,RegionInfo)",get catalog family row connection ri return return the link h constants catalog family row from hbase meta ,Method,"getRegionLocation(Connection,RegionInfo)","org.apache.hadoop.hbase.master.procedure.EnableTableProcedure+getReplicaCountInMeta(Connection,int,List<RegionInfo>)",org.apache.hadoop.hbase.client.Get+Get(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getMetaKeyForRegion(RegionInfo) org.apache.hadoop.hbase.CatalogFamilyFormat+getMetaKeyForRegion(RegionInfo) org.apache.hadoop.hbase.client.Get+addFamily(byte[]) org.apache.hadoop.hbase.client.Get+addFamily(byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Table+get(Get) org.apache.hadoop.hbase.client.Table+get(Get),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,391,11,9,"/** 
 * @return Return the {@link HConstants#CATALOG_FAMILY} row from hbase:meta.
 */
","/**
   * @return Return the {@link HConstants#CATALOG_FAMILY} row from hbase:meta.
   */
  public static Result getCatalogFamilyRow(Connection connection, RegionInfo ri)
    throws IOException {
    Get get = new Get(CatalogFamilyFormat.getMetaKeyForRegion(ri));
    get.addFamily(HConstants.CATALOG_FAMILY);
    try (Table t = getMetaHTable(connection)) {
      return t.get(get);
    }
  }",False,False,This method retrieves the row with the {@link HConstants#CATALOG_FAMILY} from the hbase:meta table using the given connection and region information. It returns the result of the retrieval operation.,False,"This code is a method named ""getCatalogFamilyRow"" that retrieves the row with the HConstants.CATALOG_FAMILY column from the hbase:meta table. It takes a Connection object and a RegionInfo object as parameters, and returns a Result object containing the retrieved row."
16,"getRegionResult(Connection,byte[])",get region result connection region name gets the result in hbase meta for the specified region param connection connection we re using param region name region we re looking for return result of the specified region ,Method,"getMergeRegions(Connection,byte[]) hasMergeRegions(Connection,byte[]) deleteMergeQualifiers(Connection,RegionInfo)",org.apache.hadoop.hbase.master.procedure.TestHBCKSCP+test() org.apache.hadoop.hbase.master.procedure.TestHBCKSCP+test() org.apache.hadoop.hbase.master.procedure.TestHBCKSCP+test() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testCleanMergeReference() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testCleanMergeReference(),org.apache.hadoop.hbase.client.Get+Get(byte[]) org.apache.hadoop.hbase.client.Get+addFamily(byte[]) org.apache.hadoop.hbase.client.Get+addFamily(byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Table+get(Get) org.apache.hadoop.hbase.client.Table+get(Get),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,478,14,9,"/** 
 * Gets the result in hbase:meta for the specified region.
 * @param connection connection we're using
 * @param regionName region we're looking for
 * @return result of the specified region
 */
","/**
   * Gets the result in hbase:meta for the specified region.
   * @param connection connection we're using
   * @param regionName region we're looking for
   * @return result of the specified region
   */
  public static Result getRegionResult(Connection connection, byte[] regionName)
    throws IOException {
    Get get = new Get(regionName);
    get.addFamily(HConstants.CATALOG_FAMILY);
    try (Table t = getMetaHTable(connection)) {
      return t.get(get);
    }
  }",False,False,"This method serves to retrieve the result from the hbase:meta table for a specified region. It takes a connection and a region name as parameters, and returns the result of the specified region.",False,"This code defines a method called ""getRegionResult"" that retrieves the result of a specified region from the hbase:meta table. It takes a connection and a region name as parameters, creates a Get object with the region name, adds the HConstants.CATALOG_FAMILY family to it, and then retrieves the result using the getMetaHTable method. The result is returned as a Result object."
17,"scanByRegionEncodedName(Connection,String)",scan by region encoded name connection region encoded name scans meta table for a row whose key contains the specified region encoded name returning a single related result instance if any row is found null otherwise param connection the connection to query meta table param region encoded name the region encoded name to look for at meta return result instance with the row related info in meta null otherwise throws io exception if any errors occur while querying meta ,Method,,"org.apache.hadoop.hbase.master.assignment.RegionStateStore+visitMetaForRegion(String,RegionStateVisitor) org.apache.hadoop.hbase.TestMetaTableAccessor+testScanByRegionEncodedNameExistingRegion() org.apache.hadoop.hbase.TestMetaTableAccessor+testScanByRegionEncodedNameNonExistingRegion()","org.apache.hadoop.hbase.filter.RowFilter+RowFilter(CompareOperator,ByteArrayComparable) org.apache.hadoop.hbase.filter.SubstringComparator+SubstringComparator(String) org.apache.hadoop.hbase.MetaTableAccessor+getMetaScan(Connection,int) org.apache.hadoop.hbase.MetaTableAccessor+getMetaScan(Connection,int) org.apache.hadoop.hbase.client.Scan+setFilter(Filter) org.apache.hadoop.hbase.client.Scan+setFilter(Filter) org.apache.hadoop.hbase.client.Table+getScanner(Scan) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Table+getScanner(Scan) org.apache.hadoop.hbase.client.ResultScanner+next() org.apache.hadoop.hbase.client.ResultScanner+next()",,916,17,9,"/** 
 * Scans META table for a row whose key contains the specified <B>regionEncodedName</B>, returning a single related <code>Result</code> instance if any row is found, null otherwise.
 * @param connection the connection to query META table.
 * @param regionEncodedName the region encoded name to look for at META.
 * @return <code>Result</code> instance with the row related info in META, null otherwise.
 * @throws IOException if any errors occur while querying META.
 */
","/**
   * Scans META table for a row whose key contains the specified <B>regionEncodedName</B>, returning
   * a single related <code>Result</code> instance if any row is found, null otherwise.
   * @param connection the connection to query META table.
   * @param regionEncodedName the region encoded name to look for at META.
   * @return <code>Result</code> instance with the row related info in META, null otherwise.
   * @throws IOException if any errors occur while querying META.
   */
  public static Result scanByRegionEncodedName(Connection connection, String regionEncodedName)
    throws IOException {
    RowFilter rowFilter =
      new RowFilter(CompareOperator.EQUAL, new SubstringComparator(regionEncodedName));
    Scan scan = getMetaScan(connection, 1);
    scan.setFilter(rowFilter);
    ResultScanner resultScanner = getMetaHTable(connection).getScanner(scan);
    return resultScanner.next();
  }",False,False,"The function of this method is to scan the META table for a row whose key contains the specified regionEncodedName and return a single related Result instance if any row is found, or null otherwise. It takes a connection to query the META table and the region encoded name to look for at META as parameters. It throws an IOException if any errors occur while querying META.",False,"This code defines a method called `scanByRegionEncodedName` that scans a META table for a row whose key contains a specified region encoded name. It returns a single `Result` instance if a matching row is found, otherwise it returns null. The method uses a row filter and a scanner to query the META table."
18,"getMergeRegions(Connection,byte[])",get merge regions connection region name return return all regioninfos listed in the info merge columns of the region name row ,Method,,"org.apache.hadoop.hbase.master.assignment.GCMultipleMergedRegionsProcedure+executeFromState(MasterProcedureEnv,GCMergedRegionsState) org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure+testMerge(TableName,int) org.apache.hadoop.hbase.master.TestMetaFixer+testOverlapWithSmallMergeCount() org.apache.hadoop.hbase.master.TestMetaFixer+testOverlapWithSmallMergeCount() org.apache.hadoop.hbase.TestMetaTableAccessor+testGetMergeRegions() org.apache.hadoop.hbase.TestMetaTableAccessor+testGetMergeRegions() org.apache.hadoop.hbase.TestMetaTableAccessor+testGetMergeRegions() org.apache.hadoop.hbase.TestMetaTableAccessor+testGetMergeRegions() org.apache.hadoop.hbase.TestSplitMerge+testMergeRegionOrder()","org.apache.hadoop.hbase.MetaTableAccessor+getMergeRegions(Cell[]) org.apache.hadoop.hbase.MetaTableAccessor+getMergeRegions(Cell[]) org.apache.hadoop.hbase.client.Result+rawCells() org.apache.hadoop.hbase.MetaTableAccessor+getRegionResult(Connection,byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getRegionResult(Connection,byte[]) org.apache.hadoop.hbase.client.Result+rawCells()",,342,9,9,"/** 
 * @return Return all regioninfos listed in the 'info:merge*' columns of the<code>regionName</code> row.
 */
","/**
   * @return Return all regioninfos listed in the 'info:merge*' columns of the
   *         <code>regionName</code> row.
   */
  @Nullable
  public static List<RegionInfo> getMergeRegions(Connection connection, byte[] regionName)
    throws IOException {
    return getMergeRegions(getRegionResult(connection, regionName).rawCells());
  }",False,False,"The function serves to retrieve all regioninfos listed in the 'info:merge*' columns of a specific regionName row. It takes a connection and regionName as input, and returns a list of RegionInfo objects.",False,"The given code is a method named ""getMergeRegions"" that takes a connection and a region name as input parameters. It retrieves the merge regions listed in the 'info:merge*' columns of the specified region's row and returns them as a list of RegionInfo objects."
19,"hasMergeRegions(Connection,byte[])",has merge regions conn region name check whether the given code region name has any info merge columns ,Method,,org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure+prepareMergeRegion(MasterProcedureEnv),"org.apache.hadoop.hbase.MetaTableAccessor+hasMergeRegions(Cell[]) org.apache.hadoop.hbase.MetaTableAccessor+hasMergeRegions(Cell[]) org.apache.hadoop.hbase.client.Result+rawCells() org.apache.hadoop.hbase.MetaTableAccessor+getRegionResult(Connection,byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getRegionResult(Connection,byte[]) org.apache.hadoop.hbase.client.Result+rawCells()",,263,6,9,"/** 
 * Check whether the given  {@code regionName} has any 'info:merge*' columns.
 */
","/**
   * Check whether the given {@code regionName} has any 'info:merge*' columns.
   */
  public static boolean hasMergeRegions(Connection conn, byte[] regionName) throws IOException {
    return hasMergeRegions(getRegionResult(conn, regionName).rawCells());
  }",False,False,The function of the method is to check whether a given region has any 'info:merge*' columns by calling the method `hasMergeRegions` with the `rawCells` of the region result obtained from the `getRegionResult` method.,False,"The given code is a method named ""hasMergeRegions"" that takes a connection object and a byte array representing a region name as input. It returns a boolean value indicating whether the specified region has any columns with names starting with ""info:merge"". The method internally calls another method named ""getRegionResult"" to retrieve the raw cells of the region and passes them to a helper method named ""hasMergeRegions"" to perform the actual check."
20,getMergeRegionsWithName(Cell[]),get merge regions with name cells return deserialized values of lt qualifier regioninfo gt pairs taken from column values thatmatch the regex info merge in array of cells ,Method,getMergeRegions(Cell[]),"org.apache.hadoop.hbase.master.webapp.RegionReplicaInfo+RegionReplicaInfo(Result,HRegionLocation)","org.apache.hadoop.hbase.MetaTableAccessor+isMergeQualifierPrefix(Cell) org.apache.hadoop.hbase.MetaTableAccessor+isMergeQualifierPrefix(Cell) org.apache.hadoop.hbase.client.RegionInfo+parseFromOrNull(byte[],int,int) org.apache.hadoop.hbase.client.RegionInfo+parseFromOrNull(byte[],int,int) org.apache.hadoop.hbase.Cell+getValueArray() org.apache.hadoop.hbase.Cell+getValueArray() org.apache.hadoop.hbase.Cell+getValueOffset() org.apache.hadoop.hbase.Cell+getValueOffset() org.apache.hadoop.hbase.Cell+getValueLength() org.apache.hadoop.hbase.Cell+getValueLength() java.util.Map+put(K,V) java.util.Map+put(K,V) org.apache.hadoop.hbase.util.Bytes+toString(byte[]) org.apache.hadoop.hbase.util.Bytes+toString(byte[]) org.apache.hadoop.hbase.CellUtil+cloneQualifier(Cell) org.apache.hadoop.hbase.CellUtil+cloneQualifier(Cell)",,906,26,9,"/** 
 * @return Deserialized values of &lt;qualifier,regioninfo&gt; pairs taken from column values thatmatch the regex 'info:merge.*' in array of <code>cells</code>.
 */
","/**
   * @return Deserialized values of &lt;qualifier,regioninfo&gt; pairs taken from column values that
   *         match the regex 'info:merge.*' in array of <code>cells</code>.
   */
  @Nullable
  public static Map<String, RegionInfo> getMergeRegionsWithName(Cell[] cells) {
    if (cells == null) {
      return null;
    }
    Map<String, RegionInfo> regionsToMerge = null;
    for (Cell cell : cells) {
      if (!isMergeQualifierPrefix(cell)) {
        continue;
      }
      // Ok. This cell is that of a info:merge* column.
      RegionInfo ri = RegionInfo.parseFromOrNull(cell.getValueArray(), cell.getValueOffset(),
        cell.getValueLength());
      if (ri != null) {
        if (regionsToMerge == null) {
          regionsToMerge = new LinkedHashMap<>();
        }
        regionsToMerge.put(Bytes.toString(CellUtil.cloneQualifier(cell)), ri);
      }
    }
    return regionsToMerge;
  }",False,True,"This method deserializes values of <qualifier, regioninfo> pairs from column values that match the regex 'info:merge.*' in the given array of cells. It returns a map of these deserialized values, where the qualifier is the key and the regioninfo is the value.",False,"This code is a method named ""getMergeRegionsWithName"" that takes an array of cells as input and returns a map of deserialized values. It iterates through the cells, checks if they match a specific regex pattern, and if so, extracts the region information and adds it to the map. The map is then returned as the result."
21,getMergeRegions(Cell[]),get merge regions cells return deserialized regioninfo values taken from column values that match the regex info merge in array of cells ,Method,"getMergeRegions(Connection,byte[])",org.apache.hadoop.hbase.master.CatalogJanitor+scan() org.apache.hadoop.hbase.master.HbckChore+scanForMergedParentRegions() org.apache.hadoop.hbase.util.HBaseFsck+visit(Result) org.apache.hadoop.hbase.master.TestMetaFixer+testOverlap() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testCleanMergeReference(),org.apache.hadoop.hbase.MetaTableAccessor+getMergeRegionsWithName(Cell[]) org.apache.hadoop.hbase.MetaTableAccessor+getMergeRegionsWithName(Cell[]) java.util.Map+values() java.util.Map+values(),,422,9,9,"/** 
 * @return Deserialized regioninfo values taken from column values that match the regex'info:merge.*' in array of <code>cells</code>.
 */
","/**
   * @return Deserialized regioninfo values taken from column values that match the regex
   *         'info:merge.*' in array of <code>cells</code>.
   */
  @Nullable
  public static List<RegionInfo> getMergeRegions(Cell[] cells) {
    Map<String, RegionInfo> mergeRegionsWithName = getMergeRegionsWithName(cells);
    return (mergeRegionsWithName == null) ? null : new ArrayList<>(mergeRegionsWithName.values());
  }",False,True,This method returns a list of deserialized regioninfo values from the given array of cells. It matches the regex 'info:merge.*' in the column values of the cells and returns the corresponding regioninfo values.,False,"This code is a method named ""getMergeRegions"" that takes an array of cells as input. It deserializes the regioninfo values from the cells that match the regex pattern 'info:merge.*' and returns a list of RegionInfo objects."
22,hasMergeRegions(Cell[]),has merge regions cells return true if any merge regions present in cells i e the column in cell matches the regex info merge ,Method,"hasMergeRegions(Connection,byte[])","org.apache.hadoop.hbase.master.CatalogJanitor+cleanParent(RegionInfo,Result) org.apache.hadoop.hbase.master.CatalogJanitor+ReportMakingVisitor.visit(Result) org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testCleanMergeReference() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testCleanMergeReference() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testCleanMergeReference()",org.apache.hadoop.hbase.MetaTableAccessor+isMergeQualifierPrefix(Cell) org.apache.hadoop.hbase.MetaTableAccessor+isMergeQualifierPrefix(Cell),,365,13,9,"/** 
 * @return True if any merge regions present in <code>cells</code>; i.e. the column in<code>cell</code> matches the regex 'info:merge.*'.
 */
","/**
   * @return True if any merge regions present in <code>cells</code>; i.e. the column in
   *         <code>cell</code> matches the regex 'info:merge.*'.
   */
  public static boolean hasMergeRegions(Cell[] cells) {
    for (Cell cell : cells) {
      if (!isMergeQualifierPrefix(cell)) {
        continue;
      }
      return true;
    }
    return false;
  }",False,True,"The function checks if any merge regions are present in the given array of cells. It iterates through each cell and returns true if the column in the cell matches the regex 'info:merge.*', indicating a merge region. If no merge regions are found, it returns false.",False,"The code is a method named ""hasMergeRegions"" that takes an array of cells as input and returns a boolean value. It checks if any of the cells have a column that matches the regex pattern 'info:merge.*', indicating the presence of merge regions."
23,isMergeQualifierPrefix(Cell),is merge qualifier prefix cell return true if the column in cell matches the regex info merge ,Method,"getMergeRegionsWithName(Cell[]) hasMergeRegions(Cell[]) deleteMergeQualifiers(Connection,RegionInfo)",,"org.apache.hadoop.hbase.CellUtil+matchingFamily(Cell,byte[]) org.apache.hadoop.hbase.CellUtil+matchingFamily(Cell,byte[]) org.apache.hadoop.hbase.PrivateCellUtil+qualifierStartsWith(Cell,byte[]) org.apache.hadoop.hbase.PrivateCellUtil+qualifierStartsWith(Cell,byte[])",org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+MERGE_QUALIFIER_PREFIX,409,8,10,"/** 
 * @return True if the column in <code>cell</code> matches the regex 'info:merge.*'.
 */
","/**
   * @return True if the column in <code>cell</code> matches the regex 'info:merge.*'.
   */
  private static boolean isMergeQualifierPrefix(Cell cell) {
    // Check to see if has family and that qualifier starts with the merge qualifier 'merge'
    return CellUtil.matchingFamily(cell, HConstants.CATALOG_FAMILY) &&
      PrivateCellUtil.qualifierStartsWith(cell, HConstants.MERGE_QUALIFIER_PREFIX);
  }",False,True,The function of the method isMergeQualifierPrefix is to check if the column in the given cell matches the regex 'info:merge.*'. It returns true if the cell has the specified family and the qualifier starts with the merge qualifier 'merge'.,False,"The method ""isMergeQualifierPrefix"" checks if the column in the given cell matches the regex pattern 'info:merge.*'. It returns true if the cell's family matches the catalog family and the qualifier starts with the merge qualifier prefix 'merge'."
24,"getAllRegions(Connection,boolean)",get all regions connection exclude offlined split parents lists all of the regions currently in meta param connection to connect with param exclude offlined split parents false if we are to include offlined splitparents regions true and we ll leave out offlined regions from returned list return list of all user space regions ,Method,,org.apache.hadoop.hbase.util.HBaseFsck+checkRegionBoundaries() org.apache.hadoop.hbase.master.TestClusterRestart+test() org.apache.hadoop.hbase.master.TestClusterRestart+test() org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction+RegionChecker.verifyRegionsUsingMetaTableAccessor(),"org.apache.hadoop.hbase.MetaTableAccessor+getTableRegionsAndLocations(Connection,TableName,boolean) org.apache.hadoop.hbase.MetaTableAccessor+getTableRegionsAndLocations(Connection,TableName,boolean) org.apache.hadoop.hbase.MetaTableAccessor+getListOfRegionInfos(List<Pair<RegionInfo,ServerName>>) org.apache.hadoop.hbase.MetaTableAccessor+getListOfRegionInfos(List<Pair<RegionInfo,ServerName>>)",,652,17,9,"/** 
 * Lists all of the regions currently in META.
 * @param connection to connect with
 * @param excludeOfflinedSplitParents False if we are to include offlined/splitparents regions,true and we'll leave out offlined regions from returned list
 * @return List of all user-space regions.
 */
","/**
   * Lists all of the regions currently in META.
   * @param connection to connect with
   * @param excludeOfflinedSplitParents False if we are to include offlined/splitparents regions,
   *          true and we'll leave out offlined regions from returned list
   * @return List of all user-space regions.
   */
  @VisibleForTesting
  public static List<RegionInfo> getAllRegions(Connection connection,
    boolean excludeOfflinedSplitParents) throws IOException {
    List<Pair<RegionInfo, ServerName>> result;

    result = getTableRegionsAndLocations(connection, null, excludeOfflinedSplitParents);

    return getListOfRegionInfos(result);

  }",False,False,"The function of this method is to list all of the regions currently in META. It takes a connection parameter to connect with, and a boolean parameter to determine whether to include offlined/splitparents regions or not. It returns a list of all user-space regions.",False,"This code defines a method called ""getAllRegions"" that takes a connection and a boolean parameter as input. It returns a list of all user-space regions by calling the ""getTableRegionsAndLocations"" method and converting the result into a list of RegionInfo objects. The method is marked as visible for testing purposes."
25,"getTableRegions(Connection,TableName)",get table regions connection table name gets all of the regions of the specified table do not use this method to get meta table regions use methods in meta table locator instead param connection connection we re using param table name table we re looking for return ordered list of link region info ,Method,,"org.apache.hadoop.hbase.backup.util.BackupUtils+copyTableRegionInfo(Connection,BackupInfo,Configuration) org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure+updateReplicaColumnsIfNeeded(MasterProcedureEnv,TableDescriptor,TableDescriptor) org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier+verifyRegions(SnapshotManifest) org.apache.hadoop.hbase.client.TestMetaWithReplicasShutdownHandling+shutdownMetaAndDoValidations(HBaseTestingUtility) org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure+testMerge(TableName,int) org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure+testMerge(TableName,int) org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas+testCreateTableWithSingleReplica() org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas+testCreateTableWithMultipleReplicas() org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas+testCreateTableWithMultipleReplicas() org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas+testIncompleteMetaTableReplicaInformation() org.apache.hadoop.hbase.master.TestMetaFixer+testPlugsHolesWithReadReplicaInternal(TableName,int) org.apache.hadoop.hbase.master.TestMetaFixer+testPlugsHolesWithReadReplicaInternal(TableName,int) org.apache.hadoop.hbase.master.TestMetaFixer+testOneRegionTable() org.apache.hadoop.hbase.master.TestMetaFixer+testOneRegionTable() org.apache.hadoop.hbase.master.TestMetaFixer+testOneRegionTable() org.apache.hadoop.hbase.master.TestMetaFixer+testOverlapCommon(TableName) org.apache.hadoop.hbase.master.TestMetaFixer+testMergeWithMergedChildRegion() org.apache.hadoop.hbase.master.TestMetaFixer+testOverlapWithMergeOfNonContiguous() org.apache.hadoop.hbase.quotas.TestSpaceQuotaBasicFunctioning+evaluate() org.apache.hadoop.hbase.quotas.TestSpaceQuotaDropTable+evaluate() org.apache.hadoop.hbase.TestMetaTableAccessor+testScanMetaForTable() org.apache.hadoop.hbase.TestMetaTableAccessor+testScanMetaForTable() org.apache.hadoop.hbase.TestMetaTableAccessor+testGettingTableRegions(Connection,TableName,int) org.apache.hadoop.hbase.TestSplitMerge+testMergeRegionOrder()","org.apache.hadoop.hbase.MetaTableAccessor+getTableRegions(Connection,TableName,boolean) org.apache.hadoop.hbase.MetaTableAccessor+getTableRegions(Connection,TableName,boolean)",,479,11,9,"/** 
 * Gets all of the regions of the specified table. Do not use this method to get meta table regions, use methods in MetaTableLocator instead.
 * @param connection connection we're using
 * @param tableName table we're looking for
 * @return Ordered list of {@link RegionInfo}.
 */
","/**
   * Gets all of the regions of the specified table. Do not use this method to get meta table
   * regions, use methods in MetaTableLocator instead.
   * @param connection connection we're using
   * @param tableName table we're looking for
   * @return Ordered list of {@link RegionInfo}.
   */
  public static List<RegionInfo> getTableRegions(Connection connection, TableName tableName)
    throws IOException {
    return getTableRegions(connection, tableName, false);
  }",False,False,This method is used to retrieve all the regions of a specified table. It takes a connection and table name as parameters and returns an ordered list of RegionInfo objects.,False,"This code defines a method called ""getTableRegions"" that takes a connection and table name as input and returns an ordered list of RegionInfo objects. It is used to retrieve all regions of a specified table, excluding meta table regions."
26,"getTableRegions(Connection,TableName,boolean)",get table regions connection table name exclude offlined split parents gets all of the regions of the specified table do not use this method to get meta table regions use methods in meta table locator instead param connection connection we re using param table name table we re looking for param exclude offlined split parents if true do not include offlined split parents in thereturn return ordered list of link region info ,Method,"getTableRegions(Connection,TableName)",org.apache.hadoop.hbase.namespace.NamespaceAuditor+initialize() org.apache.hadoop.hbase.quotas.QuotaCache+QuotaRefresherChore.updateQuotaFactors() org.apache.hadoop.hbase.client.TestAdmin1+testSplitShouldNotHappenIfSplitIsDisabledForTable() org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction+RegionSplitter.run() org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction+RegionChecker.verifyRegionsUsingMetaTableAccessor(),"org.apache.hadoop.hbase.MetaTableAccessor+getTableRegionsAndLocations(Connection,TableName,boolean) org.apache.hadoop.hbase.MetaTableAccessor+getTableRegionsAndLocations(Connection,TableName,boolean) org.apache.hadoop.hbase.MetaTableAccessor+getListOfRegionInfos(List<Pair<RegionInfo,ServerName>>) org.apache.hadoop.hbase.MetaTableAccessor+getListOfRegionInfos(List<Pair<RegionInfo,ServerName>>)",,756,15,9,"/** 
 * Gets all of the regions of the specified table. Do not use this method to get meta table regions, use methods in MetaTableLocator instead.
 * @param connection connection we're using
 * @param tableName table we're looking for
 * @param excludeOfflinedSplitParents If true, do not include offlined split parents in thereturn.
 * @return Ordered list of {@link RegionInfo}.
 */
","/**
   * Gets all of the regions of the specified table. Do not use this method to get meta table
   * regions, use methods in MetaTableLocator instead.
   * @param connection connection we're using
   * @param tableName table we're looking for
   * @param excludeOfflinedSplitParents If true, do not include offlined split parents in the
   *          return.
   * @return Ordered list of {@link RegionInfo}.
   */
  public static List<RegionInfo> getTableRegions(Connection connection, TableName tableName,
    final boolean excludeOfflinedSplitParents) throws IOException {
    List<Pair<RegionInfo, ServerName>> result =
      getTableRegionsAndLocations(connection, tableName, excludeOfflinedSplitParents);
    return getListOfRegionInfos(result);
  }",False,False,"This method serves to get all the regions of a specified table, excluding offlined split parents. It takes a connection, table name, and a boolean flag as parameters, and returns an ordered list of RegionInfo objects.",False,"This code defines a method called ""getTableRegions"" that takes a connection, table name, and a boolean flag as input. It retrieves a list of region information for the specified table, excluding offlined split parents, and returns it as an ordered list of RegionInfo objects."
27,"getListOfRegionInfos(List<Pair<RegionInfo,ServerName>>)",get list of region infos pairs ,Method,"getAllRegions(Connection,boolean) getTableRegions(Connection,TableName,boolean)",,java.util.List+isEmpty() java.util.List+isEmpty() java.util.Collections+emptyList() java.util.Collections+emptyList() java.util.List+size() java.util.List+size() java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.util.Pair+getFirst() org.apache.hadoop.hbase.util.Pair+getFirst(),,373,11,10,,"private static List<RegionInfo>
    getListOfRegionInfos(final List<Pair<RegionInfo, ServerName>> pairs) {
    if (pairs == null || pairs.isEmpty()) {
      return Collections.emptyList();
    }
    List<RegionInfo> result = new ArrayList<>(pairs.size());
    for (Pair<RegionInfo, ServerName> pair : pairs) {
      result.add(pair.getFirst());
    }
    return result;
  }",False,False,"This method takes a list of pairs containing RegionInfo and ServerName objects. It returns a list of RegionInfo objects by extracting the first element from each pair. If the input list is null or empty, it returns an empty list.",False,"This code defines a private static method named ""getListOfRegionInfos"" that takes a list of pairs containing RegionInfo and ServerName objects as input. It returns a list of RegionInfo objects extracted from the input pairs. If the input list is null or empty, it returns an empty list."
28,"getScanForTableName(Connection,TableName)",get scan for table name connection table name this method creates a scan object that will only scan catalog rows that belong to the specified table it doesn t specify any columns this is a better alternative to just using a start row and scan until it hits a new table since that requires parsing the hri to get the table name param table name bytes of table s name return configured scan object deprecated this is internal so please remove it when we get a chance ,Method,,"org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure+cleanRegionsInMeta(MasterProcedureEnv,TableName) org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure+updateReplicaColumnsIfNeeded(MasterProcedureEnv,TableDescriptor,TableDescriptor) org.apache.hadoop.hbase.client.TestEnableTable+testDeleteForSureClearsAllTableRowsFromMeta() org.apache.hadoop.hbase.client.TestEnableTable+testDeleteForSureClearsAllTableRowsFromMeta()","org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.MetaTableAccessor+getMetaScan(Connection,int) org.apache.hadoop.hbase.MetaTableAccessor+getMetaScan(Connection,int) org.apache.hadoop.hbase.client.Scan+withStartRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStartRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStopRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStopRow(byte[])",,1019,20,9,"/** 
 * This method creates a Scan object that will only scan catalog rows that belong to the specified table. It doesn't specify any columns. This is a better alternative to just using a start row and scan until it hits a new table since that requires parsing the HRI to get the table name.
 * @param tableName bytes of table's name
 * @return configured Scan object
 * @deprecated This is internal so please remove it when we get a chance.
 */
","/**
   * This method creates a Scan object that will only scan catalog rows that belong to the specified
   * table. It doesn't specify any columns. This is a better alternative to just using a start row
   * and scan until it hits a new table since that requires parsing the HRI to get the table name.
   * @param tableName bytes of table's name
   * @return configured Scan object
   * @deprecated This is internal so please remove it when we get a chance.
   */
  @Deprecated
  public static Scan getScanForTableName(Connection connection, TableName tableName) {
    // Start key is just the table name with delimiters
    byte[] startKey = ClientMetaTableAccessor.getTableStartRowForMeta(tableName, QueryType.REGION);
    // Stop key appends the smallest possible char to the table name
    byte[] stopKey = ClientMetaTableAccessor.getTableStopRowForMeta(tableName, QueryType.REGION);

    Scan scan = getMetaScan(connection, -1);
    scan.withStartRow(startKey);
    scan.withStopRow(stopKey);
    return scan;
  }",False,False,"This method creates a Scan object that is configured to scan catalog rows belonging to a specified table. It does not specify any columns. It is a better alternative to using a start row and scanning until it hits a new table, as it avoids the need to parse the HRI to get the table name.",False,"This code defines a deprecated method called ""getScanForTableName"" that creates a Scan object for scanning catalog rows belonging to a specified table. It sets the start and stop keys for the scan based on the table name and returns the configured Scan object."
29,"getMetaScan(Connection,int)",get meta scan connection row upper limit ,Method,"scanByRegionEncodedName(Connection,String) getScanForTableName(Connection,TableName) scanMeta(Connection,byte[],byte[],QueryType,Filter,int,ClientMetaTableAccessor.Visitor) getClosestRegionInfo(Connection,TableName,byte[])",,org.apache.hadoop.hbase.client.Scan+Scan() org.apache.hadoop.hbase.client.Connection+getConfiguration() org.apache.hadoop.hbase.client.Connection+getConfiguration() org.apache.hadoop.hbase.client.Connection+getConfiguration() org.apache.hadoop.hbase.client.Connection+getConfiguration() org.apache.hadoop.hbase.client.Scan+setConsistency(Consistency) org.apache.hadoop.hbase.client.Scan+setConsistency(Consistency) org.apache.hadoop.hbase.client.Scan+setLimit(int) org.apache.hadoop.hbase.client.Scan+setLimit(int) org.apache.hadoop.hbase.client.Scan+setReadType(ReadType) org.apache.hadoop.hbase.client.Scan+setReadType(ReadType) org.apache.hadoop.hbase.client.Scan+setCaching(int) org.apache.hadoop.hbase.client.Scan+setCaching(int),org.apache.hadoop.hbase.HConstants+HBASE_META_SCANNER_CACHING org.apache.hadoop.hbase.HConstants+DEFAULT_HBASE_META_SCANNER_CACHING org.apache.hadoop.hbase.HConstants+USE_META_REPLICAS org.apache.hadoop.hbase.HConstants+DEFAULT_USE_META_REPLICAS,613,15,10,,"private static Scan getMetaScan(Connection connection, int rowUpperLimit) {
    Scan scan = new Scan();
    int scannerCaching = connection.getConfiguration().getInt(HConstants.HBASE_META_SCANNER_CACHING,
      HConstants.DEFAULT_HBASE_META_SCANNER_CACHING);
    if (connection.getConfiguration().getBoolean(HConstants.USE_META_REPLICAS,
      HConstants.DEFAULT_USE_META_REPLICAS)) {
      scan.setConsistency(Consistency.TIMELINE);
    }
    if (rowUpperLimit > 0) {
      scan.setLimit(rowUpperLimit);
      scan.setReadType(Scan.ReadType.PREAD);
    }
    scan.setCaching(scannerCaching);
    return scan;
  }",False,False,"This method returns a Scan object with specific configurations. It sets the caching size, consistency level, limit, and read type of the scan based on the provided parameters.",False,"This code defines a method called ""getMetaScan"" that takes a connection and a rowUpperLimit as parameters and returns a Scan object. The method sets various properties of the Scan object based on the configuration and input parameters, such as caching, consistency, limit, and read type."
30,"getTableRegionsAndLocations(Connection,TableName)",get table regions and locations connection table name do not use this method to get meta table regions use methods in meta table locator instead param connection connection we re using param table name table we re looking for return return list of regioninfos and server ,Method,,"org.apache.hadoop.hbase.tool.CanaryTool+RegionMonitor.checkWriteTableDistribution() org.apache.hadoop.hbase.client.TestAdmin1+testSplitAndMergeWithReplicaTable() org.apache.hadoop.hbase.HBaseTestingUtility+explainTableAvailability(TableName) org.apache.hadoop.hbase.master.procedure.TestHBCKSCP+searchMeta(HMaster,ServerName) org.apache.hadoop.hbase.master.TestMaster+testMasterOpsWhileSplitting() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testCleanMergeReference() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testMergeWithReplicas() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+testMergeWithReplicas() org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+requestMergeRegion(HMaster,TableName,int,int) org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+waitAndVerifyRegionNum(HMaster,TableName,int) org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+waitAndVerifyRegionNum(HMaster,TableName,int) org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster+createTableAndLoadData(HMaster,TableName,int,int) org.apache.hadoop.hbase.TestPartialResultsFromClientSide+moveRegion(Table,int)","org.apache.hadoop.hbase.MetaTableAccessor+getTableRegionsAndLocations(Connection,TableName,boolean) org.apache.hadoop.hbase.MetaTableAccessor+getTableRegionsAndLocations(Connection,TableName,boolean)",,470,10,9,"/** 
 * Do not use this method to get meta table regions, use methods in MetaTableLocator instead.
 * @param connection connection we're using
 * @param tableName table we're looking for
 * @return Return list of regioninfos and server.
 */
","/**
   * Do not use this method to get meta table regions, use methods in MetaTableLocator instead.
   * @param connection connection we're using
   * @param tableName table we're looking for
   * @return Return list of regioninfos and server.
   */
  public static List<Pair<RegionInfo, ServerName>>
    getTableRegionsAndLocations(Connection connection, TableName tableName) throws IOException {
    return getTableRegionsAndLocations(connection, tableName, true);
  }",False,False,This method returns a list of region information and server names for a given table using the provided connection. It is recommended to use methods in MetaTableLocator instead of this method to get meta table regions.,False,"This code defines a method called ""getTableRegionsAndLocations"" that takes a connection and a table name as input parameters. It returns a list of pairs, where each pair contains a region info and the server name where the region is located. The method is used to retrieve the regions and their corresponding server locations for a given table."
31,"getTableRegionsAndLocations(Connection,TableName,boolean)",get table regions and locations connection table name exclude offlined split parents do not use this method to get meta table regions use methods in meta table locator instead param connection connection we re using param table name table to work with can be null for getting all regions param exclude offlined split parents don t return split parents return return list of regioninfos and server addresses ,Method,"getAllRegions(Connection,boolean) getTableRegions(Connection,TableName,boolean) getTableRegionsAndLocations(Connection,TableName)",org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler+process() org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager+execProcedure(ProcedureDescription) org.apache.hadoop.hbase.master.TestMaster+testMasterOpsWhileSplitting(),"org.apache.hadoop.hbase.TableName+equals(Object) org.apache.hadoop.hbase.TableName+equals(Object) java.io.IOException+IOException(String) org.apache.hadoop.hbase.ClientMetaTableAccessor.CollectRegionLocationsVisitor+CollectRegionLocationsVisitor(boolean) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor.CollectingVisitor+getResults() org.apache.hadoop.hbase.ClientMetaTableAccessor.CollectingVisitor+getResults()",org.apache.hadoop.hbase.TableName+META_TABLE_NAME,1360,24,9,"/** 
 * Do not use this method to get meta table regions, use methods in MetaTableLocator instead.
 * @param connection connection we're using
 * @param tableName table to work with, can be null for getting all regions
 * @param excludeOfflinedSplitParents don't return split parents
 * @return Return list of regioninfos and server addresses.
 */
","/**
   * Do not use this method to get meta table regions, use methods in MetaTableLocator instead.
   * @param connection connection we're using
   * @param tableName table to work with, can be null for getting all regions
   * @param excludeOfflinedSplitParents don't return split parents
   * @return Return list of regioninfos and server addresses.
   */
  // What happens here when 1M regions in hbase:meta? This won't scale?
  public static List<Pair<RegionInfo, ServerName>> getTableRegionsAndLocations(
    Connection connection, @Nullable final TableName tableName,
    final boolean excludeOfflinedSplitParents) throws IOException {
    if (tableName != null && tableName.equals(TableName.META_TABLE_NAME)) {
      throw new IOException(
        ""This method can't be used to locate meta regions;"" + "" use MetaTableLocator instead"");
    }
    // Make a version of CollectingVisitor that collects RegionInfo and ServerAddress
    ClientMetaTableAccessor.CollectRegionLocationsVisitor visitor =
      new ClientMetaTableAccessor.CollectRegionLocationsVisitor(excludeOfflinedSplitParents);
    scanMeta(connection,
      ClientMetaTableAccessor.getTableStartRowForMeta(tableName, QueryType.REGION),
      ClientMetaTableAccessor.getTableStopRowForMeta(tableName, QueryType.REGION), QueryType.REGION,
      visitor);
    return visitor.getResults();
  }",False,False,This method is used to get the list of regioninfos and server addresses for a given table. It excludes split parents and does not work for meta table regions.,False,"This code defines a method called ""getTableRegionsAndLocations"" that retrieves a list of region information and server addresses for a given table in HBase. It checks if the table is the meta table and throws an exception if so. The method uses a visitor pattern to scan the meta table and collect the desired information."
32,fullScanMetaAndPrint(Connection),full scan meta and print connection ,Method,,"org.apache.hadoop.hbase.client.TestAdmin3+testGetTableDescriptor() org.apache.hadoop.hbase.client.TestScannerTimeout+test3686a() org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure+testMerge(TableName,int) org.apache.hadoop.hbase.TestRegionRebalancing+testRebalanceOnRegionServerNumberChange() org.apache.hadoop.hbase.util.hbck.OfflineMetaRebuildTestCore+scanMeta()","org.apache.hadoop.hbase.client.Result+isEmpty() org.apache.hadoop.hbase.client.Result+isEmpty() org.apache.hadoop.hbase.CatalogFamilyFormat+getTableState(Result) org.apache.hadoop.hbase.CatalogFamilyFormat+getTableState(Result) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionLocations(Result) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionLocations(Result) org.apache.hadoop.hbase.RegionLocations+getRegionLocations() org.apache.hadoop.hbase.RegionLocations+getRegionLocations() org.apache.hadoop.hbase.HRegionLocation+getRegion() org.apache.hadoop.hbase.HRegionLocation+getRegion() org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor)",org.apache.hadoop.hbase.MetaTableAccessor+LOG org.apache.hadoop.hbase.MetaTableAccessor+LOG org.apache.hadoop.hbase.MetaTableAccessor+LOG,865,24,9,,"public static void fullScanMetaAndPrint(Connection connection) throws IOException {
    ClientMetaTableAccessor.Visitor v = r -> {
      if (r == null || r.isEmpty()) {
        return true;
      }
      LOG.info(""fullScanMetaAndPrint.Current Meta Row: "" + r);
      TableState state = CatalogFamilyFormat.getTableState(r);
      if (state != null) {
        LOG.info(""fullScanMetaAndPrint.Table State={}"" + state);
      } else {
        RegionLocations locations = CatalogFamilyFormat.getRegionLocations(r);
        if (locations == null) {
          return true;
        }
        for (HRegionLocation loc : locations.getRegionLocations()) {
          if (loc != null) {
            LOG.info(""fullScanMetaAndPrint.HRI Print={}"", loc.getRegion());
          }
        }
      }
      return true;
    };
    scanMeta(connection, null, null, QueryType.ALL, v);
  }",False,False,"The function of this method is to perform a full scan of the metadata table and print information about the current meta row, table state, and region locations.",False,"The code defines a method named `fullScanMetaAndPrint` that takes a `Connection` object as a parameter. It performs a full scan of the meta table in HBase, retrieves information about table states and region locations, and prints them using the `LOG` object."
33,"scanMetaForTableRegions(Connection,ClientMetaTableAccessor.Visitor,TableName)",scan meta for table regions connection visitor table name ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,TableName,QueryType,int,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,TableName,QueryType,int,Visitor)",java.lang.Integer+MAX_VALUE,239,4,9,,"public static void scanMetaForTableRegions(Connection connection,
    ClientMetaTableAccessor.Visitor visitor, TableName tableName) throws IOException {
    scanMeta(connection, tableName, QueryType.REGION, Integer.MAX_VALUE, visitor);
  }",False,False,The function of this method is to scan the metadata of a table in a given connection and invoke a visitor for each region of the table.,False,"The given code is a method named ""scanMetaForTableRegions"" that takes a connection, a visitor, and a table name as parameters. It calls another method named ""scanMeta"" with the provided parameters and a query type of ""REGION"" to scan the meta table and visit the regions of the specified table."
34,"scanMeta(Connection,TableName,QueryType,int,ClientMetaTableAccessor.Visitor)",scan meta connection table type max rows visitor ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,int,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,int,Visitor) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType)",,352,5,10,,"private static void scanMeta(Connection connection, TableName table, QueryType type, int maxRows,
    final ClientMetaTableAccessor.Visitor visitor) throws IOException {
    scanMeta(connection, ClientMetaTableAccessor.getTableStartRowForMeta(table, type),
      ClientMetaTableAccessor.getTableStopRowForMeta(table, type), type, maxRows, visitor);
  }",False,False,"This method is used to scan the metadata of a table in a database connection. It takes parameters such as the connection, table name, query type, maximum number of rows, and a visitor object. It then calls another method to perform the actual scanning.",False,"The given code is a private static method named ""scanMeta"" that takes in a connection, table name, query type, maximum number of rows, and a visitor. It calls another overloaded version of the ""scanMeta"" method with additional parameters, which is responsible for scanning the meta table using the provided parameters and the visitor object."
35,"scanMeta(Connection,byte[],byte[],QueryType,ClientMetaTableAccessor.Visitor)",scan meta connection start row stop row type visitor ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,int,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,int,Visitor)",java.lang.Integer+MAX_VALUE,291,5,10,,"private static void scanMeta(Connection connection, @Nullable final byte[] startRow,
    @Nullable final byte[] stopRow, QueryType type, final ClientMetaTableAccessor.Visitor visitor)
    throws IOException {
    scanMeta(connection, startRow, stopRow, type, Integer.MAX_VALUE, visitor);
  }",False,False,"The method scans the metadata table in a given connection, with optional start and stop rows, and a specified query type. It uses a visitor to process the results.",False,"The given code is a private static method named ""scanMeta"" that takes a Connection object, startRow and stopRow byte arrays, a QueryType enum, and a Visitor object as parameters. It performs a scan operation on the meta table using the provided parameters and calls another overloaded version of the same method with an additional parameter set to Integer.MAX_VALUE. It throws an IOException if any error occurs during the scan operation."
36,"scanMeta(Connection,ClientMetaTableAccessor.Visitor,TableName,byte[],int)",scan meta connection visitor table name row row limit performs a scan of meta table for given table starting from given row param connection connection we re using param visitor visitor to call param table name table withing we scan param row start scan from this row param row limit max number of rows to return ,Method,,,"org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.MetaTableAccessor+getClosestRegionInfo(Connection,TableName,byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getClosestRegionInfo(Connection,TableName,byte[]) org.apache.hadoop.hbase.client.RegionInfo+createRegionName(TableName,byte[],String,boolean) org.apache.hadoop.hbase.client.RegionInfo+createRegionName(TableName,byte[],String,boolean) org.apache.hadoop.hbase.client.RegionInfo+getStartKey() org.apache.hadoop.hbase.client.RegionInfo+getStartKey() org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,int,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,int,Visitor)",org.apache.hadoop.hbase.HConstants+ZEROES,1090,23,9,"/** 
 * Performs a scan of META table for given table starting from given row.
 * @param connection connection we're using
 * @param visitor visitor to call
 * @param tableName table withing we scan
 * @param row start scan from this row
 * @param rowLimit max number of rows to return
 */
","/**
   * Performs a scan of META table for given table starting from given row.
   * @param connection connection we're using
   * @param visitor visitor to call
   * @param tableName table withing we scan
   * @param row start scan from this row
   * @param rowLimit max number of rows to return
   */
  public static void scanMeta(Connection connection, final ClientMetaTableAccessor.Visitor visitor,
    final TableName tableName, final byte[] row, final int rowLimit) throws IOException {
    byte[] startRow = null;
    byte[] stopRow = null;
    if (tableName != null) {
      startRow = ClientMetaTableAccessor.getTableStartRowForMeta(tableName, QueryType.REGION);
      if (row != null) {
        RegionInfo closestRi = getClosestRegionInfo(connection, tableName, row);
        startRow =
          RegionInfo.createRegionName(tableName, closestRi.getStartKey(), HConstants.ZEROES, false);
      }
      stopRow = ClientMetaTableAccessor.getTableStopRowForMeta(tableName, QueryType.REGION);
    }
    scanMeta(connection, startRow, stopRow, QueryType.REGION, rowLimit, visitor);
  }",False,False,"This method performs a scan of the META table for a given table starting from a given row. It takes a connection, a visitor, a table name, a row, and a row limit as parameters. It scans the META table based on the provided parameters and calls the visitor for each scanned row.",False,This code performs a scan of the META table for a given table starting from a specified row. It uses the provided visitor to process the scanned data. The scan is limited to a maximum number of rows specified by the rowLimit parameter.
37,"scanMeta(Connection,byte[],byte[],QueryType,int,ClientMetaTableAccessor.Visitor)",scan meta connection start row stop row type max rows visitor performs a scan of meta table param connection connection we re using param start row where to start the scan pass null if want to begin scan at first row param stop row where to stop the scan pass null if want to scan all rows from the start one param type scanned part of meta param max rows maximum rows to return param visitor visitor invoked against each row ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Filter,int,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Filter,int,Visitor)",,711,14,8,"/** 
 * Performs a scan of META table.
 * @param connection connection we're using
 * @param startRow Where to start the scan. Pass null if want to begin scan at first row.
 * @param stopRow Where to stop the scan. Pass null if want to scan all rows from the start one
 * @param type scanned part of meta
 * @param maxRows maximum rows to return
 * @param visitor Visitor invoked against each row.
 */
","/**
   * Performs a scan of META table.
   * @param connection connection we're using
   * @param startRow Where to start the scan. Pass null if want to begin scan at first row.
   * @param stopRow Where to stop the scan. Pass null if want to scan all rows from the start one
   * @param type scanned part of meta
   * @param maxRows maximum rows to return
   * @param visitor Visitor invoked against each row.
   */
  static void scanMeta(Connection connection, @Nullable final byte[] startRow,
    @Nullable final byte[] stopRow, QueryType type, int maxRows,
    final ClientMetaTableAccessor.Visitor visitor) throws IOException {
    scanMeta(connection, startRow, stopRow, type, null, maxRows, visitor);
  }",False,False,The function performs a scan of the META table using the provided parameters. It starts the scan at the specified start row and stops at the specified stop row. It scans a specific part of the meta table based on the type parameter. It returns a maximum number of rows and invokes a visitor against each row.,False,"This code defines a method called ""scanMeta"" that performs a scan of the META table. It takes various parameters such as the connection, start and stop rows, query type, maximum rows to return, and a visitor object. The method invokes another overloaded version of ""scanMeta"" with additional parameters and performs the scan operation using the provided visitor."
38,"scanMeta(Connection,byte[],byte[],QueryType,Filter,int,ClientMetaTableAccessor.Visitor)",scan meta connection start row stop row type filter max rows visitor ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+getMetaScan(Connection,int) org.apache.hadoop.hbase.MetaTableAccessor+getMetaScan(Connection,int) org.apache.hadoop.hbase.client.Scan+addFamily(byte[]) org.apache.hadoop.hbase.client.Scan+addFamily(byte[]) org.apache.hadoop.hbase.client.Scan+withStartRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStartRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStopRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStopRow(byte[]) org.apache.hadoop.hbase.client.Scan+setFilter(Filter) org.apache.hadoop.hbase.client.Scan+setFilter(Filter) org.apache.hadoop.hbase.util.Bytes+toStringBinary(byte[]) org.apache.hadoop.hbase.util.Bytes+toStringBinary(byte[]) org.apache.hadoop.hbase.util.Bytes+toStringBinary(byte[]) org.apache.hadoop.hbase.util.Bytes+toStringBinary(byte[]) org.apache.hadoop.hbase.client.Scan+getCaching() org.apache.hadoop.hbase.client.Scan+getCaching() org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Table+getScanner(Scan) org.apache.hadoop.hbase.client.Table+getScanner(Scan) org.apache.hadoop.hbase.client.ResultScanner+next() org.apache.hadoop.hbase.client.ResultScanner+next() org.apache.hadoop.hbase.client.Result+isEmpty() org.apache.hadoop.hbase.client.Result+isEmpty() org.apache.hadoop.hbase.ClientMetaTableAccessor.Visitor+visit(Result) org.apache.hadoop.hbase.ClientMetaTableAccessor.Visitor+visit(Result) java.io.Closeable+close() java.io.Closeable+close() org.apache.hadoop.hbase.util.ExceptionUtil+rethrowIfInterrupt(Throwable) org.apache.hadoop.hbase.util.ExceptionUtil+rethrowIfInterrupt(Throwable)",java.lang.Integer+MAX_VALUE org.apache.hadoop.hbase.MetaTableAccessor+LOG org.apache.hadoop.hbase.MetaTableAccessor+LOG org.apache.hadoop.hbase.MetaTableAccessor+LOG,1673,52,10,,"private static void scanMeta(Connection connection, @Nullable final byte[] startRow,
    @Nullable final byte[] stopRow, QueryType type, @Nullable Filter filter, int maxRows,
    final ClientMetaTableAccessor.Visitor visitor) throws IOException {
    int rowUpperLimit = maxRows > 0 ? maxRows : Integer.MAX_VALUE;
    Scan scan = getMetaScan(connection, rowUpperLimit);

    for (byte[] family : type.getFamilies()) {
      scan.addFamily(family);
    }
    if (startRow != null) {
      scan.withStartRow(startRow);
    }
    if (stopRow != null) {
      scan.withStopRow(stopRow);
    }
    if (filter != null) {
      scan.setFilter(filter);
    }

    if (LOG.isTraceEnabled()) {
      LOG.trace(""Scanning META"" + "" starting at row="" + Bytes.toStringBinary(startRow) +
        "" stopping at row="" + Bytes.toStringBinary(stopRow) + "" for max="" + rowUpperLimit +
        "" with caching="" + scan.getCaching());
    }

    int currentRow = 0;
    try (Table metaTable = getMetaHTable(connection)) {
      try (ResultScanner scanner = metaTable.getScanner(scan)) {
        Result data;
        while ((data = scanner.next()) != null) {
          if (data.isEmpty()) {
            continue;
          }
          // Break if visit returns false.
          if (!visitor.visit(data)) {
            break;
          }
          if (++currentRow >= rowUpperLimit) {
            break;
          }
        }
      }
    }
    if (visitor instanceof Closeable) {
      try {
        ((Closeable) visitor).close();
      } catch (Throwable t) {
        ExceptionUtil.rethrowIfInterrupt(t);
        LOG.debug(""Got exception in closing the meta scanner visitor"", t);
      }
    }
  }",False,False,"This method scans a meta table in a database connection, using a specified start and stop row, a query type, a filter, and a maximum number of rows. It visits each row in the table and performs a specified action on the row's data.",False,"This code defines a method called `scanMeta` that scans a meta table in a database using the given parameters. It creates a scan object with specified start and stop rows, filter, and maximum number of rows. It then iterates through the result scanner and calls a visitor function for each non-empty result, breaking the loop if the visitor returns false or the maximum number of rows is reached."
39,"getClosestRegionInfo(Connection,TableName,byte[])",get closest region info connection table name row return get closest metatable region row to passed row ,Method,"scanMeta(Connection,ClientMetaTableAccessor.Visitor,TableName,byte[],int)",,"org.apache.hadoop.hbase.client.RegionInfo+createRegionName(TableName,byte[],String,boolean) org.apache.hadoop.hbase.client.RegionInfo+createRegionName(TableName,byte[],String,boolean) org.apache.hadoop.hbase.MetaTableAccessor+getMetaScan(Connection,int) org.apache.hadoop.hbase.MetaTableAccessor+getMetaScan(Connection,int) org.apache.hadoop.hbase.client.Scan+setReversed(boolean) org.apache.hadoop.hbase.client.Scan+setReversed(boolean) org.apache.hadoop.hbase.client.Scan+withStartRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStartRow(byte[]) org.apache.hadoop.hbase.client.Table+getScanner(Scan) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Table+getScanner(Scan) org.apache.hadoop.hbase.client.ResultScanner+next() org.apache.hadoop.hbase.client.ResultScanner+next() org.apache.hadoop.hbase.TableNotFoundException+TableNotFoundException(String) org.apache.hadoop.hbase.util.Bytes+toStringBinary(byte[]) org.apache.hadoop.hbase.util.Bytes+toStringBinary(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionInfo(Result) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionInfo(Result) java.io.IOException+IOException(String) org.apache.hadoop.hbase.util.Bytes+toStringBinary(byte[]) org.apache.hadoop.hbase.util.Bytes+toStringBinary(byte[])",org.apache.hadoop.hbase.HConstants+NINES,1050,24,10,"/** 
 * @return Get closest metatable region row to passed <code>row</code>
 */
","/**
   * @return Get closest metatable region row to passed <code>row</code>
   */
  @NonNull
  private static RegionInfo getClosestRegionInfo(Connection connection,
    @NonNull final TableName tableName, @NonNull final byte[] row) throws IOException {
    byte[] searchRow = RegionInfo.createRegionName(tableName, row, HConstants.NINES, false);
    Scan scan = getMetaScan(connection, 1);
    scan.setReversed(true);
    scan.withStartRow(searchRow);
    try (ResultScanner resultScanner = getMetaHTable(connection).getScanner(scan)) {
      Result result = resultScanner.next();
      if (result == null) {
        throw new TableNotFoundException(""Cannot find row in META "" + "" for table: "" + tableName +
          "", row="" + Bytes.toStringBinary(row));
      }
      RegionInfo regionInfo = CatalogFamilyFormat.getRegionInfo(result);
      if (regionInfo == null) {
        throw new IOException(""RegionInfo was null or empty in Meta for "" + tableName + "", row="" +
          Bytes.toStringBinary(row));
      }
      return regionInfo;
    }
  }",False,False,"This method returns the closest metatable region row to the passed row. It takes a connection, table name, and row as parameters, and uses a scan to search for the closest region. If no region is found, it throws a TableNotFoundException. If the region info is null, it throws an IOException. Finally, it returns the found region info.",False,"This code defines a private static method called ""getClosestRegionInfo"" that takes a connection, table name, and row as input. It retrieves the closest metatable region row to the passed row by performing a scan on the metatable, and returns the corresponding RegionInfo. If the row or RegionInfo is not found, it throws an appropriate exception."
40,"getTargetServerName(Result,int)",get target server name r replica id returns the link server name from catalog table link result where the region istransitioning on it should be the same as link catalog family format get server name result int if the server is at open state param r result to pull the transitioning server name from return a server name instance or link catalog family format get server name result int ifnecessary fields not found or empty ,Method,,"org.apache.hadoop.hbase.master.assignment.RegionStateStore+visitMetaEntry(RegionStateVisitor,Result) org.apache.hadoop.hbase.master.webapp.RegionReplicaInfo+RegionReplicaInfo(Result,HRegionLocation)","org.apache.hadoop.hbase.client.Result+getColumnLatestCell(byte[],byte[]) org.apache.hadoop.hbase.client.Result+getColumnLatestCell(byte[],byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerNameColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerNameColumn(int) org.apache.hadoop.hbase.Cell+getValueLength() org.apache.hadoop.hbase.Cell+getValueLength() org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionLocations(Result) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionLocations(Result) org.apache.hadoop.hbase.RegionLocations+getRegionLocation(int) org.apache.hadoop.hbase.RegionLocations+getRegionLocation(int) org.apache.hadoop.hbase.HRegionLocation+getServerName() org.apache.hadoop.hbase.HRegionLocation+getServerName() org.apache.hadoop.hbase.ServerName+parseServerName(String) org.apache.hadoop.hbase.ServerName+parseServerName(String) org.apache.hadoop.hbase.util.Bytes+toString(byte[],int,int) org.apache.hadoop.hbase.util.Bytes+toString(byte[],int,int) org.apache.hadoop.hbase.Cell+getValueArray() org.apache.hadoop.hbase.Cell+getValueArray() org.apache.hadoop.hbase.Cell+getValueOffset() org.apache.hadoop.hbase.Cell+getValueOffset() org.apache.hadoop.hbase.Cell+getValueLength() org.apache.hadoop.hbase.Cell+getValueLength()",org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,1164,25,9,"/** 
 * Returns the  {@link ServerName} from catalog table {@link Result} where the region istransitioning on. It should be the same as {@link CatalogFamilyFormat#getServerName(Result,int)} if the server is at OPEN state.
 * @param r Result to pull the transitioning server name from
 * @return A ServerName instance or {@link CatalogFamilyFormat#getServerName(Result,int)} ifnecessary fields not found or empty.
 */
","/**
   * Returns the {@link ServerName} from catalog table {@link Result} where the region is
   * transitioning on. It should be the same as
   * {@link CatalogFamilyFormat#getServerName(Result,int)} if the server is at OPEN state.
   * @param r Result to pull the transitioning server name from
   * @return A ServerName instance or {@link CatalogFamilyFormat#getServerName(Result,int)} if
   *         necessary fields not found or empty.
   */
  @Nullable
  public static ServerName getTargetServerName(final Result r, final int replicaId) {
    final Cell cell = r.getColumnLatestCell(HConstants.CATALOG_FAMILY,
      CatalogFamilyFormat.getServerNameColumn(replicaId));
    if (cell == null || cell.getValueLength() == 0) {
      RegionLocations locations = CatalogFamilyFormat.getRegionLocations(r);
      if (locations != null) {
        HRegionLocation location = locations.getRegionLocation(replicaId);
        if (location != null) {
          return location.getServerName();
        }
      }
      return null;
    }
    return ServerName.parseServerName(
      Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
  }",False,False,"This method returns the ServerName from a catalog table Result where the region is transitioning on. If the server is at OPEN state, it returns the same ServerName as CatalogFamilyFormat#getServerName(Result,int). If necessary fields are not found or empty, it returns null.",False,"This code is a method called `getTargetServerName` that takes a `Result` object and a replica ID as parameters. It retrieves the server name from the catalog table `Result` based on the replica ID. If the server name is not found or empty, it falls back to retrieving the server name from the region locations. The method returns a `ServerName` instance or `null` if the necessary fields are not found."
41,getDaughterRegions(Result),get daughter regions data returns the daughter regions by reading the corresponding columns of the catalog table result param data a result object from the catalog table scan return pair of region info or pair of same type null null if region is not a split parent ,Method,,"org.apache.hadoop.hbase.master.CatalogJanitor+scan() org.apache.hadoop.hbase.master.CatalogJanitor+cleanParent(RegionInfo,Result) org.apache.hadoop.hbase.master.webapp.RegionReplicaInfo+RegionReplicaInfo(Result,HRegionLocation) org.apache.hadoop.hbase.util.HBaseFsck+visit(Result) org.apache.hadoop.hbase.master.TestCatalogJanitorInMemoryStates+waitOnDaughters(RegionInfo) org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction+blockUntilRegionSplit(Configuration,long,byte[],boolean)","org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionInfo(Result,byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionInfo(Result,byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionInfo(Result,byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionInfo(Result,byte[])",org.apache.hadoop.hbase.HConstants+SPLITA_QUALIFIER org.apache.hadoop.hbase.HConstants+SPLITB_QUALIFIER,582,10,9,"/** 
 * Returns the daughter regions by reading the corresponding columns of the catalog table Result.
 * @param data a Result object from the catalog table scan
 * @return pair of RegionInfo or PairOfSameType(null, null) if region is not a split parent
 */
","/**
   * Returns the daughter regions by reading the corresponding columns of the catalog table Result.
   * @param data a Result object from the catalog table scan
   * @return pair of RegionInfo or PairOfSameType(null, null) if region is not a split parent
   */
  public static PairOfSameType<RegionInfo> getDaughterRegions(Result data) {
    RegionInfo splitA = CatalogFamilyFormat.getRegionInfo(data, HConstants.SPLITA_QUALIFIER);
    RegionInfo splitB = CatalogFamilyFormat.getRegionInfo(data, HConstants.SPLITB_QUALIFIER);
    return new PairOfSameType<>(splitA, splitB);
  }",False,False,"This method returns a pair of daughter regions by reading the corresponding columns of the catalog table Result. If the region is not a split parent, it returns a PairOfSameType(null, null).",False,
42,"getTableState(Connection,TableName)",get table state conn table name fetch table state for given table from meta table param conn connection to use param table name table to fetch state for ,Method,,"org.apache.hadoop.hbase.master.HMaster+waitForNamespaceOnline() org.apache.hadoop.hbase.master.TableNamespaceManager+start() org.apache.hadoop.hbase.master.TableStateManager+readMetaState(TableName) org.apache.hadoop.hbase.util.HBaseFsck+checkAndFixTableStates() org.apache.hadoop.hbase.util.HBaseFsck+checkAndFixTableStates() org.apache.hadoop.hbase.client.TestAdminBase+getStateFromMeta(TableName) org.apache.hadoop.hbase.HBaseTestingUtility+explainTableState(TableName,TableState.State) org.apache.hadoop.hbase.master.TestTableStateManager+testMigration() org.apache.hadoop.hbase.master.TestTableStateManager+testMigration() org.apache.hadoop.hbase.master.TestTableStateManager+testMigration()","org.apache.hadoop.hbase.TableName+equals(Object) org.apache.hadoop.hbase.TableName+equals(Object) org.apache.hadoop.hbase.client.TableState+TableState(TableName,State) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Get+addColumn(byte[],byte[]) org.apache.hadoop.hbase.client.Get+Get(byte[]) org.apache.hadoop.hbase.TableName+getName() org.apache.hadoop.hbase.TableName+getName() org.apache.hadoop.hbase.client.Get+addColumn(byte[],byte[]) org.apache.hadoop.hbase.client.Table+get(Get) org.apache.hadoop.hbase.client.Table+get(Get) org.apache.hadoop.hbase.CatalogFamilyFormat+getTableState(Result) org.apache.hadoop.hbase.CatalogFamilyFormat+getTableState(Result)",org.apache.hadoop.hbase.TableName+META_TABLE_NAME org.apache.hadoop.hbase.client.TableState+State org.apache.hadoop.hbase.HConstants+TABLE_FAMILY org.apache.hadoop.hbase.HConstants+TABLE_STATE_QUALIFIER,647,16,9,"/** 
 * Fetch table state for given table from META table
 * @param conn connection to use
 * @param tableName table to fetch state for
 */
","/**
   * Fetch table state for given table from META table
   * @param conn connection to use
   * @param tableName table to fetch state for
   */
  @Nullable
  public static TableState getTableState(Connection conn, TableName tableName) throws IOException {
    if (tableName.equals(TableName.META_TABLE_NAME)) {
      return new TableState(tableName, TableState.State.ENABLED);
    }
    Table metaHTable = getMetaHTable(conn);
    Get get = new Get(tableName.getName()).addColumn(HConstants.TABLE_FAMILY,
      HConstants.TABLE_STATE_QUALIFIER);
    Result result = metaHTable.get(get);
    return CatalogFamilyFormat.getTableState(result);
  }",False,False,"This method fetches the state of a given table from the META table. It takes a connection and table name as parameters, and returns the table state. If the table name is the META table, it returns an enabled state. Otherwise, it retrieves the table state from the META table using the given connection and table name.",False,"This code fetches the state of a given table from the META table in HBase. It first checks if the table is the META table, and if so, returns an enabled state. Otherwise, it retrieves the table state from the META table using the given connection and table name."
43,getTableStates(Connection),get table states conn fetch table states from meta table param conn connection to use return map table name gt state ,Method,,org.apache.hadoop.hbase.util.HBaseFsck+loadTableStates(),"org.apache.hadoop.hbase.CatalogFamilyFormat+getTableState(Result) org.apache.hadoop.hbase.CatalogFamilyFormat+getTableState(Result) java.util.Map+put(K,V) java.util.Map+put(K,V) org.apache.hadoop.hbase.client.TableState+getTableName() org.apache.hadoop.hbase.client.TableState+getTableName() org.apache.hadoop.hbase.MetaTableAccessor+fullScanTables(Connection,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+fullScanTables(Connection,Visitor)",,578,17,9,"/** 
 * Fetch table states from META table
 * @param conn connection to use
 * @return map {tableName -&gt; state}
 */
","/**
   * Fetch table states from META table
   * @param conn connection to use
   * @return map {tableName -&gt; state}
   */
  public static Map<TableName, TableState> getTableStates(Connection conn) throws IOException {
    final Map<TableName, TableState> states = new LinkedHashMap<>();
    ClientMetaTableAccessor.Visitor collector = r -> {
      TableState state = CatalogFamilyFormat.getTableState(r);
      if (state != null) {
        states.put(state.getTableName(), state);
      }
      return true;
    };
    fullScanTables(conn, collector);
    return states;
  }",False,False,"The function of this method is to fetch table states from the META table and return them as a map, where the keys are table names and the values are the corresponding states.",False,This code fetches the states of tables from the META table using the provided connection. It returns a map where the keys are table names and the values are the corresponding table states. The states are collected by iterating over the META table and adding the non-null states to the map.
44,"updateTableState(Connection,TableName,TableState.State)",update table state conn table name actual updates state in meta do not use for internal use only param conn connection to use param table name table to look for ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+updateTableState(Connection,TableState) org.apache.hadoop.hbase.MetaTableAccessor+updateTableState(Connection,TableState) org.apache.hadoop.hbase.client.TableState+TableState(TableName,State)",,339,9,9,"/** 
 * Updates state in META Do not use. For internal use only.
 * @param conn connection to use
 * @param tableName table to look for
 */
","/**
   * Updates state in META Do not use. For internal use only.
   * @param conn connection to use
   * @param tableName table to look for
   */
  public static void updateTableState(Connection conn, TableName tableName, TableState.State actual)
    throws IOException {
    updateTableState(conn, new TableState(tableName, actual));
  }",False,False,"This method updates the state of a table in the META. It takes a connection, table name, and the actual state as parameters, and updates the state of the table in the META.",False,"This code defines a method called ""updateTableState"" that is used internally to update the state of a table in a database. It takes a connection, table name, and the desired state as input parameters, and updates the state of the table accordingly."
45,"makePutFromRegionInfo(RegionInfo,long)",make put from region info region info ts generates and returns a put containing the region into for the catalog table ,Method,"addSplitsToParent(Connection,RegionInfo,RegionInfo,RegionInfo) addRegionsToMeta(Connection,List<RegionInfo>,int,long) mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)","org.apache.hadoop.hbase.favored.FavoredNodeAssignmentHelper+makePutFromRegionInfo(RegionInfo,List<ServerName>) org.apache.hadoop.hbase.util.HBaseFsck+resetSplitParent(HbckRegionInfo) org.apache.hadoop.hbase.util.HBaseFsckRepair+fixMetaHoleOnlineAndAddReplicas(Configuration,RegionInfo,Collection<ServerName>,int) org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestMetaFixer+makeOverlap(MasterServices,RegionInfo,RegionInfo) org.apache.hadoop.hbase.master.TestMetaFixer+testMergeWithMergedChildRegion() org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore+testUsingMetaAndBinary()","org.apache.hadoop.hbase.MetaTableAccessor+addRegionInfo(Put,RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+addRegionInfo(Put,RegionInfo) org.apache.hadoop.hbase.client.Put+Put(byte[],long) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.RegionInfo+getRegionName()",,269,6,9,"/** 
 * Generates and returns a Put containing the region into for the catalog table
 */
","/**
   * Generates and returns a Put containing the region into for the catalog table
   */
  public static Put makePutFromRegionInfo(RegionInfo regionInfo, long ts) throws IOException {
    return addRegionInfo(new Put(regionInfo.getRegionName(), ts), regionInfo);
  }",False,False,This method generates and returns a Put object that contains the region information for the catalog table.,False,"This code defines a static method called ""makePutFromRegionInfo"" that takes a RegionInfo object and a timestamp as input. It creates a Put object with the region name and timestamp, and then calls another method called ""addRegionInfo"" to add additional information to the Put object before returning it."
46,"makeDeleteFromRegionInfo(RegionInfo,long)",make delete from region info region info ts generates and returns a delete containing the region info for the catalog table ,Method,"mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int)",,"java.lang.IllegalArgumentException+IllegalArgumentException(String) org.apache.hadoop.hbase.client.Delete+Delete(byte[]) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.Delete+addFamily(byte[],long) org.apache.hadoop.hbase.client.Delete+addFamily(byte[],long)",org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,430,11,10,"/** 
 * Generates and returns a Delete containing the region info for the catalog table
 */
","/**
   * Generates and returns a Delete containing the region info for the catalog table
   */
  private static Delete makeDeleteFromRegionInfo(RegionInfo regionInfo, long ts) {
    if (regionInfo == null) {
      throw new IllegalArgumentException(""Can't make a delete for null region"");
    }
    Delete delete = new Delete(regionInfo.getRegionName());
    delete.addFamily(HConstants.CATALOG_FAMILY, ts);
    return delete;
  }",False,False,This method generates and returns a Delete object that contains the region information for the catalog table. It throws an exception if the regionInfo parameter is null.,False,"This code defines a private static method called ""makeDeleteFromRegionInfo"" that takes a RegionInfo object and a timestamp as input. It creates and returns a Delete object that represents a delete operation on the catalog table, specifically for the given region. The delete operation includes a specific family (HConstants.CATALOG_FAMILY) and timestamp."
47,"addDaughtersToPut(Put,RegionInfo,RegionInfo)",add daughters to put put split a split b adds split daughters to the put ,Method,"addSplitsToParent(Connection,RegionInfo,RegionInfo,RegionInfo) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)",,org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.RegionInfo+toByteArray(RegionInfo) org.apache.hadoop.hbase.client.RegionInfo+toByteArray(RegionInfo) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.RegionInfo+toByteArray(RegionInfo) org.apache.hadoop.hbase.client.RegionInfo+toByteArray(RegionInfo),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+SPLITA_QUALIFIER org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+SPLITB_QUALIFIER,839,19,10,"/** 
 * Adds split daughters to the Put
 */
","/**
   * Adds split daughters to the Put
   */
  private static Put addDaughtersToPut(Put put, RegionInfo splitA, RegionInfo splitB)
    throws IOException {
    if (splitA != null) {
      put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(put.getRow())
        .setFamily(HConstants.CATALOG_FAMILY).setQualifier(HConstants.SPLITA_QUALIFIER)
        .setTimestamp(put.getTimestamp()).setType(Type.Put).setValue(RegionInfo.toByteArray(splitA))
        .build());
    }
    if (splitB != null) {
      put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(put.getRow())
        .setFamily(HConstants.CATALOG_FAMILY).setQualifier(HConstants.SPLITB_QUALIFIER)
        .setTimestamp(put.getTimestamp()).setType(Type.Put).setValue(RegionInfo.toByteArray(splitB))
        .build());
    }
    return put;
  }",False,False,The function of the method is to add split daughters (splitA and splitB) to a Put object.,False,This code is a private method that adds split daughters to a Put object. It takes two RegionInfo objects as parameters and adds them as cells to the Put object with specific qualifiers. The method returns the modified Put object.
48,"putToMetaTable(Connection,Put)",put to meta table connection p put the passed p to the hbase meta table param connection connection we re using param p put to add to hbase meta ,Method,"updateTableState(Connection,TableState) updateLocation(Connection,RegionInfo,ServerName,long,long)",,"org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+put(Table,Put) org.apache.hadoop.hbase.MetaTableAccessor+put(Table,Put)",,338,10,10,"/** 
 * Put the passed <code>p</code> to the <code>hbase:meta</code> table.
 * @param connection connection we're using
 * @param p Put to add to hbase:meta
 */
","/**
   * Put the passed <code>p</code> to the <code>hbase:meta</code> table.
   * @param connection connection we're using
   * @param p Put to add to hbase:meta
   */
  private static void putToMetaTable(Connection connection, Put p) throws IOException {
    try (Table table = getMetaHTable(connection)) {
      put(table, p);
    }
  }",False,False,"This method puts the passed Put object to the hbase:meta table using the provided connection. It first gets the hbase:meta table using the connection, and then calls the put method to add the Put object to the table.",False,"The code defines a private method called ""putToMetaTable"" that takes a connection and a Put object as parameters. It puts the passed Put object into the ""hbase:meta"" table using the provided connection. The method uses a try-with-resources block to ensure the Table object is properly closed after use."
49,"put(Table,Put)",put t p param t table to use param p put to make ,Method,"putToMetaTable(Connection,Put)",,org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutation(Mutation) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutation(Mutation) org.apache.hadoop.hbase.client.Table+put(Put) org.apache.hadoop.hbase.client.Table+put(Put),,168,8,10,"/** 
 * @param t Table to use
 * @param p put to make
 */
","/**
   * @param t Table to use
   * @param p put to make
   */
  private static void put(Table t, Put p) throws IOException {
    debugLogMutation(p);
    t.put(p);
  }",False,False,"This method serves to put a mutation into a table. It takes a table and a put object as parameters, logs the mutation, and then puts the mutation into the table.",False,"The given code is a private static method named ""put"" that takes a Table object and a Put object as parameters. It logs the mutation using a debugLogMutation method and then calls the put method of the Table object to perform the put operation. It throws an IOException if an error occurs during the put operation."
50,"putsToMetaTable(Connection,List<Put>)",puts to meta table connection ps put the passed ps to the hbase meta table param connection connection we re using param ps put to add to hbase meta ,Method,"updateRegionState(Connection,RegionInfo,RegionState.State) addRegionsToMeta(Connection,List<RegionInfo>,int,long)","org.apache.hadoop.hbase.favored.FavoredNodeAssignmentHelper+updateMetaWithFavoredNodesInfo(Map<RegionInfo,List<ServerName>>,Connection) org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestMetaFixer+makeOverlap(MasterServices,RegionInfo,RegionInfo)",java.util.List+isEmpty() java.util.List+isEmpty() org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutations(List<? extends Mutation>) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutations(List<? extends Mutation>) java.util.List+size() java.util.List+size() org.apache.hadoop.hbase.client.Table+put(Put) org.apache.hadoop.hbase.client.Table+put(Put) java.util.List+get(int) java.util.List+get(int) org.apache.hadoop.hbase.client.Table+put(List<Put>) org.apache.hadoop.hbase.client.Table+put(List<Put>),,605,20,9,"/** 
 * Put the passed <code>ps</code> to the <code>hbase:meta</code> table.
 * @param connection connection we're using
 * @param ps Put to add to hbase:meta
 */
","/**
   * Put the passed <code>ps</code> to the <code>hbase:meta</code> table.
   * @param connection connection we're using
   * @param ps Put to add to hbase:meta
   */
  public static void putsToMetaTable(final Connection connection, final List<Put> ps)
    throws IOException {
    if (ps.isEmpty()) {
      return;
    }
    try (Table t = getMetaHTable(connection)) {
      debugLogMutations(ps);
      // the implementation for putting a single Put is much simpler so here we do a check first.
      if (ps.size() == 1) {
        t.put(ps.get(0));
      } else {
        t.put(ps);
      }
    }
  }",False,False,"This method puts the passed list of Put objects to the hbase:meta table using the provided connection. If the list is empty, it returns without doing anything. If the list contains only one Put object, it puts that object directly. Otherwise, it puts the entire list of Put objects.",False,"This code is a method called `putsToMetaTable` that takes a connection and a list of `Put` objects as parameters. It puts the `Put` objects into the `hbase:meta` table using the provided connection. If the list is empty, it returns without performing any action."
51,"deleteFromMetaTable(Connection,Delete)",delete from meta table connection d delete the passed d from the hbase meta table param connection connection we re using param d delete to add to hbase meta ,Method,"removeRegionReplicasFromMeta(Set<byte[]>,int,int,Connection) deleteTableState(Connection,TableName) deleteRegionInfo(Connection,RegionInfo) deleteMergeQualifiers(Connection,RegionInfo)",,"java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,List<Delete>) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,List<Delete>)",,395,11,10,"/** 
 * Delete the passed <code>d</code> from the <code>hbase:meta</code> table.
 * @param connection connection we're using
 * @param d Delete to add to hbase:meta
 */
","/**
   * Delete the passed <code>d</code> from the <code>hbase:meta</code> table.
   * @param connection connection we're using
   * @param d Delete to add to hbase:meta
   */
  private static void deleteFromMetaTable(final Connection connection, final Delete d)
    throws IOException {
    List<Delete> dels = new ArrayList<>(1);
    dels.add(d);
    deleteFromMetaTable(connection, dels);
  }",False,False,"This method is used to delete a specific row from the ""hbase:meta"" table in HBase. It takes a connection and a Delete object as parameters, and internally calls another method to perform the deletion.",False,"The code defines a private method called ""deleteFromMetaTable"" that takes a Connection object and a Delete object as parameters. It creates a list of Delete objects and adds the passed Delete object to the list. Then, it calls another overloaded version of the deleteFromMetaTable method with the Connection object and the list of Delete objects as parameters."
52,"deleteFromMetaTable(Connection,List<Delete>)",delete from meta table connection deletes delete the passed deletes from the hbase meta table param connection connection we re using param deletes deletes to add to hbase meta this list should support remove ,Method,"deleteFromMetaTable(Connection,Delete) deleteRegionInfos(Connection,List<RegionInfo>,long)",,org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutations(List<? extends Mutation>) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutations(List<? extends Mutation>) org.apache.hadoop.hbase.client.Table+delete(List<Delete>) org.apache.hadoop.hbase.client.Table+delete(List<Delete>),,463,12,10,"/** 
 * Delete the passed <code>deletes</code> from the <code>hbase:meta</code> table.
 * @param connection connection we're using
 * @param deletes Deletes to add to hbase:meta This list should support #remove.
 */
","/**
   * Delete the passed <code>deletes</code> from the <code>hbase:meta</code> table.
   * @param connection connection we're using
   * @param deletes Deletes to add to hbase:meta This list should support #remove.
   */
  private static void deleteFromMetaTable(final Connection connection, final List<Delete> deletes)
    throws IOException {
    try (Table t = getMetaHTable(connection)) {
      debugLogMutations(deletes);
      t.delete(deletes);
    }
  }",False,False,This method serves to delete the passed deletes from the hbase:meta table using the provided connection. It takes a list of Delete objects and deletes them from the table.,False,"The code defines a private method called ""deleteFromMetaTable"" that takes a connection and a list of Delete objects as parameters. It deletes the specified Delete objects from the ""hbase:meta"" table using the provided connection."
53,"removeRegionReplicasFromMeta(Set<byte[]>,int,int,Connection)",remove region replicas from meta meta rows replica index to delete from num replicas to remove connection deletes some replica columns corresponding to replicas for the passed rows param meta rows rows in hbase meta param replica index to delete from the replica id we would start deleting from param num replicas to remove how many replicas to remove param connection connection we re using to access meta table ,Method,,"org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure+updateReplicaColumnsIfNeeded(MasterProcedureEnv,TableDescriptor,TableDescriptor) org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsRemovedAtTableDeletion()","org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.client.Delete+Delete(byte[]) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerColumn(int) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.CatalogFamilyFormat+getSeqNumColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getSeqNumColumn(int) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.CatalogFamilyFormat+getStartCodeColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getStartCodeColumn(int) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerNameColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerNameColumn(int) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionStateColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getRegionStateColumn(int) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,Delete) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,Delete)",org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,1522,30,9,"/** 
 * Deletes some replica columns corresponding to replicas for the passed rows
 * @param metaRows rows in hbase:meta
 * @param replicaIndexToDeleteFrom the replica ID we would start deleting from
 * @param numReplicasToRemove how many replicas to remove
 * @param connection connection we're using to access meta table
 */
","/**
   * Deletes some replica columns corresponding to replicas for the passed rows
   * @param metaRows rows in hbase:meta
   * @param replicaIndexToDeleteFrom the replica ID we would start deleting from
   * @param numReplicasToRemove how many replicas to remove
   * @param connection connection we're using to access meta table
   */
  public static void removeRegionReplicasFromMeta(Set<byte[]> metaRows,
    int replicaIndexToDeleteFrom, int numReplicasToRemove, Connection connection)
    throws IOException {
    int absoluteIndex = replicaIndexToDeleteFrom + numReplicasToRemove;
    for (byte[] row : metaRows) {
      long now = EnvironmentEdgeManager.currentTime();
      Delete deleteReplicaLocations = new Delete(row);
      for (int i = replicaIndexToDeleteFrom; i < absoluteIndex; i++) {
        deleteReplicaLocations.addColumns(HConstants.CATALOG_FAMILY,
          CatalogFamilyFormat.getServerColumn(i), now);
        deleteReplicaLocations.addColumns(HConstants.CATALOG_FAMILY,
          CatalogFamilyFormat.getSeqNumColumn(i), now);
        deleteReplicaLocations.addColumns(HConstants.CATALOG_FAMILY,
          CatalogFamilyFormat.getStartCodeColumn(i), now);
        deleteReplicaLocations.addColumns(HConstants.CATALOG_FAMILY,
          CatalogFamilyFormat.getServerNameColumn(i), now);
        deleteReplicaLocations.addColumns(HConstants.CATALOG_FAMILY,
          CatalogFamilyFormat.getRegionStateColumn(i), now);
      }

      deleteFromMetaTable(connection, deleteReplicaLocations);
    }
  }",False,False,"The function removes a specified number of replica columns from the hbase:meta table for the given rows, starting from a specified replica ID.",False,"This code is a method that removes a specified number of replica columns from the hbase:meta table for a given set of rows. It takes the starting replica index, the number of replicas to remove, and the connection to the meta table as input parameters."
54,"addRegionStateToPut(Put,RegionState.State)",add region state to put put state ,Method,,,org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.util.Bytes+toBytes(ByteBuffer) org.apache.hadoop.hbase.util.Bytes+toBytes(ByteBuffer),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+STATE_QUALIFIER org.apache.hadoop.hbase.Cell+Type,405,7,10,,"private static Put addRegionStateToPut(Put put, RegionState.State state) throws IOException {
    put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(put.getRow())
      .setFamily(HConstants.CATALOG_FAMILY).setQualifier(HConstants.STATE_QUALIFIER)
      .setTimestamp(put.getTimestamp()).setType(Cell.Type.Put).setValue(Bytes.toBytes(state.name()))
      .build());
    return put;
  }",False,False,This method adds the region state to a Put object by creating a new cell with the state information and adding it to the Put object. It then returns the modified Put object.,False,"The code defines a private static method named ""addRegionStateToPut"" that takes a Put object and a RegionState.State enum as parameters. It adds a new cell to the Put object with the specified row, family, qualifier, timestamp, type, and value. The modified Put object is then returned."
55,"updateRegionState(Connection,RegionInfo,RegionState.State)",update region state connection ri state update state column in hbase meta ,Method,,,"org.apache.hadoop.hbase.client.Put+Put(byte[]) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.RegionReplicaUtil+getRegionInfoForDefaultReplica(RegionInfo) org.apache.hadoop.hbase.client.RegionReplicaUtil+getRegionInfoForDefaultReplica(RegionInfo) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.MetaTableAccessor+putsToMetaTable(Connection,List<Put>) org.apache.hadoop.hbase.MetaTableAccessor+putsToMetaTable(Connection,List<Put>) java.util.Collections+singletonList(T) java.util.Collections+singletonList(T)",,366,8,9,"/** 
 * Update state column in hbase:meta.
 */
","/**
   * Update state column in hbase:meta.
   */
  public static void updateRegionState(Connection connection, RegionInfo ri,
    RegionState.State state) throws IOException {
    Put put = new Put(RegionReplicaUtil.getRegionInfoForDefaultReplica(ri).getRegionName());
    putsToMetaTable(connection, Collections.singletonList(addRegionStateToPut(put, state)));
  }",False,False,"This method updates the state column in the hbase:meta table for a given region. It takes a connection, region info, and state as input, and uses a Put operation to update the state in the meta table.",False,"This code defines a method called ""updateRegionState"" that updates the state column in the hbase:meta table. It takes a connection, region info, and state as input parameters, and uses a Put operation to update the state of the region in the meta table."
56,"addSplitsToParent(Connection,RegionInfo,RegionInfo,RegionInfo)",add splits to parent connection region info split a split b adds daughter region infos to hbase meta row for the specified region note that this does not add its daughter s as different rows but adds information about the daughters in the same row as the parent use link split region connection region info long region info region info server name int ifyou want to do that param connection connection we re using param region info region info of parent region param split a first split daughter of the parent region info param split b second split daughter of the parent region info throws io exception if problem connecting or updating meta ,Method,,"org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper+RestoreMetaChanges.updateMetaParentRegions(Connection,List<RegionInfo>)","org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.MetaTableAccessor+addDaughtersToPut(Put,RegionInfo,RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+addDaughtersToPut(Put,RegionInfo,RegionInfo) org.apache.hadoop.hbase.client.Table+put(Put) org.apache.hadoop.hbase.client.Table+put(Put) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutation(Mutation) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutation(Mutation) org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString()",org.apache.hadoop.hbase.MetaTableAccessor+LOG,1117,22,9,"/** 
 * Adds daughter region infos to hbase:meta row for the specified region. Note that this does not add its daughter's as different rows, but adds information about the daughters in the same row as the parent. Use {@link #splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)} ifyou want to do that.
 * @param connection connection we're using
 * @param regionInfo RegionInfo of parent region
 * @param splitA first split daughter of the parent regionInfo
 * @param splitB second split daughter of the parent regionInfo
 * @throws IOException if problem connecting or updating meta
 */
","/**
   * Adds daughter region infos to hbase:meta row for the specified region. Note that this does not
   * add its daughter's as different rows, but adds information about the daughters in the same row
   * as the parent. Use
   * {@link #splitRegion(Connection, RegionInfo, long, RegionInfo, RegionInfo, ServerName, int)} if
   * you want to do that.
   * @param connection connection we're using
   * @param regionInfo RegionInfo of parent region
   * @param splitA first split daughter of the parent regionInfo
   * @param splitB second split daughter of the parent regionInfo
   * @throws IOException if problem connecting or updating meta
   */
  public static void addSplitsToParent(Connection connection, RegionInfo regionInfo,
    RegionInfo splitA, RegionInfo splitB) throws IOException {
    try (Table meta = getMetaHTable(connection)) {
      Put put = makePutFromRegionInfo(regionInfo, EnvironmentEdgeManager.currentTime());
      addDaughtersToPut(put, splitA, splitB);
      meta.put(put);
      debugLogMutation(put);
      LOG.debug(""Added region {}"", regionInfo.getRegionNameAsString());
    }
  }",False,False,"The function of this method is to add daughter region information to the hbase:meta row for a specified region. It does not add the daughters as separate rows, but adds information about them in the same row as the parent.",False,"This code adds information about two daughter regions to the hbase:meta row of a specified parent region. The method takes the connection, parent region info, and daughter region infos as parameters, and updates the meta table accordingly."
57,"addRegionToMeta(Connection,RegionInfo)",add region to meta connection region info adds a single hbase meta row for the specified new region and its daughters note that this does not add its daughter s as different rows but adds information about the daughters in the same row as the parent use link split region connection region info long region info region info server name int ifyou want to do that param connection connection we re using param region info region information throws io exception if problem connecting or updating meta ,Method,,"org.apache.hadoop.hbase.HBaseTestingUtility+createMultiRegionsInMeta(Configuration,TableDescriptor,byte[][]) org.apache.hadoop.hbase.master.assignment.TestAssignmentManager+testLoadRegionFromMetaAfterRegionManuallyAdded() org.apache.hadoop.hbase.TestMetaTableAccessor+metaTask() org.apache.hadoop.hbase.util.hbck.OfflineMetaRebuildTestCore+createRegion(Configuration,Table,byte[],byte[])","org.apache.hadoop.hbase.MetaTableAccessor+addRegionsToMeta(Connection,List<RegionInfo>,int) org.apache.hadoop.hbase.MetaTableAccessor+addRegionsToMeta(Connection,List<RegionInfo>,int) java.util.Collections+singletonList(T) java.util.Collections+singletonList(T)",,728,15,9,"/** 
 * Adds a (single) hbase:meta row for the specified new region and its daughters. Note that this does not add its daughter's as different rows, but adds information about the daughters in the same row as the parent. Use {@link #splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)} ifyou want to do that.
 * @param connection connection we're using
 * @param regionInfo region information
 * @throws IOException if problem connecting or updating meta
 */
","/**
   * Adds a (single) hbase:meta row for the specified new region and its daughters. Note that this
   * does not add its daughter's as different rows, but adds information about the daughters in the
   * same row as the parent. Use
   * {@link #splitRegion(Connection, RegionInfo, long, RegionInfo, RegionInfo, ServerName, int)} if
   * you want to do that.
   * @param connection connection we're using
   * @param regionInfo region information
   * @throws IOException if problem connecting or updating meta
   */
  @VisibleForTesting
  public static void addRegionToMeta(Connection connection, RegionInfo regionInfo)
    throws IOException {
    addRegionsToMeta(connection, Collections.singletonList(regionInfo), 1);
  }",False,False,"The function of this method is to add a single hbase:meta row for a new region and its daughters. It does not add the daughters as separate rows, but includes information about the daughters in the same row as the parent.",False,"The given code is a method named ""addRegionToMeta"" that adds a single hbase:meta row for a specified new region and its daughters. It does not add the daughters as separate rows, but includes information about the daughters in the same row as the parent. This method is used to update the meta information in HBase."
58,"addRegionsToMeta(Connection,List<RegionInfo>,int)",add regions to meta connection region infos region replication adds a hbase meta row for each of the specified new regions initial state for new regions is closed param connection connection we re using param region infos region information list throws io exception if problem connecting or updating meta ,Method,"addRegionToMeta(Connection,RegionInfo)","org.apache.hadoop.hbase.master.MasterWalManager+createMetaEntries(MasterServices,List<RegionInfo>) org.apache.hadoop.hbase.master.procedure.CreateTableProcedure+addRegionsToMeta(MasterProcedureEnv,TableDescriptor,List<RegionInfo>) org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure+addRegionsToMeta(MasterProcedureEnv,TableDescriptor,List<RegionInfo>) org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure+updateMETA(MasterProcedureEnv) org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsRemovedAtTableDeletion() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsAddedAtTableCreation() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsAddedAtRegionSplit() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsAddedAtRegionMerge() org.apache.hadoop.hbase.TestMetaTableAccessor+testMastersSystemTimeIsUsedInUpdateLocations() org.apache.hadoop.hbase.TestMetaTableAccessor+testMastersSystemTimeIsUsedInMergeRegions() org.apache.hadoop.hbase.TestMetaTableAccessor+testEmptyMetaDaughterLocationDuringSplit()","org.apache.hadoop.hbase.MetaTableAccessor+addRegionsToMeta(Connection,List<RegionInfo>,int,long) org.apache.hadoop.hbase.MetaTableAccessor+addRegionsToMeta(Connection,List<RegionInfo>,int,long) org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime()",,532,12,9,"/** 
 * Adds a hbase:meta row for each of the specified new regions. Initial state for new regions is CLOSED.
 * @param connection connection we're using
 * @param regionInfos region information list
 * @throws IOException if problem connecting or updating meta
 */
","/**
   * Adds a hbase:meta row for each of the specified new regions. Initial state for new regions is
   * CLOSED.
   * @param connection connection we're using
   * @param regionInfos region information list
   * @throws IOException if problem connecting or updating meta
   */
  public static void addRegionsToMeta(Connection connection, List<RegionInfo> regionInfos,
    int regionReplication) throws IOException {
    addRegionsToMeta(connection, regionInfos, regionReplication,
      EnvironmentEdgeManager.currentTime());
  }",False,False,"The function adds new regions to the HBase metadata table with an initial state of CLOSED. It takes a connection, a list of region information, and a region replication factor as input parameters. It throws an IOException if there is a problem connecting or updating the metadata.",False,"This code is a method called ""addRegionsToMeta"" that adds new regions to the HBase metadata table. The method takes a connection, a list of region information, and a region replication factor as parameters. It throws an IOException if there is a problem connecting or updating the metadata."
59,"addRegionsToMeta(Connection,List<RegionInfo>,int,long)",add regions to meta connection region infos region replication ts adds a hbase meta row for each of the specified new regions initial state for new regions is closed param connection connection we re using param region infos region information list param ts desired timestamp throws io exception if problem connecting or updating meta ,Method,"addRegionsToMeta(Connection,List<RegionInfo>,int) overwriteRegions(Connection,List<RegionInfo>,int)",,"org.apache.hadoop.hbase.client.RegionReplicaUtil+isDefaultReplica(RegionInfo) org.apache.hadoop.hbase.client.RegionReplicaUtil+isDefaultReplica(RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+addEmptyLocation(Put,int) org.apache.hadoop.hbase.MetaTableAccessor+addEmptyLocation(Put,int) java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.MetaTableAccessor+putsToMetaTable(Connection,List<Put>) org.apache.hadoop.hbase.MetaTableAccessor+putsToMetaTable(Connection,List<Put>) java.util.List+size() java.util.List+size()",org.apache.hadoop.hbase.MetaTableAccessor+LOG,1179,27,10,"/** 
 * Adds a hbase:meta row for each of the specified new regions. Initial state for new regions is CLOSED.
 * @param connection connection we're using
 * @param regionInfos region information list
 * @param ts desired timestamp
 * @throws IOException if problem connecting or updating meta
 */
","/**
   * Adds a hbase:meta row for each of the specified new regions. Initial state for new regions is
   * CLOSED.
   * @param connection connection we're using
   * @param regionInfos region information list
   * @param ts desired timestamp
   * @throws IOException if problem connecting or updating meta
   */
  private static void addRegionsToMeta(Connection connection, List<RegionInfo> regionInfos,
    int regionReplication, long ts) throws IOException {
    List<Put> puts = new ArrayList<>();
    for (RegionInfo regionInfo : regionInfos) {
      if (RegionReplicaUtil.isDefaultReplica(regionInfo)) {
        Put put = makePutFromRegionInfo(regionInfo, ts);
        // New regions are added with initial state of CLOSED.
        addRegionStateToPut(put, RegionState.State.CLOSED);
        // Add empty locations for region replicas so that number of replicas can be cached
        // whenever the primary region is looked up from meta
        for (int i = 1; i < regionReplication; i++) {
          addEmptyLocation(put, i);
        }
        puts.add(put);
      }
    }
    putsToMetaTable(connection, puts);
    LOG.info(""Added {} regions to meta."", puts.size());
  }",False,False,This method adds new regions to the HBase meta table with an initial state of CLOSED. It also adds empty locations for region replicas.,False,"This method adds new regions to the HBase metadata table. Each region is initially set to a CLOSED state and empty locations are added for region replicas. The method takes a connection, a list of region information, the desired timestamp, and the region replication factor as input parameters."
60,"addMergeRegions(Put,Collection<RegionInfo>)",add merge regions put merge regions ,Method,"mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int)",org.apache.hadoop.hbase.TestMetaTableAccessor+testAddMergeRegions(),"java.util.Collection+size() java.util.Collection+size() java.lang.RuntimeException+RuntimeException(String) java.lang.String+format(String,Object[]) java.lang.String+format(String,Object[]) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.RegionInfo+toByteArray(RegionInfo) org.apache.hadoop.hbase.client.RegionInfo+toByteArray(RegionInfo)",org.apache.hadoop.hbase.HConstants+MERGE_QUALIFIER_PREFIX_STR org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,923,19,8,,"@VisibleForTesting
  static Put addMergeRegions(Put put, Collection<RegionInfo> mergeRegions) throws IOException {
    int limit = 10000; // Arbitrary limit. No room in our formatted 'task0000' below for more.
    int max = mergeRegions.size();
    if (max > limit) {
      // Should never happen!!!!! But just in case.
      throw new RuntimeException(
        ""Can't merge "" + max + "" regions in one go; "" + limit + "" is upper-limit."");
    }
    int counter = 0;
    for (RegionInfo ri : mergeRegions) {
      String qualifier = String.format(HConstants.MERGE_QUALIFIER_PREFIX_STR + ""%04d"", counter++);
      put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(put.getRow())
        .setFamily(HConstants.CATALOG_FAMILY).setQualifier(Bytes.toBytes(qualifier))
        .setTimestamp(put.getTimestamp()).setType(Type.Put).setValue(RegionInfo.toByteArray(ri))
        .build());
    }
    return put;
  }",False,True,"This method adds merge regions to a Put object. It checks if the number of merge regions exceeds a limit, and throws an exception if it does. It then iterates over the merge regions and adds them to the Put object with a specific qualifier. Finally, it returns the modified Put object.",False,"This code is a method called ""addMergeRegions"" that takes a Put object and a collection of RegionInfo objects as input. It adds the merge regions to the Put object by creating a new cell for each region and setting the necessary attributes. The modified Put object is then returned."
61,"mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int)",merge regions connection merged region parent seq num sn region replication merge regions into one in an atomic operation deletes the merging regions in hbase meta and adds the merged region param connection connection we re using param merged region the merged region param parent seq num parent regions to merge and their next open sequence id used by serialreplication set to 1 if not needed by this table param sn the location of the region ,Method,,"org.apache.hadoop.hbase.master.assignment.RegionStateStore+mergeRegions(RegionInfo,RegionInfo[],ServerName) org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsAddedAtRegionMerge() org.apache.hadoop.hbase.TestMetaTableAccessor+testMastersSystemTimeIsUsedInMergeRegions()","java.util.Map+entrySet() java.util.Map+entrySet() java.util.Map.Entry+getKey() java.util.Map.Entry+getKey() java.util.Map.Entry+getValue() java.util.Map.Entry+getValue() java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.MetaTableAccessor+makeDeleteFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+makeDeleteFromRegionInfo(RegionInfo,long) java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.MetaTableAccessor+makePutForReplicationBarrier(RegionInfo,long,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutForReplicationBarrier(RegionInfo,long,long) java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+addMergeRegions(Put,Collection<RegionInfo>) org.apache.hadoop.hbase.MetaTableAccessor+addMergeRegions(Put,Collection<RegionInfo>) java.util.Map+keySet() java.util.Map+keySet() java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.MetaTableAccessor+addLocation(Put,ServerName,long,int) org.apache.hadoop.hbase.MetaTableAccessor+addLocation(Put,ServerName,long,int) org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.MetaTableAccessor+addEmptyLocation(Put,int) org.apache.hadoop.hbase.MetaTableAccessor+addEmptyLocation(Put,int) java.util.List+isEmpty() java.util.List+isEmpty() org.apache.hadoop.hbase.MetaTableAccessor+addReplicationParent(Put,List<RegionInfo>) org.apache.hadoop.hbase.MetaTableAccessor+addReplicationParent(Put,List<RegionInfo>) org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.MetaTableAccessor+multiMutate(Connection,byte[],List<Mutation>) org.apache.hadoop.hbase.MetaTableAccessor+multiMutate(Connection,byte[],List<Mutation>)",org.apache.hadoop.hbase.HConstants+LATEST_TIMESTAMP org.apache.hadoop.hbase.HConstants+DELIMITER,2732,54,9,"/** 
 * Merge regions into one in an atomic operation. Deletes the merging regions in hbase:meta and adds the merged region.
 * @param connection connection we're using
 * @param mergedRegion the merged region
 * @param parentSeqNum Parent regions to merge and their next open sequence id used by serialreplication. Set to -1 if not needed by this table.
 * @param sn the location of the region
 */
","/**
   * Merge regions into one in an atomic operation. Deletes the merging regions in hbase:meta and
   * adds the merged region.
   * @param connection connection we're using
   * @param mergedRegion the merged region
   * @param parentSeqNum Parent regions to merge and their next open sequence id used by serial
   *          replication. Set to -1 if not needed by this table.
   * @param sn the location of the region
   */
  public static void mergeRegions(Connection connection, RegionInfo mergedRegion,
    Map<RegionInfo, Long> parentSeqNum, ServerName sn, int regionReplication) throws IOException {
    long time = HConstants.LATEST_TIMESTAMP;
    List<Mutation> mutations = new ArrayList<>();
    List<RegionInfo> replicationParents = new ArrayList<>();
    for (Map.Entry<RegionInfo, Long> e : parentSeqNum.entrySet()) {
      RegionInfo ri = e.getKey();
      long seqNum = e.getValue();
      // Deletes for merging regions
      mutations.add(makeDeleteFromRegionInfo(ri, time));
      if (seqNum > 0) {
        mutations.add(makePutForReplicationBarrier(ri, seqNum, time));
        replicationParents.add(ri);
      }
    }
    // Put for parent
    Put putOfMerged = makePutFromRegionInfo(mergedRegion, time);
    putOfMerged = addMergeRegions(putOfMerged, parentSeqNum.keySet());
    // Set initial state to CLOSED.
    // NOTE: If initial state is not set to CLOSED then merged region gets added with the
    // default OFFLINE state. If Master gets restarted after this step, start up sequence of
    // master tries to assign this offline region. This is followed by re-assignments of the
    // merged region from resumed {@link MergeTableRegionsProcedure}
    addRegionStateToPut(putOfMerged, RegionState.State.CLOSED);
    mutations.add(putOfMerged);
    // The merged is a new region, openSeqNum = 1 is fine. ServerName may be null
    // if crash after merge happened but before we got to here.. means in-memory
    // locations of offlined merged, now-closed, regions is lost. Should be ok. We
    // assign the merged region later.
    if (sn != null) {
      addLocation(putOfMerged, sn, 1, mergedRegion.getReplicaId());
    }

    // Add empty locations for region replicas of the merged region so that number of replicas
    // can be cached whenever the primary region is looked up from meta
    for (int i = 1; i < regionReplication; i++) {
      addEmptyLocation(putOfMerged, i);
    }
    // add parent reference for serial replication
    if (!replicationParents.isEmpty()) {
      addReplicationParent(putOfMerged, replicationParents);
    }
    byte[] tableRow = Bytes.toBytes(mergedRegion.getRegionNameAsString() + HConstants.DELIMITER);
    multiMutate(connection, tableRow, mutations);
  }",False,True,This method merges multiple regions into one in an atomic operation. It deletes the merging regions in the hbase:meta table and adds the merged region. It also handles replication and sets the initial state of the merged region to CLOSED.,False,This code merges multiple regions into one in an atomic operation. It deletes the merging regions from the hbase:meta table and adds the merged region. It also handles replication and sets the initial state of the merged region to CLOSED.
62,"splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)",split region connection parent parent open seq num split a split b sn region replication splits the region into two in an atomic operation offlines the parent region with the information that it is split into two and also adds the daughter regions does not add the location information to the daughter regions since they are not open yet param connection connection we re using param parent the parent region which is split param parent open seq num the next open sequence id for parent region used by serialreplication 1 if not necessary param split a split daughter region a param split b split daughter region b param sn the location of the region ,Method,,"org.apache.hadoop.hbase.master.assignment.RegionStateStore+splitRegion(RegionInfo,RegionInfo,RegionInfo,ServerName) org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationForRegionReplicasIsAddedAtRegionSplit() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaUpdatesGoToPriorityQueue() org.apache.hadoop.hbase.TestMetaTableAccessor+testEmptyMetaDaughterLocationDuringSplit()","org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.client.RegionInfoBuilder+build() org.apache.hadoop.hbase.client.RegionInfoBuilder+setSplit(boolean) org.apache.hadoop.hbase.client.RegionInfoBuilder+setOffline(boolean) org.apache.hadoop.hbase.client.RegionInfoBuilder+newBuilder(RegionInfo) org.apache.hadoop.hbase.client.RegionInfoBuilder+newBuilder(RegionInfo) org.apache.hadoop.hbase.client.RegionInfoBuilder+setOffline(boolean) org.apache.hadoop.hbase.client.RegionInfoBuilder+setSplit(boolean) org.apache.hadoop.hbase.client.RegionInfoBuilder+build() org.apache.hadoop.hbase.MetaTableAccessor+addDaughtersToPut(Put,RegionInfo,RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+addDaughtersToPut(Put,RegionInfo,RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromRegionInfo(RegionInfo,long) org.apache.hadoop.hbase.MetaTableAccessor+addReplicationBarrier(Put,long) org.apache.hadoop.hbase.MetaTableAccessor+addReplicationBarrier(Put,long) org.apache.hadoop.hbase.MetaTableAccessor+addReplicationParent(Put,List<RegionInfo>) org.apache.hadoop.hbase.MetaTableAccessor+addReplicationParent(Put,List<RegionInfo>) java.util.Collections+singletonList(T) java.util.Collections+singletonList(T) org.apache.hadoop.hbase.MetaTableAccessor+addReplicationParent(Put,List<RegionInfo>) org.apache.hadoop.hbase.MetaTableAccessor+addReplicationParent(Put,List<RegionInfo>) java.util.Collections+singletonList(T) java.util.Collections+singletonList(T) org.apache.hadoop.hbase.MetaTableAccessor+addSequenceNum(Put,long,int) org.apache.hadoop.hbase.MetaTableAccessor+addSequenceNum(Put,long,int) org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.MetaTableAccessor+addSequenceNum(Put,long,int) org.apache.hadoop.hbase.MetaTableAccessor+addSequenceNum(Put,long,int) org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.MetaTableAccessor+addEmptyLocation(Put,int) org.apache.hadoop.hbase.MetaTableAccessor+addEmptyLocation(Put,int) org.apache.hadoop.hbase.MetaTableAccessor+addEmptyLocation(Put,int) org.apache.hadoop.hbase.MetaTableAccessor+addEmptyLocation(Put,int) org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.MetaTableAccessor+multiMutate(Connection,byte[],List<Mutation>) org.apache.hadoop.hbase.MetaTableAccessor+multiMutate(Connection,byte[],List<Mutation>) java.util.Arrays+asList(T[]) java.util.Arrays+asList(T[])",org.apache.hadoop.hbase.HConstants+DELIMITER,2565,49,9,"/** 
 * Splits the region into two in an atomic operation. Offlines the parent region with the information that it is split into two, and also adds the daughter regions. Does not add the location information to the daughter regions since they are not open yet.
 * @param connection connection we're using
 * @param parent the parent region which is split
 * @param parentOpenSeqNum the next open sequence id for parent region, used by serialreplication. -1 if not necessary.
 * @param splitA Split daughter region A
 * @param splitB Split daughter region B
 * @param sn the location of the region
 */
","/**
   * Splits the region into two in an atomic operation. Offlines the parent region with the
   * information that it is split into two, and also adds the daughter regions. Does not add the
   * location information to the daughter regions since they are not open yet.
   * @param connection connection we're using
   * @param parent the parent region which is split
   * @param parentOpenSeqNum the next open sequence id for parent region, used by serial
   *          replication. -1 if not necessary.
   * @param splitA Split daughter region A
   * @param splitB Split daughter region B
   * @param sn the location of the region
   */
  public static void splitRegion(Connection connection, RegionInfo parent, long parentOpenSeqNum,
    RegionInfo splitA, RegionInfo splitB, ServerName sn, int regionReplication) throws IOException {
    long time = EnvironmentEdgeManager.currentTime();
    // Put for parent
    Put putParent = makePutFromRegionInfo(
      RegionInfoBuilder.newBuilder(parent).setOffline(true).setSplit(true).build(), time);
    addDaughtersToPut(putParent, splitA, splitB);

    // Puts for daughters
    Put putA = makePutFromRegionInfo(splitA, time);
    Put putB = makePutFromRegionInfo(splitB, time);
    if (parentOpenSeqNum > 0) {
      addReplicationBarrier(putParent, parentOpenSeqNum);
      addReplicationParent(putA, Collections.singletonList(parent));
      addReplicationParent(putB, Collections.singletonList(parent));
    }
    // Set initial state to CLOSED
    // NOTE: If initial state is not set to CLOSED then daughter regions get added with the
    // default OFFLINE state. If Master gets restarted after this step, start up sequence of
    // master tries to assign these offline regions. This is followed by re-assignments of the
    // daughter regions from resumed {@link SplitTableRegionProcedure}
    addRegionStateToPut(putA, RegionState.State.CLOSED);
    addRegionStateToPut(putB, RegionState.State.CLOSED);

    addSequenceNum(putA, 1, splitA.getReplicaId()); // new regions, openSeqNum = 1 is fine.
    addSequenceNum(putB, 1, splitB.getReplicaId());

    // Add empty locations for region replicas of daughters so that number of replicas can be
    // cached whenever the primary region is looked up from meta
    for (int i = 1; i < regionReplication; i++) {
      addEmptyLocation(putA, i);
      addEmptyLocation(putB, i);
    }

    byte[] tableRow = Bytes.toBytes(parent.getRegionNameAsString() + HConstants.DELIMITER);
    multiMutate(connection, tableRow, Arrays.asList(putParent, putA, putB));
  }",False,True,"The function of this method is to split a region into two daughter regions in an atomic operation. It offline the parent region, adds the daughter regions, and sets their initial state to CLOSED. It also adds replication information if necessary and adds empty locations for region replicas.",False,"This code is a method called ""splitRegion"" that splits a region into two daughter regions in an atomic operation. It creates Put objects for the parent region and the two daughter regions, sets their state to CLOSED, and adds replication information if necessary. Finally, it performs a multi-mutation operation to update the region information in the table."
63,"updateTableState(Connection,TableState)",update table state connection state update state of the table in meta param connection what we use for update param state new state ,Method,"updateTableState(Connection,TableName,TableState.State)","org.apache.hadoop.hbase.master.TableStateManager+updateMetaState(TableName,TableState.State) org.apache.hadoop.hbase.util.HBaseFsck+checkAndFixTableStates() org.apache.hadoop.hbase.HBaseTestingUtility+createMultiRegionsInMeta(Configuration,TableDescriptor,byte[][])","org.apache.hadoop.hbase.MetaTableAccessor+makePutFromTableState(TableState,long) org.apache.hadoop.hbase.MetaTableAccessor+makePutFromTableState(TableState,long) org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.MetaTableAccessor+putToMetaTable(Connection,Put) org.apache.hadoop.hbase.MetaTableAccessor+putToMetaTable(Connection,Put)",org.apache.hadoop.hbase.MetaTableAccessor+LOG,396,10,10,"/** 
 * Update state of the table in meta.
 * @param connection what we use for update
 * @param state new state
 */
","/**
   * Update state of the table in meta.
   * @param connection what we use for update
   * @param state new state
   */
  private static void updateTableState(Connection connection, TableState state) throws IOException {
    Put put = makePutFromTableState(state, EnvironmentEdgeManager.currentTime());
    putToMetaTable(connection, put);
    LOG.info(""Updated {} in hbase:meta"", state);
  }",False,False,"This method updates the state of a table in the meta table of HBase. It takes a connection and the new state as parameters, creates a Put object from the table state, and then puts it into the meta table. It also logs the update.",False,"The code is a private method called ""updateTableState"" that takes a connection and a table state as parameters. It creates a Put object from the table state and the current time, then calls the ""putToMetaTable"" method to update the state of the table in the HBase meta table. Finally, it logs a message indicating that the table state has been updated."
64,"makePutFromTableState(TableState,long)",make put from table state state ts construct put for given state param state new state ,Method,"updateTableState(Connection,TableState)",,"org.apache.hadoop.hbase.client.Put+Put(byte[],long) org.apache.hadoop.hbase.TableName+getName() org.apache.hadoop.hbase.client.TableState+getTableName() org.apache.hadoop.hbase.client.TableState+getTableName() org.apache.hadoop.hbase.TableName+getName() org.apache.hadoop.hbase.client.Put+addColumn(byte[],byte[],byte[]) org.apache.hadoop.hbase.client.Put+addColumn(byte[],byte[],byte[]) org.apache.hadoop.hbase.client.TableState+convert() org.apache.hadoop.hbase.client.TableState+convert()",org.apache.hadoop.hbase.HConstants+TABLE_FAMILY org.apache.hadoop.hbase.HConstants+TABLE_STATE_QUALIFIER,337,10,9,"/** 
 * Construct PUT for given state
 * @param state new state
 */
","/**
   * Construct PUT for given state
   * @param state new state
   */
  public static Put makePutFromTableState(TableState state, long ts) {
    Put put = new Put(state.getTableName().getName(), ts);
    put.addColumn(HConstants.TABLE_FAMILY, HConstants.TABLE_STATE_QUALIFIER,
      state.convert().toByteArray());
    return put;
  }",False,False,"This method constructs a PUT operation for a given state by creating a Put object, setting the table name and timestamp, and adding a column with the state data. The method returns the constructed Put object.",False,"This code defines a static method called ""makePutFromTableState"" that takes a TableState object and a timestamp as input. It constructs a Put object with the given state and timestamp, and adds a column to it with the table state converted to a byte array. The method then returns the constructed Put object."
65,"deleteTableState(Connection,TableName)",delete table state connection table remove state for table from meta param connection to use for deletion param table to delete state for ,Method,,org.apache.hadoop.hbase.master.TableStateManager+setDeletedTable(TableName) org.apache.hadoop.hbase.util.HBaseFsck+checkAndFixTableStates() org.apache.hadoop.hbase.master.TestTableStateManager+testMigration(),"org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.client.Delete+Delete(byte[]) org.apache.hadoop.hbase.TableName+getName() org.apache.hadoop.hbase.TableName+getName() org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,Delete) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,Delete)",org.apache.hadoop.hbase.HConstants+TABLE_FAMILY org.apache.hadoop.hbase.HConstants+TABLE_STATE_QUALIFIER org.apache.hadoop.hbase.MetaTableAccessor+LOG,528,12,9,"/** 
 * Remove state for table from meta
 * @param connection to use for deletion
 * @param table to delete state for
 */
","/**
   * Remove state for table from meta
   * @param connection to use for deletion
   * @param table to delete state for
   */
  public static void deleteTableState(Connection connection, TableName table) throws IOException {
    long time = EnvironmentEdgeManager.currentTime();
    Delete delete = new Delete(table.getName());
    delete.addColumns(HConstants.TABLE_FAMILY, HConstants.TABLE_STATE_QUALIFIER, time);
    deleteFromMetaTable(connection, delete);
    LOG.info(""Deleted table "" + table + "" state from META"");
  }",False,False,"This method serves to delete the state of a table from the meta table in a database. It takes a connection and table as parameters, and uses them to create a delete operation on the meta table. The method then deletes the table state from the meta table and logs the deletion.",False,"This code defines a method called `deleteTableState` that removes the state of a table from the metadata in HBase. It takes a connection and the name of the table as parameters, and it deletes the table state by adding a delete operation to the meta table. The method also logs a message indicating that the table state has been successfully deleted."
66,"multiMutate(Connection,byte[],List<Mutation>)",multi mutate conn row mutations performs an atomic multi mutate operation against the given table used by the likes of merge and split as these want to make atomic mutations across multiple rows throws io exception even if we encounter a runtime exception we ll still wrap it in an ioe ,Method,"mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)",,"org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutations(List<? extends Mutation>) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutations(List<? extends Mutation>) org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil+toMutation(MutationType,Mutation) org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil+toMutation(MutationType,Mutation) org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil+toMutation(MutationType,Mutation) org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil+toMutation(MutationType,Mutation) org.apache.hadoop.hbase.DoNotRetryIOException+DoNotRetryIOException(String) java.lang.Class+getName() java.lang.Object+getClass() java.lang.Object+getClass() java.lang.Class+getName() org.apache.hadoop.hbase.client.AsyncConnection+getTable(TableName) org.apache.hadoop.hbase.client.Connection+toAsyncConnection() org.apache.hadoop.hbase.client.Connection+toAsyncConnection() org.apache.hadoop.hbase.client.AsyncConnection+getTable(TableName) org.apache.hadoop.hbase.util.FutureUtils+get(Future<T>) org.apache.hadoop.hbase.util.FutureUtils+get(Future<T>)",org.apache.hadoop.hbase.TableName+META_TABLE_NAME,1464,29,10,"/** 
 * Performs an atomic multi-mutate operation against the given table. Used by the likes of merge and split as these want to make atomic mutations across multiple rows.
 * @throws IOException even if we encounter a RuntimeException, we'll still wrap it in an IOE.
 */
","/**
   * Performs an atomic multi-mutate operation against the given table. Used by the likes of merge
   * and split as these want to make atomic mutations across multiple rows.
   * @throws IOException even if we encounter a RuntimeException, we'll still wrap it in an IOE.
   */
  private static void multiMutate(Connection conn, byte[] row, List<Mutation> mutations)
    throws IOException {
    debugLogMutations(mutations);
    MutateRowsRequest.Builder builder = MutateRowsRequest.newBuilder();
    for (Mutation mutation : mutations) {
      if (mutation instanceof Put) {
        builder.addMutationRequest(
          ProtobufUtil.toMutation(ClientProtos.MutationProto.MutationType.PUT, mutation));
      } else if (mutation instanceof Delete) {
        builder.addMutationRequest(
          ProtobufUtil.toMutation(ClientProtos.MutationProto.MutationType.DELETE, mutation));
      } else {
        throw new DoNotRetryIOException(
          ""multi in MetaEditor doesn't support "" + mutation.getClass().getName());
      }
    }
    MutateRowsRequest request = builder.build();
    AsyncTable<?> table = conn.toAsyncConnection().getTable(TableName.META_TABLE_NAME);
    CompletableFuture<MutateRowsResponse> future =
      table.<MultiRowMutationService, MutateRowsResponse> coprocessorService(
        MultiRowMutationService::newStub,
        (stub, controller, done) -> stub.mutateRows(controller, request, done), row);
    FutureUtils.get(future);
  }",False,True,"The function of this method is to perform an atomic multi-mutate operation against a given table. It takes a connection, a row, and a list of mutations as input. It builds a request with the mutations, sends the request to the table's coprocessor service, and waits for the response.",False,"This code defines a private method called `multiMutate` that performs an atomic multi-mutate operation on a given table. It takes a connection, a row, and a list of mutations as input, and uses the HBase API to build a `MutateRowsRequest` with the specified mutations. It then uses an asynchronous table to execute the mutation and waits for the result."
67,"updateRegionLocation(Connection,RegionInfo,ServerName,long,long)",update region location connection region info sn open seq num master system time updates the location of the specified region in hbase meta to be the specified server hostname and startcode uses passed catalog tracker to get a connection to the server hosting hbase meta and makes edits to that region param connection connection we re using param region info region to update location of param open seq num the latest sequence number obtained when the region was open param sn server name param master system time wall clock time from master if passed in the open region rp c ,Method,,org.apache.hadoop.hbase.regionserver.HRegionServer+skipReportingTransition(RegionStateTransitionContext) org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationsForRegionReplicas() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationsForRegionReplicas() org.apache.hadoop.hbase.TestMetaTableAccessor+testMetaLocationsForRegionReplicas() org.apache.hadoop.hbase.TestMetaTableAccessor+testMastersSystemTimeIsUsedInUpdateLocations() org.apache.hadoop.hbase.TestMetaTableAccessor+testMastersSystemTimeIsUsedInMergeRegions(),"org.apache.hadoop.hbase.MetaTableAccessor+updateLocation(Connection,RegionInfo,ServerName,long,long) org.apache.hadoop.hbase.MetaTableAccessor+updateLocation(Connection,RegionInfo,ServerName,long,long)",,831,17,9,"/** 
 * Updates the location of the specified region in hbase:meta to be the specified server hostname and startcode. <p> Uses passed catalog tracker to get a connection to the server hosting hbase:meta and makes edits to that region.
 * @param connection connection we're using
 * @param regionInfo region to update location of
 * @param openSeqNum the latest sequence number obtained when the region was open
 * @param sn Server name
 * @param masterSystemTime wall clock time from master if passed in the open region RPC
 */
","/**
   * Updates the location of the specified region in hbase:meta to be the specified server hostname
   * and startcode.
   * <p>
   * Uses passed catalog tracker to get a connection to the server hosting hbase:meta and makes
   * edits to that region.
   * @param connection connection we're using
   * @param regionInfo region to update location of
   * @param openSeqNum the latest sequence number obtained when the region was open
   * @param sn Server name
   * @param masterSystemTime wall clock time from master if passed in the open region RPC
   */
  @VisibleForTesting
  public static void updateRegionLocation(Connection connection, RegionInfo regionInfo,
    ServerName sn, long openSeqNum, long masterSystemTime) throws IOException {
    updateLocation(connection, regionInfo, sn, openSeqNum, masterSystemTime);
  }",False,False,The function updates the location of a specified region in hbase:meta to a specified server hostname and startcode. It uses a passed catalog tracker to get a connection to the server hosting hbase:meta and makes edits to that region.,False,This code updates the location of a specified region in the hbase:meta table to a specified server hostname and startcode. It uses a passed catalog tracker to establish a connection to the server hosting hbase:meta and makes the necessary edits to the region.
68,"updateLocation(Connection,RegionInfo,ServerName,long,long)",update location connection region info sn open seq num master system time updates the location of the specified region to be the specified server connects to the specified server which should be hosting the specified catalog region name to perform the edit param connection connection we re using param region info region to update location of param sn server name param open seq num the latest sequence number obtained when the region was open param master system time wall clock time from master if passed in the open region rp c throws io exception in particular could throw link java net connect exception if the serveris down on other end ,Method,"updateRegionLocation(Connection,RegionInfo,ServerName,long,long)",,"org.apache.hadoop.hbase.client.Put+Put(byte[],long) org.apache.hadoop.hbase.CatalogFamilyFormat+getMetaKeyForRegion(RegionInfo) org.apache.hadoop.hbase.CatalogFamilyFormat+getMetaKeyForRegion(RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+addRegionInfo(Put,RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+addRegionInfo(Put,RegionInfo) org.apache.hadoop.hbase.MetaTableAccessor+addLocation(Put,ServerName,long,int) org.apache.hadoop.hbase.MetaTableAccessor+addLocation(Put,ServerName,long,int) org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.client.RegionInfo+getReplicaId() org.apache.hadoop.hbase.MetaTableAccessor+putToMetaTable(Connection,Put) org.apache.hadoop.hbase.MetaTableAccessor+putToMetaTable(Connection,Put) org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString()",org.apache.hadoop.hbase.MetaTableAccessor+LOG,1197,22,10,"/** 
 * Updates the location of the specified region to be the specified server. <p> Connects to the specified server which should be hosting the specified catalog region name to perform the edit.
 * @param connection connection we're using
 * @param regionInfo region to update location of
 * @param sn Server name
 * @param openSeqNum the latest sequence number obtained when the region was open
 * @param masterSystemTime wall clock time from master if passed in the open region RPC
 * @throws IOException In particular could throw {@link java.net.ConnectException} if the serveris down on other end.
 */
","/**
   * Updates the location of the specified region to be the specified server.
   * <p>
   * Connects to the specified server which should be hosting the specified catalog region name to
   * perform the edit.
   * @param connection connection we're using
   * @param regionInfo region to update location of
   * @param sn Server name
   * @param openSeqNum the latest sequence number obtained when the region was open
   * @param masterSystemTime wall clock time from master if passed in the open region RPC
   * @throws IOException In particular could throw {@link java.net.ConnectException} if the server
   *           is down on other end.
   */
  private static void updateLocation(Connection connection, RegionInfo regionInfo, ServerName sn,
    long openSeqNum, long masterSystemTime) throws IOException {
    // region replicas are kept in the primary region's row
    Put put = new Put(CatalogFamilyFormat.getMetaKeyForRegion(regionInfo), masterSystemTime);
    addRegionInfo(put, regionInfo);
    addLocation(put, sn, openSeqNum, regionInfo.getReplicaId());
    putToMetaTable(connection, put);
    LOG.info(""Updated row {} with server="", regionInfo.getRegionNameAsString(), sn);
  }",False,False,This method updates the location of a specified region to a specified server. It connects to the server hosting the catalog region and performs the update by adding the region information and location details to the meta table. It also logs the update.,False,"This code updates the location of a specified region to a specified server by connecting to the server and performing the necessary edits. It creates a Put object with the updated information and adds it to the meta table. Finally, it logs the update with the region name and server information."
69,"deleteRegionInfo(Connection,RegionInfo)",delete region info connection region info deletes the specified region from meta param connection connection we re using param region info region to be deleted from met a ,Method,,"org.apache.hadoop.hbase.master.assignment.GCRegionProcedure+executeFromState(MasterProcedureEnv,GCRegionState) org.apache.hadoop.hbase.util.HBaseFsckRepair+removeParentInMeta(Configuration,RegionInfo) org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+testConsistency() org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+verifyMiddleHole(CatalogJanitor) org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+verifyCornerHoles(CatalogJanitor,TableName) org.apache.hadoop.hbase.master.TestCatalogJanitorCluster+verifyCornerHoles(CatalogJanitor,TableName) org.apache.hadoop.hbase.master.TestMetaFixer+deleteRegion(MasterServices,RegionInfo)","org.apache.hadoop.hbase.client.Delete+Delete(byte[]) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.Delete+addFamily(byte[],long) org.apache.hadoop.hbase.client.Delete+addFamily(byte[],long) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,Delete) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,Delete) org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString()",org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+LATEST_TIMESTAMP org.apache.hadoop.hbase.MetaTableAccessor+LOG,513,12,9,"/** 
 * Deletes the specified region from META.
 * @param connection connection we're using
 * @param regionInfo region to be deleted from META
 */
","/**
   * Deletes the specified region from META.
   * @param connection connection we're using
   * @param regionInfo region to be deleted from META
   */
  public static void deleteRegionInfo(Connection connection, RegionInfo regionInfo)
    throws IOException {
    Delete delete = new Delete(regionInfo.getRegionName());
    delete.addFamily(HConstants.CATALOG_FAMILY, HConstants.LATEST_TIMESTAMP);
    deleteFromMetaTable(connection, delete);
    LOG.info(""Deleted "" + regionInfo.getRegionNameAsString());
  }",False,False,"This method serves to delete a specified region from the META table in a HBase database. It takes a connection and a regionInfo as parameters, creates a delete object, adds a family to delete, and then deletes the region from the META table using the connection. Finally, it logs the deletion.",False,"This code defines a method called ""deleteRegionInfo"" that takes a connection and a regionInfo as parameters. It creates a Delete object with the region's name and adds a family to be deleted from the META table. It then calls a helper method to delete the entry from the META table and logs a message indicating the successful deletion."
70,"deleteRegionInfos(Connection,List<RegionInfo>)",delete region infos connection regions info deletes the specified regions from meta param connection connection we re using param regions info list of regions to be deleted from met a ,Method,,org.apache.hadoop.hbase.master.assignment.RegionStateStore+deleteRegions(List<RegionInfo>) org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure+updateMETA(MasterProcedureEnv),"org.apache.hadoop.hbase.MetaTableAccessor+deleteRegionInfos(Connection,List<RegionInfo>,long) org.apache.hadoop.hbase.MetaTableAccessor+deleteRegionInfos(Connection,List<RegionInfo>,long) org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime()",,372,9,9,"/** 
 * Deletes the specified regions from META.
 * @param connection connection we're using
 * @param regionsInfo list of regions to be deleted from META
 */
","/**
   * Deletes the specified regions from META.
   * @param connection connection we're using
   * @param regionsInfo list of regions to be deleted from META
   */
  public static void deleteRegionInfos(Connection connection, List<RegionInfo> regionsInfo)
    throws IOException {
    deleteRegionInfos(connection, regionsInfo, EnvironmentEdgeManager.currentTime());
  }",False,False,This method serves to delete the specified regions from the META table using the given connection and list of regions. It throws an IOException if there is an error.,False,"This code defines a method called ""deleteRegionInfos"" that takes a connection and a list of region information as parameters. It is used to delete the specified regions from the META table. The method throws an IOException and calls another version of the same method with an additional parameter for the current time."
71,"deleteRegionInfos(Connection,List<RegionInfo>,long)",delete region infos connection regions info ts deletes the specified regions from meta param connection connection we re using param regions info list of regions to be deleted from met a ,Method,"deleteRegionInfos(Connection,List<RegionInfo>) overwriteRegions(Connection,List<RegionInfo>,int)",,"java.util.List+size() java.util.List+size() org.apache.hadoop.hbase.client.Delete+Delete(byte[]) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.Delete+addFamily(byte[],long) org.apache.hadoop.hbase.client.Delete+addFamily(byte[],long) java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,List<Delete>) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,List<Delete>) java.util.List+size() java.util.List+size()",org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.MetaTableAccessor+LOG org.apache.hadoop.hbase.MetaTableAccessor+LOG,692,17,10,"/** 
 * Deletes the specified regions from META.
 * @param connection connection we're using
 * @param regionsInfo list of regions to be deleted from META
 */
","/**
   * Deletes the specified regions from META.
   * @param connection connection we're using
   * @param regionsInfo list of regions to be deleted from META
   */
  private static void deleteRegionInfos(Connection connection, List<RegionInfo> regionsInfo,
    long ts) throws IOException {
    List<Delete> deletes = new ArrayList<>(regionsInfo.size());
    for (RegionInfo hri : regionsInfo) {
      Delete e = new Delete(hri.getRegionName());
      e.addFamily(HConstants.CATALOG_FAMILY, ts);
      deletes.add(e);
    }
    deleteFromMetaTable(connection, deletes);
    LOG.info(""Deleted {} regions from META"", regionsInfo.size());
    LOG.debug(""Deleted regions: {}"", regionsInfo);
  }",False,False,"The function of this method is to delete the specified regions from the META table in a given connection. It takes a list of regions to be deleted, creates a list of Delete objects for each region, and then deletes them from the META table using the deleteFromMetaTable method. It also logs the number of regions deleted and the list of deleted regions.",False,"This code is a private method that deletes specified regions from the META table in HBase. It takes a connection, a list of regions to be deleted, and a timestamp as input. It creates a list of Delete objects for each region, adds the catalog family with the given timestamp to each Delete object, and then deletes the regions from the META table using the deleteFromMetaTable method."
72,"overwriteRegions(Connection,List<RegionInfo>,int)",overwrite regions connection region infos region replication overwrites the specified regions from hbase meta deletes old rows for the given regions and adds new ones regions added back have state closed param connection connection we re using param region infos list of regions to be added to met a ,Method,,org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure+updateMETA(MasterProcedureEnv),"org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.util.EnvironmentEdgeManager+currentTime() org.apache.hadoop.hbase.MetaTableAccessor+deleteRegionInfos(Connection,List<RegionInfo>,long) org.apache.hadoop.hbase.MetaTableAccessor+deleteRegionInfos(Connection,List<RegionInfo>,long) org.apache.hadoop.hbase.MetaTableAccessor+addRegionsToMeta(Connection,List<RegionInfo>,int,long) org.apache.hadoop.hbase.MetaTableAccessor+addRegionsToMeta(Connection,List<RegionInfo>,int,long) java.util.List+size() java.util.List+size()",org.apache.hadoop.hbase.MetaTableAccessor+LOG org.apache.hadoop.hbase.MetaTableAccessor+LOG,1206,21,9,"/** 
 * Overwrites the specified regions from hbase:meta. Deletes old rows for the given regions and adds new ones. Regions added back have state CLOSED.
 * @param connection connection we're using
 * @param regionInfos list of regions to be added to META
 */
","/**
   * Overwrites the specified regions from hbase:meta. Deletes old rows for the given regions and
   * adds new ones. Regions added back have state CLOSED.
   * @param connection connection we're using
   * @param regionInfos list of regions to be added to META
   */
  public static void overwriteRegions(Connection connection, List<RegionInfo> regionInfos,
    int regionReplication) throws IOException {
    // use master time for delete marker and the Put
    long now = EnvironmentEdgeManager.currentTime();
    deleteRegionInfos(connection, regionInfos, now);
    // Why sleep? This is the easiest way to ensure that the previous deletes does not
    // eclipse the following puts, that might happen in the same ts from the server.
    // See HBASE-9906, and HBASE-9879. Once either HBASE-9879, HBASE-8770 is fixed,
    // or HBASE-9905 is fixed and meta uses seqIds, we do not need the sleep.
    //
    // HBASE-13875 uses master timestamp for the mutations. The 20ms sleep is not needed
    addRegionsToMeta(connection, regionInfos, regionReplication, now + 1);
    LOG.info(""Overwritten "" + regionInfos.size() + "" regions to Meta"");
    LOG.debug(""Overwritten regions: {} "", regionInfos);
  }",False,False,The function of this method is to overwrite specified regions in the hbase:meta table. It deletes old rows for the given regions and adds new ones with a state of CLOSED.,False,"This code is a method called ""overwriteRegions"" that takes a connection, a list of regionInfos, and a regionReplication value as parameters. It deletes old rows for the specified regions from the hbase:meta table and adds new rows with a state of CLOSED. It also logs the number of regions overwritten and the list of overwritten regions."
73,"deleteMergeQualifiers(Connection,RegionInfo)",delete merge qualifiers connection merge region deletes merge qualifiers for the specified merge region param connection connection we re using param merge region the merged region ,Method,,"org.apache.hadoop.hbase.master.assignment.GCMergedRegionsProcedure+executeFromState(MasterProcedureEnv,GCMergedRegionsState) org.apache.hadoop.hbase.master.assignment.GCMultipleMergedRegionsProcedure+executeFromState(MasterProcedureEnv,GCMergedRegionsState) org.apache.hadoop.hbase.TestMetaTableAccessor+testGetMergeRegions()","org.apache.hadoop.hbase.client.Delete+Delete(byte[]) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.Result+rawCells() org.apache.hadoop.hbase.MetaTableAccessor+getRegionResult(Connection,byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getRegionResult(Connection,byte[]) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.Result+rawCells() org.apache.hadoop.hbase.MetaTableAccessor+isMergeQualifierPrefix(Cell) org.apache.hadoop.hbase.MetaTableAccessor+isMergeQualifierPrefix(Cell) org.apache.hadoop.hbase.CellUtil+cloneQualifier(Cell) org.apache.hadoop.hbase.CellUtil+cloneQualifier(Cell) java.util.List+add(E) java.util.List+add(E) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) org.apache.hadoop.hbase.client.Delete+addColumns(byte[],byte[],long) java.util.List+isEmpty() java.util.List+isEmpty() org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,Delete) org.apache.hadoop.hbase.MetaTableAccessor+deleteFromMetaTable(Connection,Delete) org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() org.apache.hadoop.hbase.client.RegionInfo+getRegionNameAsString() java.util.stream.Stream+map(Function) java.util.Collection+stream() java.util.Collection+stream() java.util.stream.Stream+map(Function) java.util.stream.Collectors+joining(CharSequence) java.util.stream.Collectors+joining(CharSequence)",org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+LATEST_TIMESTAMP org.apache.hadoop.hbase.MetaTableAccessor+LOG org.apache.hadoop.hbase.MetaTableAccessor+LOG,1621,37,9,"/** 
 * Deletes merge qualifiers for the specified merge region.
 * @param connection connection we're using
 * @param mergeRegion the merged region
 */
","/**
   * Deletes merge qualifiers for the specified merge region.
   * @param connection connection we're using
   * @param mergeRegion the merged region
   */
  public static void deleteMergeQualifiers(Connection connection, final RegionInfo mergeRegion)
    throws IOException {
    Delete delete = new Delete(mergeRegion.getRegionName());
    // NOTE: We are doing a new hbase:meta read here.
    Cell[] cells = getRegionResult(connection, mergeRegion.getRegionName()).rawCells();
    if (cells == null || cells.length == 0) {
      return;
    }
    List<byte[]> qualifiers = new ArrayList<>();
    for (Cell cell : cells) {
      if (!isMergeQualifierPrefix(cell)) {
        continue;
      }
      byte[] qualifier = CellUtil.cloneQualifier(cell);
      qualifiers.add(qualifier);
      delete.addColumns(HConstants.CATALOG_FAMILY, qualifier, HConstants.LATEST_TIMESTAMP);
    }

    // There will be race condition that a GCMultipleMergedRegionsProcedure is scheduled while
    // the previous GCMultipleMergedRegionsProcedure is still going on, in this case, the second
    // GCMultipleMergedRegionsProcedure could delete the merged region by accident!
    if (qualifiers.isEmpty()) {
      LOG.info(""No merged qualifiers for region "" + mergeRegion.getRegionNameAsString() +
        "" in meta table, they are cleaned up already, Skip."");
      return;
    }

    deleteFromMetaTable(connection, delete);
    LOG.info(""Deleted merge references in "" + mergeRegion.getRegionNameAsString() +
      "", deleted qualifiers "" +
      qualifiers.stream().map(Bytes::toStringBinary).collect(Collectors.joining("", "")));
  }",False,True,"The function of this method is to delete merge qualifiers for a specified merge region. It takes a connection and the merge region as parameters, and performs the deletion by creating a Delete object, retrieving the region result, and adding the necessary columns to the delete object. If there are no merge qualifiers, it logs a message and returns. Finally, it deletes the merge references from the meta table and logs a message indicating the deletion.",False,"This code defines a method called `deleteMergeQualifiers` that takes a connection and a merge region as input. It deletes merge qualifiers for the specified merge region by performing a new hbase:meta read, extracting the qualifiers from the cells, and deleting them from the meta table. It also logs the deleted qualifiers."
74,"addRegionInfo(Put,RegionInfo)",add region info p hri ,Method,"makePutFromRegionInfo(RegionInfo,long) updateLocation(Connection,RegionInfo,ServerName,long,long)","org.apache.hadoop.hbase.master.assignment.RegionStateStore+updateUserRegionLocation(RegionInfo,State,ServerName,long,long)",org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.RegionInfo+toByteArray(RegionInfo) org.apache.hadoop.hbase.client.RegionInfo+toByteArray(RegionInfo) org.apache.hadoop.hbase.client.RegionReplicaUtil+getRegionInfoForDefaultReplica(RegionInfo) org.apache.hadoop.hbase.client.RegionReplicaUtil+getRegionInfoForDefaultReplica(RegionInfo),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+REGIONINFO_QUALIFIER,649,11,9,,"public static Put addRegionInfo(final Put p, final RegionInfo hri) throws IOException {
    p.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(p.getRow())
      .setFamily(HConstants.CATALOG_FAMILY).setQualifier(HConstants.REGIONINFO_QUALIFIER)
      .setTimestamp(p.getTimestamp()).setType(Type.Put)
      // Serialize the Default Replica HRI otherwise scan of hbase:meta
      // shows an info:regioninfo value with encoded name and region
      // name that differs from that of the hbase;meta row.
      .setValue(RegionInfo.toByteArray(RegionReplicaUtil.getRegionInfoForDefaultReplica(hri)))
      .build());
    return p;
  }",False,False,"This method adds region information to a Put object in HBase. It creates a new CellBuilder, sets the row, family, qualifier, timestamp, and type of the cell, and then serializes the region information and adds it to the Put object. Finally, it returns the modified Put object.",True,"This code defines a method called ""addRegionInfo"" that takes a Put object and a RegionInfo object as parameters. It adds a cell to the Put object with the region information serialized as the value. The method then returns the modified Put object."
75,"addLocation(Put,ServerName,long,int)",add location p sn open seq num replica id ,Method,"mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int) updateLocation(Connection,RegionInfo,ServerName,long,long)","org.apache.hadoop.hbase.master.assignment.RegionStateStore+updateUserRegionLocation(RegionInfo,State,ServerName,long,long) org.apache.hadoop.hbase.util.HBaseFsckRepair+fixMetaHoleOnlineAndAddReplicas(Configuration,RegionInfo,Collection<ServerName>,int)",org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerColumn(int) org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.net.Address+toString() org.apache.hadoop.hbase.ServerName+getAddress() org.apache.hadoop.hbase.ServerName+getAddress() org.apache.hadoop.hbase.net.Address+toString() org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getStartCodeColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getStartCodeColumn(int) org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.util.Bytes+toBytes(long) org.apache.hadoop.hbase.util.Bytes+toBytes(long) org.apache.hadoop.hbase.ServerName+getStartcode() org.apache.hadoop.hbase.ServerName+getStartcode() org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getSeqNumColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getSeqNumColumn(int) org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.util.Bytes+toBytes(long) org.apache.hadoop.hbase.util.Bytes+toBytes(long),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.Cell+Type org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.Cell+Type org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,1022,15,9,,"public static Put addLocation(Put p, ServerName sn, long openSeqNum, int replicaId)
    throws IOException {
    CellBuilder builder = CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY);
    return p
      .add(builder.clear().setRow(p.getRow()).setFamily(HConstants.CATALOG_FAMILY)
        .setQualifier(CatalogFamilyFormat.getServerColumn(replicaId)).setTimestamp(p.getTimestamp())
        .setType(Cell.Type.Put).setValue(Bytes.toBytes(sn.getAddress().toString())).build())
      .add(builder.clear().setRow(p.getRow()).setFamily(HConstants.CATALOG_FAMILY)
        .setQualifier(CatalogFamilyFormat.getStartCodeColumn(replicaId))
        .setTimestamp(p.getTimestamp()).setType(Cell.Type.Put)
        .setValue(Bytes.toBytes(sn.getStartcode())).build())
      .add(builder.clear().setRow(p.getRow()).setFamily(HConstants.CATALOG_FAMILY)
        .setQualifier(CatalogFamilyFormat.getSeqNumColumn(replicaId)).setTimestamp(p.getTimestamp())
        .setType(Type.Put).setValue(Bytes.toBytes(openSeqNum)).build());
  }",False,False,The function of this method is to add location information to a Put object. It creates three cells using the provided parameters and adds them to the Put object.,False,"The `addLocation` method takes a `Put` object, a `ServerName` object, an open sequence number, and a replica ID as input. It creates and adds three cells to the `Put` object, each containing specific information about the server, such as its address, start code, and sequence number. The modified `Put` object is then returned."
76,"writeRegionName(ByteArrayOutputStream,byte[])",write region name out region name ,Method,getParentsBytes(List<RegionInfo>) getParentsBytes(List<RegionInfo>),,java.io.ByteArrayOutputStream+write(int) java.io.ByteArrayOutputStream+write(int) java.io.ByteArrayOutputStream+write(int) java.io.ByteArrayOutputStream+write(int),org.apache.hadoop.hbase.MetaTableAccessor+ESCAPE_BYTE org.apache.hadoop.hbase.MetaTableAccessor+ESCAPE_BYTE,215,8,10,,"private static void writeRegionName(ByteArrayOutputStream out, byte[] regionName) {
    for (byte b : regionName) {
      if (b == ESCAPE_BYTE) {
        out.write(ESCAPE_BYTE);
      }
      out.write(b);
    }
  }",True,True,"This method writes a byte array to a ByteArrayOutputStream, while checking for a specific byte value and writing it twice if found.",False,"The code defines a private static method called ""writeRegionName"" that takes an output stream and a byte array as parameters. It iterates through each byte in the array and writes it to the output stream, while also checking for a specific byte value and writing it twice if found."
77,getParentsBytes(List<RegionInfo>),get parents bytes parents ,Method,"addReplicationParent(Put,List<RegionInfo>)","org.apache.hadoop.hbase.replication.regionserver.TestSerialReplicationChecker+addParents(RegionInfo,List<RegionInfo>)","java.io.ByteArrayOutputStream+ByteArrayOutputStream() java.util.List+iterator() java.util.List+iterator() org.apache.hadoop.hbase.MetaTableAccessor+writeRegionName(ByteArrayOutputStream,byte[]) org.apache.hadoop.hbase.MetaTableAccessor+writeRegionName(ByteArrayOutputStream,byte[]) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() java.util.Iterator+next() java.util.Iterator+next() org.apache.hadoop.hbase.client.RegionInfo+getRegionName() java.util.Iterator+hasNext() java.util.Iterator+hasNext() java.io.ByteArrayOutputStream+write(int) java.io.ByteArrayOutputStream+write(int) java.io.ByteArrayOutputStream+write(int) java.io.ByteArrayOutputStream+write(int) org.apache.hadoop.hbase.MetaTableAccessor+writeRegionName(ByteArrayOutputStream,byte[]) org.apache.hadoop.hbase.MetaTableAccessor+writeRegionName(ByteArrayOutputStream,byte[]) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() java.util.Iterator+next() java.util.Iterator+next() org.apache.hadoop.hbase.client.RegionInfo+getRegionName() java.io.ByteArrayOutputStream+toByteArray() java.io.ByteArrayOutputStream+toByteArray()",org.apache.hadoop.hbase.MetaTableAccessor+ESCAPE_BYTE org.apache.hadoop.hbase.MetaTableAccessor+SEPARATED_BYTE,442,12,9,,"@VisibleForTesting
  public static byte[] getParentsBytes(List<RegionInfo> parents) {
    ByteArrayOutputStream bos = new ByteArrayOutputStream();
    Iterator<RegionInfo> iter = parents.iterator();
    writeRegionName(bos, iter.next().getRegionName());
    while (iter.hasNext()) {
      bos.write(ESCAPE_BYTE);
      bos.write(SEPARATED_BYTE);
      writeRegionName(bos, iter.next().getRegionName());
    }
    return bos.toByteArray();
  }",True,True,"This method takes a list of RegionInfo objects as input and returns a byte array. It writes the region names of the objects into a ByteArrayOutputStream, separating them with escape and separated bytes, and finally converts the ByteArrayOutputStream to a byte array.",False,"This code is a method that takes a list of RegionInfo objects as input and returns a byte array. It iterates over the list, writing the region names to a ByteArrayOutputStream with escape and separated bytes in between. The resulting byte array is then returned."
78,parseParentsBytes(byte[]),parse parents bytes bytes ,Method,getReplicationBarrierResult(Result),,java.io.ByteArrayOutputStream+ByteArrayOutputStream() java.util.List+add(E) java.util.List+add(E) java.io.ByteArrayOutputStream+toByteArray() java.io.ByteArrayOutputStream+toByteArray() java.io.ByteArrayOutputStream+reset() java.io.ByteArrayOutputStream+reset() java.io.ByteArrayOutputStream+write(int) java.io.ByteArrayOutputStream+write(int) java.io.ByteArrayOutputStream+size() java.io.ByteArrayOutputStream+size() java.util.List+add(E) java.util.List+add(E) java.io.ByteArrayOutputStream+toByteArray() java.io.ByteArrayOutputStream+toByteArray(),org.apache.hadoop.hbase.MetaTableAccessor+ESCAPE_BYTE org.apache.hadoop.hbase.MetaTableAccessor+SEPARATED_BYTE,578,20,10,,"private static List<byte[]> parseParentsBytes(byte[] bytes) {
    List<byte[]> parents = new ArrayList<>();
    ByteArrayOutputStream bos = new ByteArrayOutputStream();
    for (int i = 0; i < bytes.length; i++) {
      if (bytes[i] == ESCAPE_BYTE) {
        i++;
        if (bytes[i] == SEPARATED_BYTE) {
          parents.add(bos.toByteArray());
          bos.reset();
          continue;
        }
        // fall through to append the byte
      }
      bos.write(bytes[i]);
    }
    if (bos.size() > 0) {
      parents.add(bos.toByteArray());
    }
    return parents;
  }",True,True,This method takes a byte array as input and parses it to extract a list of byte arrays. It looks for specific bytes (ESCAPE_BYTE and SEPARATED_BYTE) to determine when to add a new byte array to the list.,False,"This code defines a private static method named `parseParentsBytes` that takes a byte array as input and returns a list of byte arrays. It iterates through the input byte array, separating it into multiple byte arrays based on specific byte values, and adds them to the list."
79,"addReplicationParent(Put,List<RegionInfo>)",add replication parent put parents ,Method,"mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)",,org.apache.hadoop.hbase.MetaTableAccessor+getParentsBytes(List<RegionInfo>) org.apache.hadoop.hbase.MetaTableAccessor+getParentsBytes(List<RegionInfo>) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp(),org.apache.hadoop.hbase.HConstants+REPLICATION_BARRIER_FAMILY org.apache.hadoop.hbase.MetaTableAccessor+REPLICATION_PARENT_QUALIFIER,417,6,10,,"private static void addReplicationParent(Put put, List<RegionInfo> parents) throws IOException {
    byte[] value = getParentsBytes(parents);
    put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(put.getRow())
      .setFamily(HConstants.REPLICATION_BARRIER_FAMILY).setQualifier(REPLICATION_PARENT_QUALIFIER)
      .setTimestamp(put.getTimestamp()).setType(Type.Put).setValue(value).build());
  }",True,True,"This method adds a replication parent to a Put object by creating a cell with the specified row, family, qualifier, timestamp, type, and value.",False,"The code defines a private static method called ""addReplicationParent"" that takes a Put object and a list of RegionInfo objects as parameters. It creates a Cell object with specific properties and adds it to the Put object. The method is responsible for adding a replication parent to the Put object."
80,"makePutForReplicationBarrier(RegionInfo,long,long)",make put for replication barrier region info open seq num ts ,Method,"mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int)",,"org.apache.hadoop.hbase.client.Put+Put(byte[],long) org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.client.RegionInfo+getRegionName() org.apache.hadoop.hbase.MetaTableAccessor+addReplicationBarrier(Put,long) org.apache.hadoop.hbase.MetaTableAccessor+addReplicationBarrier(Put,long)",,239,6,9,,"public static Put makePutForReplicationBarrier(RegionInfo regionInfo, long openSeqNum, long ts)
    throws IOException {
    Put put = new Put(regionInfo.getRegionName(), ts);
    addReplicationBarrier(put, openSeqNum);
    return put;
  }",True,True,"This method creates a Put object with a given RegionInfo, timestamp, and open sequence number. It then adds a replication barrier to the Put object and returns it.",False,"The given code is a method named ""makePutForReplicationBarrier"" that takes in a RegionInfo object, an open sequence number, and a timestamp as parameters. It creates a Put object with the region name and timestamp, adds a replication barrier using the open sequence number, and returns the Put object."
81,"addReplicationBarrier(Put,long)",add replication barrier put open seq num see class comment on serial replication checker ,Method,"splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int) makePutForReplicationBarrier(RegionInfo,long,long)","org.apache.hadoop.hbase.master.assignment.RegionStateStore+updateUserRegionLocation(RegionInfo,State,ServerName,long,long)",org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.util.Bytes+toBytes(long) org.apache.hadoop.hbase.util.Bytes+toBytes(long),org.apache.hadoop.hbase.HConstants+REPLICATION_BARRIER_FAMILY org.apache.hadoop.hbase.HConstants+SEQNUM_QUALIFIER,452,9,9,"/** 
 * See class comment on SerialReplicationChecker
 */
","/**
   * See class comment on SerialReplicationChecker
   */
  public static void addReplicationBarrier(Put put, long openSeqNum) throws IOException {
    put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(put.getRow())
      .setFamily(HConstants.REPLICATION_BARRIER_FAMILY).setQualifier(HConstants.SEQNUM_QUALIFIER)
      .setTimestamp(put.getTimestamp()).setType(Type.Put).setValue(Bytes.toBytes(openSeqNum))
      .build());
  }",False,False,"This method adds a replication barrier to a Put object. It creates a new Cell with the specified row, family, qualifier, timestamp, type, and value, and adds it to the Put object.",False,"This code defines a method called ""addReplicationBarrier"" that adds a replication barrier to a Put object. The barrier is represented as a special cell with a specific family, qualifier, and value. The method takes a Put object and a long value representing the open sequence number as parameters, and throws an IOException if an error occurs."
82,"addEmptyLocation(Put,int)",add empty location p replica id ,Method,"addRegionsToMeta(Connection,List<RegionInfo>,int,long) mergeRegions(Connection,RegionInfo,Map<RegionInfo,Long>,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)",,org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getServerColumn(int) org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getStartCodeColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getStartCodeColumn(int) org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+clear() org.apache.hadoop.hbase.CellBuilder+setRow(byte[]) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.CellBuilder+setFamily(byte[]) org.apache.hadoop.hbase.CellBuilder+setQualifier(byte[]) org.apache.hadoop.hbase.CatalogFamilyFormat+getSeqNumColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getSeqNumColumn(int) org.apache.hadoop.hbase.CellBuilder+setTimestamp(long) org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp(),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.Cell+Type org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.Cell+Type,852,13,10,,"private static Put addEmptyLocation(Put p, int replicaId) throws IOException {
    CellBuilder builder = CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY);
    return p
      .add(builder.clear().setRow(p.getRow()).setFamily(HConstants.CATALOG_FAMILY)
        .setQualifier(CatalogFamilyFormat.getServerColumn(replicaId)).setTimestamp(p.getTimestamp())
        .setType(Type.Put).build())
      .add(builder.clear().setRow(p.getRow()).setFamily(HConstants.CATALOG_FAMILY)
        .setQualifier(CatalogFamilyFormat.getStartCodeColumn(replicaId))
        .setTimestamp(p.getTimestamp()).setType(Cell.Type.Put).build())
      .add(builder.clear().setRow(p.getRow()).setFamily(HConstants.CATALOG_FAMILY)
        .setQualifier(CatalogFamilyFormat.getSeqNumColumn(replicaId)).setTimestamp(p.getTimestamp())
        .setType(Cell.Type.Put).build());
  }",False,False,This method adds empty location information to a Put object. It creates three cells with specific qualifiers and adds them to the Put object.,False,"The `addEmptyLocation` method takes a `Put` object and a replica ID as input and returns a modified `Put` object. It adds three new cells to the `Put` object, each with a specific row, family, qualifier, timestamp, and type, based on the given replica ID."
83,ReplicationBarrierResult,replication barrier result ,MemberClass,,,"org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+ReplicationBarrierResult(long[],State,List<byte[]>) org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+getBarriers() org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+getParentRegionNames() org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+toString() java.util.Arrays+toString(long[]) java.util.Arrays+toString(long[]) java.util.stream.Stream+map(Function) java.util.Collection+stream() java.util.Collection+stream() java.util.stream.Stream+map(Function) java.util.stream.Collectors+joining(CharSequence) java.util.stream.Collectors+joining(CharSequence)",org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+barriers org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+parentRegionNames org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+barriers org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+parentRegionNames org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+barriers org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+parentRegionNames org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+barriers org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+parentRegionNames,911,31,25,,"public static final class ReplicationBarrierResult {
    private final long[] barriers;
    private final RegionState.State state;
    private final List<byte[]> parentRegionNames;

    ReplicationBarrierResult(long[] barriers, State state, List<byte[]> parentRegionNames) {
      this.barriers = barriers;
      this.state = state;
      this.parentRegionNames = parentRegionNames;
    }

    public long[] getBarriers() {
      return barriers;
    }

    public RegionState.State getState() {
      return state;
    }

    public List<byte[]> getParentRegionNames() {
      return parentRegionNames;
    }

    @Override
    public String toString() {
      return ""ReplicationBarrierResult [barriers="" + Arrays.toString(barriers) + "", state="" +
        state + "", parentRegionNames="" +
        parentRegionNames.stream().map(Bytes::toStringBinary).collect(Collectors.joining("", "")) +
        ""]"";
    }
  }",False,True,,False,
84,getReplicationBarrier(Cell),get replication barrier c ,Method,,,"org.apache.hadoop.hbase.util.Bytes+toLong(byte[],int,int) org.apache.hadoop.hbase.util.Bytes+toLong(byte[],int,int) org.apache.hadoop.hbase.Cell+getValueArray() org.apache.hadoop.hbase.Cell+getValueArray() org.apache.hadoop.hbase.Cell+getValueOffset() org.apache.hadoop.hbase.Cell+getValueOffset() org.apache.hadoop.hbase.Cell+getValueLength() org.apache.hadoop.hbase.Cell+getValueLength()",,139,3,10,,"private static long getReplicationBarrier(Cell c) {
    return Bytes.toLong(c.getValueArray(), c.getValueOffset(), c.getValueLength());
  }",True,True,"The function of the given method is to retrieve a long value from a specific cell in a HBase table by using the getValueArray(), getValueOffset(), and getValueLength() methods.",False,"The given code is a private static method named ""getReplicationBarrier"" that takes a Cell object as input. It returns a long value by converting the value stored in the Cell object to a long using the Bytes class from the HBase library."
85,getReplicationBarriers(Result),get replication barriers result ,Method,"getReplicationBarrierResult(Result) getReplicationBarrier(Connection,byte[])",org.apache.hadoop.hbase.master.cleaner.ReplicationBarrierCleaner+chore() org.apache.hadoop.hbase.util.TestHBaseFsckCleanReplicationBarriers+testCleanReplicationBarrierWithDeletedTable(),"java.util.stream.Stream+mapToLong(ToLongFunction) java.util.Collection+stream() org.apache.hadoop.hbase.client.Result+getColumnCells(byte[],byte[]) org.apache.hadoop.hbase.client.Result+getColumnCells(byte[],byte[]) java.util.Collection+stream() java.util.stream.Stream+mapToLong(ToLongFunction)",org.apache.hadoop.hbase.HConstants+REPLICATION_BARRIER_FAMILY org.apache.hadoop.hbase.HConstants+SEQNUM_QUALIFIER,264,4,9,,"public static long[] getReplicationBarriers(Result result) {
    return result.getColumnCells(HConstants.REPLICATION_BARRIER_FAMILY, HConstants.SEQNUM_QUALIFIER)
      .stream().mapToLong(MetaTableAccessor::getReplicationBarrier).sorted().distinct().toArray();
  }",False,True,"This method returns an array of long values representing replication barriers. It retrieves the column cells from a given result object, filters them based on specific family and qualifier, maps them to long values using a method reference, sorts them, removes duplicates, and converts them to an array.",False,"This code defines a method named ""getReplicationBarriers"" that takes a Result object as input. It retrieves the column cells from the Result object with a specific family and qualifier, converts them to a stream, maps them to long values using the ""getReplicationBarrier"" method from the MetaTableAccessor class, sorts and removes duplicates, and finally returns the resulting array of long values."
86,getReplicationBarrierResult(Result),get replication barrier result result ,Method,"getReplicationBarrierResult(Connection,TableName,byte[],byte[])",,"org.apache.hadoop.hbase.MetaTableAccessor+getReplicationBarriers(Result) org.apache.hadoop.hbase.MetaTableAccessor+getReplicationBarriers(Result) org.apache.hadoop.hbase.client.Result+getValue(byte[],byte[]) org.apache.hadoop.hbase.client.Result+getValue(byte[],byte[]) org.apache.hadoop.hbase.util.Bytes+toString(byte[]) org.apache.hadoop.hbase.util.Bytes+toString(byte[]) org.apache.hadoop.hbase.client.Result+getValue(byte[],byte[]) org.apache.hadoop.hbase.client.Result+getValue(byte[],byte[]) org.apache.hadoop.hbase.MetaTableAccessor+parseParentsBytes(byte[]) org.apache.hadoop.hbase.MetaTableAccessor+parseParentsBytes(byte[]) java.util.Collections+emptyList() java.util.Collections+emptyList() org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+ReplicationBarrierResult(long[],State,List<byte[]>)",org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+STATE_QUALIFIER org.apache.hadoop.hbase.master.RegionState+State org.apache.hadoop.hbase.HConstants+REPLICATION_BARRIER_FAMILY org.apache.hadoop.hbase.MetaTableAccessor+REPLICATION_PARENT_QUALIFIER,695,11,10,,"private static ReplicationBarrierResult getReplicationBarrierResult(Result result) {
    long[] barriers = getReplicationBarriers(result);
    byte[] stateBytes = result.getValue(HConstants.CATALOG_FAMILY, HConstants.STATE_QUALIFIER);
    RegionState.State state =
      stateBytes != null ? RegionState.State.valueOf(Bytes.toString(stateBytes)) : null;
    byte[] parentRegionsBytes =
      result.getValue(HConstants.REPLICATION_BARRIER_FAMILY, REPLICATION_PARENT_QUALIFIER);
    List<byte[]> parentRegionNames =
      parentRegionsBytes != null ? parseParentsBytes(parentRegionsBytes) : Collections.emptyList();
    return new ReplicationBarrierResult(barriers, state, parentRegionNames);
  }",True,True,"This method takes a Result object as input and returns a ReplicationBarrierResult object. It extracts relevant data from the Result object, such as replication barriers, region state, and parent region names, and uses them to create a new ReplicationBarrierResult object.",False,"This code defines a private static method named ""getReplicationBarrierResult"" that takes a Result object as input. It retrieves specific values from the Result object and uses them to create and return a new ReplicationBarrierResult object. The ReplicationBarrierResult object contains an array of long values, a RegionState.State value, and a list of byte arrays."
87,"getReplicationBarrierResult(Connection,TableName,byte[],byte[])",get replication barrier result conn table name row encoded region name ,Method,,"org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler+canPush(Entry,byte[])","org.apache.hadoop.hbase.client.RegionInfo+createRegionName(TableName,byte[],String,boolean) org.apache.hadoop.hbase.client.RegionInfo+createRegionName(TableName,byte[],String,boolean) org.apache.hadoop.hbase.client.RegionInfo+createRegionName(TableName,byte[],String,boolean) org.apache.hadoop.hbase.client.RegionInfo+createRegionName(TableName,byte[],String,boolean) org.apache.hadoop.hbase.client.Scan+setCaching(int) org.apache.hadoop.hbase.client.Scan+setReversed(boolean) org.apache.hadoop.hbase.client.Scan+readAllVersions() org.apache.hadoop.hbase.client.Scan+addFamily(byte[]) org.apache.hadoop.hbase.client.Scan+addColumn(byte[],byte[]) org.apache.hadoop.hbase.client.Scan+withStopRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStartRow(byte[]) org.apache.hadoop.hbase.client.Scan+Scan() org.apache.hadoop.hbase.client.Scan+withStartRow(byte[]) org.apache.hadoop.hbase.client.Scan+withStopRow(byte[]) org.apache.hadoop.hbase.client.Scan+addColumn(byte[],byte[]) org.apache.hadoop.hbase.client.Scan+addFamily(byte[]) org.apache.hadoop.hbase.client.Scan+readAllVersions() org.apache.hadoop.hbase.client.Scan+setReversed(boolean) org.apache.hadoop.hbase.client.Scan+setCaching(int) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Table+getScanner(Scan) org.apache.hadoop.hbase.client.Table+getScanner(Scan) org.apache.hadoop.hbase.client.ResultScanner+next() org.apache.hadoop.hbase.client.ResultScanner+next() org.apache.hadoop.hbase.MetaTableAccessor.ReplicationBarrierResult+ReplicationBarrierResult(long[],State,List<byte[]>) java.util.Collections+emptyList() java.util.Collections+emptyList() org.apache.hadoop.hbase.client.Result+getRow() org.apache.hadoop.hbase.client.Result+getRow() org.apache.hadoop.hbase.util.Bytes+equals(byte[],byte[]) org.apache.hadoop.hbase.util.Bytes+equals(byte[],byte[]) org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.util.Bytes+toBytes(String) org.apache.hadoop.hbase.client.RegionInfo+encodeRegionName(byte[]) org.apache.hadoop.hbase.client.RegionInfo+encodeRegionName(byte[]) org.apache.hadoop.hbase.MetaTableAccessor+getReplicationBarrierResult(Result) org.apache.hadoop.hbase.MetaTableAccessor+getReplicationBarrierResult(Result)",org.apache.hadoop.hbase.HConstants+NINES org.apache.hadoop.hbase.HConstants+EMPTY_START_ROW org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY org.apache.hadoop.hbase.HConstants+STATE_QUALIFIER org.apache.hadoop.hbase.HConstants+REPLICATION_BARRIER_FAMILY,1469,27,9,,"public static ReplicationBarrierResult getReplicationBarrierResult(Connection conn,
    TableName tableName, byte[] row, byte[] encodedRegionName) throws IOException {
    byte[] metaStartKey = RegionInfo.createRegionName(tableName, row, HConstants.NINES, false);
    byte[] metaStopKey =
      RegionInfo.createRegionName(tableName, HConstants.EMPTY_START_ROW, """", false);
    Scan scan = new Scan().withStartRow(metaStartKey).withStopRow(metaStopKey)
      .addColumn(HConstants.CATALOG_FAMILY, HConstants.STATE_QUALIFIER)
      .addFamily(HConstants.REPLICATION_BARRIER_FAMILY).readAllVersions().setReversed(true)
      .setCaching(10);
    try (Table table = getMetaHTable(conn); ResultScanner scanner = table.getScanner(scan)) {
      for (Result result;;) {
        result = scanner.next();
        if (result == null) {
          return new ReplicationBarrierResult(new long[0], null, Collections.emptyList());
        }
        byte[] regionName = result.getRow();
        // TODO: we may look up a region which has already been split or merged so we need to check
        // whether the encoded name matches. Need to find a way to quit earlier when there is no
        // record for the given region, for now it will scan to the end of the table.
        if (!Bytes.equals(encodedRegionName,
          Bytes.toBytes(RegionInfo.encodeRegionName(regionName)))) {
          continue;
        }
        return getReplicationBarrierResult(result);
      }
    }
  }",True,True,"This method retrieves the replication barrier result for a given connection, table name, row, and encoded region name. It scans the meta table to find the matching region and returns the replication barrier result if found.",False,This code retrieves the replication barrier result for a specific region in a table. It scans the meta table to find the region with the given encoded region name and returns the replication barrier result for that region.
88,"getReplicationBarrier(Connection,byte[])",get replication barrier conn region name ,Method,,org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler+isParentFinished(byte[]) org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testCleanNoPeers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testCleanNoPeers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testCleanNoPeers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testCleanNoPeers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testDeleteBarriers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testDeleteBarriers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testDeleteBarriers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testDeleteBarriers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testDeleteBarriers() org.apache.hadoop.hbase.master.cleaner.TestReplicationBarrierCleaner+testDeleteRowForDeletedRegion() org.apache.hadoop.hbase.util.TestHBaseFsckCleanReplicationBarriers+testCleanReplicationBarrierWithDeletedTable() org.apache.hadoop.hbase.util.TestHBaseFsckCleanReplicationBarriers+testCleanReplicationBarrierWithExistTable(),"org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.MetaTableAccessor+getMetaHTable(Connection) org.apache.hadoop.hbase.client.Table+get(Get) org.apache.hadoop.hbase.client.Table+get(Get) org.apache.hadoop.hbase.client.Get+readAllVersions() org.apache.hadoop.hbase.client.Get+addColumn(byte[],byte[]) org.apache.hadoop.hbase.client.Get+Get(byte[]) org.apache.hadoop.hbase.client.Get+addColumn(byte[],byte[]) org.apache.hadoop.hbase.client.Get+readAllVersions() org.apache.hadoop.hbase.MetaTableAccessor+getReplicationBarriers(Result) org.apache.hadoop.hbase.MetaTableAccessor+getReplicationBarriers(Result)",org.apache.hadoop.hbase.HConstants+REPLICATION_BARRIER_FAMILY org.apache.hadoop.hbase.HConstants+SEQNUM_QUALIFIER,372,9,9,,"public static long[] getReplicationBarrier(Connection conn, byte[] regionName)
    throws IOException {
    try (Table table = getMetaHTable(conn)) {
      Result result = table.get(new Get(regionName)
        .addColumn(HConstants.REPLICATION_BARRIER_FAMILY, HConstants.SEQNUM_QUALIFIER)
        .readAllVersions());
      return getReplicationBarriers(result);
    }
  }",True,True,The function of the given method is to retrieve the replication barriers for a specific region from a HBase table using the provided connection and region name. It returns an array of long values representing the replication barriers.,False,"The given code is a method named ""getReplicationBarrier"" that takes a Connection object and a byte array as parameters. It retrieves a Result object from a table using the given Connection object and the regionName byte array. It then calls another method ""getReplicationBarriers"" to extract and return an array of long values from the Result object."
89,"getTableEncodedRegionNameAndLastBarrier(Connection,TableName)",get table encoded region name and last barrier conn table name ,Method,,"org.apache.hadoop.hbase.master.replication.AbstractPeerProcedure+setLastPushedSequenceIdForTable(MasterProcedureEnv,TableName,Map<String,Long>)","org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Visitor) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType)",,843,19,9,,"public static List<Pair<String, Long>> getTableEncodedRegionNameAndLastBarrier(Connection conn,
    TableName tableName) throws IOException {
    List<Pair<String, Long>> list = new ArrayList<>();
    scanMeta(conn,
      ClientMetaTableAccessor.getTableStartRowForMeta(tableName, QueryType.REPLICATION),
      ClientMetaTableAccessor.getTableStopRowForMeta(tableName, QueryType.REPLICATION),
      QueryType.REPLICATION, r -> {
        byte[] value =
          r.getValue(HConstants.REPLICATION_BARRIER_FAMILY, HConstants.SEQNUM_QUALIFIER);
        if (value == null) {
          return true;
        }
        long lastBarrier = Bytes.toLong(value);
        String encodedRegionName = RegionInfo.encodeRegionName(r.getRow());
        list.add(Pair.newPair(encodedRegionName, lastBarrier));
        return true;
      });
    return list;
  }",True,True,"This method returns a list of pairs, where each pair consists of an encoded region name and the last barrier value. It scans the meta table for a specific table and retrieves the encoded region name and last barrier value for each region.",False,"This code retrieves the encoded region names and last barrier values for a given table from the meta table in HBase. It scans the meta table using the provided connection and table name, and for each row, it checks if a value exists for the replication barrier family and seqnum qualifier. If a value exists, it adds the encoded region name and last barrier value to a list and returns it."
90,"getTableEncodedRegionNamesForSerialReplication(Connection,TableName)",get table encoded region names for serial replication conn table name ,Method,,,"org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Filter,int,Visitor) org.apache.hadoop.hbase.MetaTableAccessor+scanMeta(Connection,byte[],byte[],QueryType,Filter,int,Visitor) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStartRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.ClientMetaTableAccessor+getTableStopRowForMeta(TableName,QueryType) org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter+FirstKeyOnlyFilter()",java.lang.Integer+MAX_VALUE,567,12,9,,"public static List<String> getTableEncodedRegionNamesForSerialReplication(Connection conn,
    TableName tableName) throws IOException {
    List<String> list = new ArrayList<>();
    scanMeta(conn,
      ClientMetaTableAccessor.getTableStartRowForMeta(tableName, QueryType.REPLICATION),
      ClientMetaTableAccessor.getTableStopRowForMeta(tableName, QueryType.REPLICATION),
      QueryType.REPLICATION, new FirstKeyOnlyFilter(), Integer.MAX_VALUE, r -> {
        list.add(RegionInfo.encodeRegionName(r.getRow()));
        return true;
      });
    return list;
  }",True,False,"This method returns a list of encoded region names for serial replication of a given table. It scans the metadata table using the provided connection and table name, and adds the encoded region names to the list.",False,"This code is a method that retrieves the encoded region names for a given table in a serial replication scenario. It scans the meta table using the provided connection and table name, and adds the encoded region names to a list. The list is then returned as the result."
91,debugLogMutations(List<? extends Mutation>),debug log mutations mutations ,Method,"putsToMetaTable(Connection,List<Put>) deleteFromMetaTable(Connection,List<Delete>) multiMutate(Connection,byte[],List<Mutation>)",,org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutation(Mutation) org.apache.hadoop.hbase.MetaTableAccessor+debugLogMutation(Mutation),org.apache.hadoop.hbase.MetaTableAccessor+METALOG,380,10,10,,"private static void debugLogMutations(List<? extends Mutation> mutations) throws IOException {
    if (!METALOG.isDebugEnabled()) {
      return;
    }
    // Logging each mutation in separate line makes it easier to see diff between them visually
    // because of common starting indentation.
    for (Mutation mutation : mutations) {
      debugLogMutation(mutation);
    }
  }",False,False,"This method logs mutations for debugging purposes. It takes a list of mutations as input, and if debug logging is enabled, it logs each mutation in a separate line for easier visual comparison.",False,"The code defines a private static method called `debugLogMutations` that takes a list of `Mutation` objects as input. It checks if debug logging is enabled and then iterates over each mutation in the list, calling another method called `debugLogMutation` to log each mutation separately."
92,debugLogMutation(Mutation),debug log mutation p ,Method,"put(Table,Put) addSplitsToParent(Connection,RegionInfo,RegionInfo,RegionInfo) debugLogMutations(List<? extends Mutation>)",,java.lang.Class+getSimpleName() java.lang.Object+getClass() java.lang.Object+getClass() java.lang.Class+getSimpleName() org.apache.hadoop.hbase.client.Operation+toJSON() org.apache.hadoop.hbase.client.Operation+toJSON(),org.apache.hadoop.hbase.MetaTableAccessor+METALOG,143,3,10,,"private static void debugLogMutation(Mutation p) throws IOException {
    METALOG.debug(""{} {}"", p.getClass().getSimpleName(), p.toJSON());
  }",False,False,This method is used to log the details of a mutation object for debugging purposes. It logs the class name of the mutation object and its JSON representation using a logger named METALOG.,False,The code defines a private static method called `debugLogMutation` that takes a `Mutation` object as a parameter. It logs the class name of the object and its JSON representation using a logger called `METALOG`. The method throws an `IOException` if there is an error during the logging process.
93,"addSequenceNum(Put,long,int)",add sequence num p open seq num replica id ,Method,"splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int) splitRegion(Connection,RegionInfo,long,RegionInfo,RegionInfo,ServerName,int)",,org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.client.Put+add(Cell) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.CellBuilderFactory+create(CellBuilderType) org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.client.Mutation+getRow() org.apache.hadoop.hbase.CatalogFamilyFormat+getSeqNumColumn(int) org.apache.hadoop.hbase.CatalogFamilyFormat+getSeqNumColumn(int) org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.client.Mutation+getTimestamp() org.apache.hadoop.hbase.util.Bytes+toBytes(long) org.apache.hadoop.hbase.util.Bytes+toBytes(long),org.apache.hadoop.hbase.HConstants+CATALOG_FAMILY,403,6,10,,"private static Put addSequenceNum(Put p, long openSeqNum, int replicaId) throws IOException {
    return p.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(p.getRow())
      .setFamily(HConstants.CATALOG_FAMILY)
      .setQualifier(CatalogFamilyFormat.getSeqNumColumn(replicaId)).setTimestamp(p.getTimestamp())
      .setType(Type.Put).setValue(Bytes.toBytes(openSeqNum)).build());
  }",False,True,This method adds a sequence number to a Put object and returns the modified Put object.,False,"The code defines a private static method called ""addSequenceNum"" that takes a Put object, an open sequence number, and a replica ID as parameters. It creates a new Cell object with the specified row, family, qualifier, timestamp, type, and value, and adds it to the given Put object. The method returns the modified Put object."
